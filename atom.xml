<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Fraser's IdM Blog</title>
    <link href="https://frasertweedale.github.io/blog-redhat/atom.xml" rel="self" />
    <link href="https://frasertweedale.github.io/blog-redhat" />
    <id>https://frasertweedale.github.io/blog-redhat/atom.xml</id>
    <author>
        <name>Fraser Tweedale</name>
        <email>frase@frase.id.au</email>
    </author>
    <updated>2022-08-29T00:00:00Z</updated>
    <entry>
    <title>Controlling header formatting in JAX-RS applications</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2022-08-29-jax-rs-header-formatting.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2022-08-29-jax-rs-header-formatting.html</id>
    <published>2022-08-29T00:00:00Z</published>
    <updated>2022-08-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="controlling-header-formatting-in-jax-rs-applications">Controlling header formatting in JAX-RS applications</h1>
<p>I’m been implementing an <a href="https://www.rfc-editor.org/rfc/rfc7030"><em>Enrollment over Secure Transport
(EST)</em></a> service in Dogtag PKI. During testing, I found
that a notable client implementation parses the response
<code>Content-Type</code> header in the following way:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode c"><code class="sourceCode c"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="op">(!</span>strncmp<span class="op">(</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    multipart_get_data_content_type<span class="op">(</span>parser<span class="op">),</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;application/pkcs7-mime; smime-type=certs-only&quot;</span><span class="op">,</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="dv">45</span><span class="op">)</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  <span class="op">)</span> <span class="op">{</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="op">...</span></span></code></pre></div>
<p>The Dogtag EST service is a <a href="https://projects.eclipse.org/projects/ee4j.rest"><em>Jakarta RESTful Web Services
(JAX-RS)</em></a> application. It produces a <code>Content-Type</code> header
value different from what the client expects (note the lack of
whitespace):</p>
<pre><code>application/pkcs7-mime;smime-type=certs-only</code></pre>
<p>As a consequence, the EST client fails to process the response.
This is certainly a defect in the EST client implementation. But
EST is used by many embedded or hard to update network devices. Or
updates might not be available (now, <em>ever?</em>)</p>
<p>So, I needed to find a way to override the header default header
formatting. This blog post describes my solution.</p>
<h2 id="specifying-the-content-type-header">Specifying the <code>Content-Type</code> header <a href="#specifying-the-content-type-header" class="section">§</a></h2>
<p>The JAX-RS <code>@Produces</code> annotation specifies the <code>Content-Type</code>
header value for a particular resource:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode java"><code class="sourceCode java"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="at">@POST</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="at">@Path</span><span class="op">(</span><span class="st">&quot;simpleenroll&quot;</span><span class="op">)</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="at">@Consumes</span><span class="op">(</span><span class="st">&quot;application/pkcs10&quot;</span><span class="op">)</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="at">@Produces</span><span class="op">(</span><span class="st">&quot;application/pkcs7-mime; smime-type=certs-only&quot;</span><span class="op">)</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="kw">public</span> <span class="bu">Response</span> <span class="fu">simpleenroll</span><span class="op">(</span><span class="dt">byte</span><span class="op">[]</span> data<span class="op">)</span> <span class="op">{</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">...</span></span></code></pre></div>
<p>Note that the string value is not used <em>verbatim</em>. Instead, it is
parsed into a <a href="https://docs.oracle.com/javaee/7/api/javax/ws/rs/core/MediaType.html"><code>MediaType</code></a> value and stored as such in
the response headers (a <code>MultivaluedMap&lt;String, Object&gt;</code>).</p>
<p>When serialising the <code>Response</code>, header values are stringified via
types that implement the
<a href="https://docs.oracle.com/javaee/7/api/javax/ws/rs/ext/RuntimeDelegate.HeaderDelegate.html"><code>RuntimeDelegate.HeaderDelegate&lt;T&gt;</code></a> interface,
where <code>T</code> is the real type of the header value <code>Object</code>. To
serialise a <code>MediaType</code> header value, the JAX-RS machinery uses a
instance of a a class that implements
<code>RuntimeDelegate.HeaderDelegate&lt;MediaType&gt;</code>.</p>
<p><code>HeaderDelegate</code> <em>implementations</em> are not part of the JAX-RS API.
They are provided by the JAX-RS implementation. In Dogtag PKI,
that’s <a href="https://resteasy.dev/">RESTEasy</a>. The class in question is:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode java"><code class="sourceCode java"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">public</span> <span class="kw">class</span> MediaTypeHeaderDelegate</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">implements</span> RuntimeDelegate<span class="op">.</span><span class="fu">HeaderDelegate</span><span class="op">&lt;</span>MediaType<span class="op">&gt;</span> <span class="op">{</span></span></code></pre></div>
<p>The <code>toString(MediaType type)</code> method provided by this class prints
the value without a space character between the subtype and the
parameters. For the example resource above, it produces the string:</p>
<pre><code>application/pkcs7-mime;smime-type=certs-only</code></pre>
<p>This is a legal production in the HTTP grammar, according to <a href="https://www.rfc-editor.org/rfc/rfc7230#section-3.2.3">RFC
7230</a> and <a href="https://www.rfc-editor.org/rfc/rfc7231#section-3.1.1.1">RFC 7231</a>:</p>
<pre><code>media-type = type &quot;/&quot; subtype *( OWS &quot;;&quot; OWS parameter )
OWS = *( SP / HTAB )</code></pre>
<p>However, we already saw that at least one EST client is unable to
process this value, because it expects a space character before the
parameters:</p>
<pre><code>application/pkcs7-mime; smime-type=certs-only</code></pre>
<p>This is also a legal production. But the client is using <code>strncmp</code>
to look for this exact string, instead of properly parsing the
value. If we can’t fix the client behaviour, we have to find a
workaround on the server to produce the exact string the client
expects.</p>
<h2 id="idea-1-custom-headerdelegate">Idea 1: custom <code>HeaderDelegate</code> <a href="#idea-1-custom-headerdelegate" class="section">§</a></h2>
<p>My first idea was to override the <code>HeaderDelegate&lt;MediaType&gt;</code> with
our own implementation. I couldn’t find a general way to do that
via the JAX-RS API. It does seem that you can do it using RESTEasy
classes directly:</p>
<ol type="1">
<li>Implement the custom <code>HeaderDelegate&lt;MediaType&gt;</code>. To avoid
unnecessary work you could extend RESTEasy’s
<code>MediaTypeHeaderDelegate</code> and override just the
<code>toString(MediaType)</code> method.</li>
<li>Obtain <code>ResteasyProviderFactory.getInstance()</code>. Invoke
<code>.addHeaderDelegate(MediaType.class, customInst)</code> to replace the
<code>HeaderDelegate&lt;MediaType&gt;</code>.</li>
</ol>
<p>This approach has several disadvantages:</p>
<ul>
<li>Directly coupled to the RESTEasy implementation. May break if
RESTEasy implementation details change and will not work with
other JAX-RS implementations.</li>
<li>Need to implement a custom <code>HeaderDelegate&lt;MediaType&gt;</code> with the
“correct” serialisation behaviour.</li>
<li><strong>The “correct” serialisation behaviour might break <em>other</em> clients
with different bugs/quirks.</strong></li>
</ul>
<p>For these reasons I rejected the first idea and sought an approach
that avoids these disadvantages.</p>
<h2 id="idea-2-response-filter">Idea 2: response filter <a href="#idea-2-response-filter" class="section">§</a></h2>
<p>My next idea was to use a <em>response filter</em> to reformat the
<code>Content-Type</code> response header. The Servlet API defines the
<a href="https://docs.oracle.com/javaee/7/api/javax/ws/rs/container/ContainerResponseFilter.html"><code>ContainerResponseFilter</code></a> interface:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode java"><code class="sourceCode java"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">public</span> <span class="kw">interface</span> ContainerResponseFilter <span class="op">{</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">void</span> <span class="fu">filter</span><span class="op">(</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>      ContainerRequestContext requestContext<span class="op">,</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>      ContainerResponseContext responseContext<span class="op">)</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">throws</span> <span class="bu">IOException</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>The application applies each registered filter to each response,
before serialising and sending the response. At the time response
filters are applied, the <code>Content-Type</code> header value is a
<code>MediaType</code>. It has not yet been converted to a <code>String</code>.</p>
<p>A response filter can add, remove, or replace response headers.
Recall that headers are stored in a <code>MultivaluedMap&lt;String, Object&gt;</code>. This means that we can replace a <code>MediaType</code> value (whose
serialisation is determined by the <code>HeaderDelegate</code>) with a <code>String</code>
value (which will be written <em>as is</em>).</p>
<p>The <code>.equals</code> equality test for <code>MediaType</code> properly compares the
properties of the instance without regard to string representation.
As it should. This enables a succinct implementation where we:</p>
<ol type="1">
<li>Decalre <em>verbatim</em> <code>String</code> header values we want to see in the
response.</li>
<li>Parse those strings into <code>MediaType</code> values.</li>
<li>Match the <code>Content-Type</code> value in the response against parsed
values.</li>
<li>Replace matched header values with the corresponding <em>verbatim</em>
<code>String</code>.</li>
</ol>
<p>The implementation is straightforward:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode java"><code class="sourceCode java"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="at">@Provider</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="kw">public</span> <span class="kw">class</span> ReformatContentTypeResponseFilter</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">implements</span> ContainerResponseFilter <span class="op">{</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>  <span class="kw">private</span> <span class="dt">static</span> <span class="bu">String</span><span class="op">[]</span> verbatim <span class="op">=</span> <span class="op">{</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;application/pkcs7-mime; smime-type=certs-only&quot;</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>  <span class="op">};</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>  <span class="kw">private</span> <span class="dt">static</span> <span class="bu">HashMap</span><span class="op">&lt;</span>MediaType<span class="op">,</span> <span class="bu">String</span><span class="op">&gt;</span> substitutions <span class="op">=</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">new</span> <span class="bu">HashMap</span><span class="op">&lt;&gt;();</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>  <span class="dt">static</span> <span class="op">{</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="bu">String</span> s <span class="op">:</span> verbatim<span class="op">)</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>      substitutions<span class="op">.</span><span class="fu">put</span><span class="op">(</span>MediaType<span class="op">.</span><span class="fu">valueOf</span><span class="op">(</span>s<span class="op">),</span> s<span class="op">);</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">@Override</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>  <span class="kw">public</span> <span class="dt">void</span> <span class="fu">filter</span><span class="op">(</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>      ContainerRequestContext requestContext<span class="op">,</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>      ContainerResponseContext responseContext<span class="op">)</span> <span class="op">{</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    MultivaluedMap<span class="op">&lt;</span><span class="bu">String</span><span class="op">,</span> <span class="bu">Object</span><span class="op">&gt;</span> headers <span class="op">=</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>      responseContext<span class="op">.</span><span class="fu">getHeaders</span><span class="op">()</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">Object</span> v <span class="op">=</span> headers<span class="op">.</span><span class="fu">getFirst</span><span class="op">(</span>HttpHeaders<span class="op">.</span><span class="fu">CONTENT_TYPE</span><span class="op">);</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>v <span class="op">!=</span> <span class="kw">null</span> <span class="op">&amp;&amp;</span> v <span class="kw">instanceof</span> MediaType</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>        <span class="op">&amp;&amp;</span> substitutions<span class="op">.</span><span class="fu">containsKey</span><span class="op">(</span>v<span class="op">))</span> <span class="op">{</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>      headers<span class="op">.</span><span class="fu">putSingle</span><span class="op">(</span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>        HttpHeaders<span class="op">.</span><span class="fu">CONTENT_TYPE</span><span class="op">,</span> substitutions<span class="op">.</span><span class="fu">get</span><span class="op">(</span>v<span class="op">));</span></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>There is currently only one header value whose formatting I need to
precisely control. If we discover more, we only need to add the
desired string serialisation to the <code>verbatim</code> array.</p>
<p>We must consider the possible scenario of different clients with
different quirks. In that case, we could maintain separate
substitutions maps for each known problematic client. We would use
the <code>User-Agent</code> header, or other request characteristics, to
identify the client and select the corresponding substitution map
(if any). Hopefully this situation does not arise. But if it does,
the increase in complexity of the solution is tolerable.</p>
<p>This solution works well and avoids the disadvantages of my first
idea:</p>
<ul>
<li>Only uses official Servlet and JAX-RS classes and interfaces.
This solution will work across all JAX-RS implementations.</li>
<li>Does not (re)implement <code>MediaType</code> serialsation. You just declare
the exact string values you want to see in responses.</li>
<li>With a moderate increase in complexity, can handle different
clients with incompatible quriks.</li>
</ul>
<h2 id="conclusion">Conclusion <a href="#conclusion" class="section">§</a></h2>
<p>It’s unfortunate that this workaround was even necessary. But given
that it was, I’m happy with the solution. It is simple and portable
across Servlet and JAX-RS implementations.</p>
<p>The same approach could be used for controlling formatting of any
header value types, not just <code>Content-Type</code> / <code>MediaType</code>. I hope
that sharing this solution will help people who encounter similar
problems. At the very least, I hope that because of this post you
learned something about Servlet and JAX-RS response header
processing.</p>]]></summary>
</entry>
<entry>
    <title>Experimenting with ExternalDNS</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2022-03-24-k8s-external-dns.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2022-03-24-k8s-external-dns.html</id>
    <published>2022-03-24T00:00:00Z</published>
    <updated>2022-03-24T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="experimenting-with-externaldns">Experimenting with ExternalDNS</h1>
<p>DNS is a critical piece of the puzzle for exposing Kubernetes-hosted
applications to the Internet. Running the application means nothing
if you can’t get traffic to it. Keeping public DNS records in sync
with the deployed applications is important. The Kubernetes
<a href="https://github.com/kubernetes-sigs/external-dns">ExternalDNS</a> was developed for this purpose.</p>
<p>ExternalDNS exposes Kubernetes Services and Routes in by managing
records in external DNS providers. It <a href="https://github.com/kubernetes-sigs/external-dns/blob/570b51659fdc218281e3504a558a437178465f29/README.md#status-of-providers">supports many DNS
providers</a>, including the DNS services of the popular
cloud providers (AWS, Google Cloud, Azure, …).</p>
<p>I have been experimenting with ExternalDNS. My purpose is not only
to understand installation and basic usage, but also whether it can
meet the specific DNS requirements of FreeIPA, such as <code>SRV</code>
records. This post outlines my findings.</p>
<h2 id="operator-installation">Operator installation <a href="#operator-installation" class="section">§</a></h2>
<p>The <a href="https://github.com/kubernetes-sigs/external-dns">ExternalDNS</a> controller is a Kubernetes sub-project (or
SIG—<em>special interest group</em>). In the OpenShift ecosystem, the
<a href="https://github.com/openshift/external-dns-operator">ExternalDNS Operator</a> creates and manages ExternalDNS controller
instances defined by <em>custom resources</em> (CRs) of <code>kind: ExternalDNS</code>.</p>
<p>The ExternalDNS Operator is available as a <em>Tech Preview</em> in
OpenShift Container Platform 4.10. So, it is visible in the
<em>OperatorHub</em> catalogue out-of-the-box. The <a href="https://docs.openshift.com/container-platform/4.10/networking/external_dns_operator/nw-installing-external-dns-operator.html">official docs</a>
explain how to install the operator via the OperatorHub web console.
The instructions were easy to follow.</p>
<p>I prefer using the CLI where possible. The OperatorHub system is
complex but I eventually worked out what commands and objects are
needed to install the ExternalDNS Operator from the CLI.</p>
<p>First, create the <em>operand</em> namespaces and RBAC objects. The
operand namespace is where the ExternalDNS controllers (as opposed
to the ExternalDNS <em>Operator</em> controller) will live.</p>
<pre class="shell"><code>$ oc create ns external-dns
namespace/external-dns created

$ oc apply -f \
    https://raw.githubusercontent.com/openshift/external-dns-operator/release-0.1/config/rbac/extra-roles.yaml
role.rbac.authorization.k8s.io/external-dns-operator created
rolebinding.rbac.authorization.k8s.io/external-dns-operator created
clusterrole.rbac.authorization.k8s.io/external-dns created
clusterrolebinding.rbac.authorization.k8s.io/external-dns created</code></pre>
<p>Next, create the <code>external-dns-operator</code> namespace where the
operator itself shall live:</p>
<pre class="shell"><code>% oc create ns external-dns-operator
namespace/external-dns-operator created</code></pre>
<p>Finally create the OperatorGroup and OperatorHub Subscription
objects. Note the contents of <code>external-dns-operator.yaml</code>:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> operators.coreos.com/v1</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> OperatorGroup</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">generateName</span><span class="kw">:</span><span class="at"> external-dns-operator-</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">namespace</span><span class="kw">:</span><span class="at"> external-dns-operator</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">targetNamespaces</span><span class="kw">:</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> external-dns-operator</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="pp">---</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> operators.coreos.com/v1alpha1</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Subscription</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> external-dns-operator</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">namespace</span><span class="kw">:</span><span class="at"> external-dns-operator</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> external-dns-operator</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">source</span><span class="kw">:</span><span class="at"> redhat-operators</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">sourceNamespace</span><span class="kw">:</span><span class="at"> openshift-marketplace</span></span></code></pre></div>
<p>Create the objects:</p>
<pre class="shell"><code>% oc create -f external-dns-operator.yaml
operatorgroup.operators.coreos.com/external-dns-operator-8852w created
subscription.operators.coreos.com/external-dns-operator created</code></pre>
<p>After a short delay (~1 minute for me) the operator installation
should finish. Observe the various Kubernetes objects that
represent the running operator:</p>
<pre class="shell"><code>% oc get -n external-dns-operator all
NAME                                         READY   STATUS    RESTARTS      AGE
pod/external-dns-operator-594b465984-r2pc5   2/2     Running   2 (59s ago)   5m13s

NAME                                            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/external-dns-operator-metrics-service   ClusterIP   172.30.151.142   &lt;none&gt;        8443/TCP   5m15s
service/external-dns-operator-service           ClusterIP   172.30.210.21    &lt;none&gt;        9443/TCP   59s

NAME                                    READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/external-dns-operator   1/1     1            1           5m14s

NAME                                               DESIRED   CURRENT   READY   AGE
replicaset.apps/external-dns-operator-594b465984   1         1         1       5m15s</code></pre>
<h2 id="the-externaldns-custom-resource">The <code>ExternalDNS</code> custom resource <a href="#the-externaldns-custom-resource" class="section">§</a></h2>
<p>Now that the operator is installed, we can define an <code>ExternalDNS</code>
customer resource (CR). The operator creates an ExternalDNS
controller instance for each CR. Here is an example
(<code>externaldns-test.yaml</code>):</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> externaldns.olm.openshift.io/v1alpha1</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> ExternalDNS</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> test</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">domains</span><span class="kw">:</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> </span><span class="fu">filterType</span><span class="kw">:</span><span class="at"> Include </span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">matchType</span><span class="kw">:</span><span class="at"> Exact </span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">name</span><span class="kw">:</span><span class="at"> ci-ln-053y10k-72292.origin-ci-int-gce.dev.rhcloud.com</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">provider</span><span class="kw">:</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">type</span><span class="kw">:</span><span class="at"> GCP</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">source</span><span class="kw">:</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">type</span><span class="kw">:</span><span class="at"> Service</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">service</span><span class="kw">:</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">serviceType</span><span class="kw">:</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> LoadBalancer</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">labelFilter</span><span class="kw">:</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">matchLabels</span><span class="kw">:</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">app</span><span class="kw">:</span><span class="at"> echo</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">fqdnTemplate</span><span class="kw">:</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> </span><span class="st">&quot;{{.Name}}.ci-ln-053y10k-72292.origin-ci-int-gce.dev.rhcloud.com&quot;</span></span></code></pre></div>
<p>Breaking down the <code>spec</code>, we see the following fields:</p>
<ul>
<li><p><strong><code>domains</code></strong> gives a rule for which domains this <code>ExternalDNS</code>
controller must manage. In this case, any domain name with a
<em>suffix</em> matching the <code>name</code> subfield will match the rule.</p></li>
<li><p><strong><code>provider</code></strong> specifies the cloud provider—in this case GCP
(Google Cloud). For GCP there is nothing else to configure; the
controller will use the main cluster secret to authenticate to
Google Cloud.</p></li>
<li><p><strong><code>source</code></strong> specifies which kinds of objects the controller will
monitor to determine the DNS records to be created/managed. We
configure the controller to watch Service objects. Further
configuration is specified in subfields:</p>
<ul>
<li><p><strong><code>serviceType</code></strong> restricts the type(s) of Service objects to be
considered.</p></li>
<li><p><strong><code>labelFilter</code></strong> can be set to further restrict the set of
source objects by matching on the <code>label</code> field. In this
example, we only match Service objects with label <code>app: echo</code>.</p></li>
<li><p><strong><code>fqdnTemplate</code></strong> specifies how to derive the fully qualified
DNS name from the Service object.</p></li>
<li><p><strong><code>hostnameAnnotation</code></strong> can be set to <code>Allow</code> to allow the FQDN
to be specified via the
<code>external-dns.alpha.kubernetes.io/hostname</code> annotation on the
Service object. The default value is <code>Ignore</code>, in which case
<code>fqdnTemplate</code> is required.</p></li>
</ul></li>
</ul>
<p>Aside from <code>type: Service</code>, the <code>ExternalDNS</code> CR also recognises
<code>type: OpenShiftRoute</code>. This type uses <code>Route</code> objects as the
source, creating <code>CNAME</code> records to alias the FQDN derived from the
<code>Route</code> object to the canonical DNS name of the ingress controller.
This isn’t the behaviour I’m looking for, so the rest of this
article focuses on the behaviour for <code>Service</code> sources.</p>
<h2 id="creating-the-externaldns-controller">Creating the ExternalDNS controller <a href="#creating-the-externaldns-controller" class="section">§</a></h2>
<p>Now that we have defined an <code>ExternalDNS</code> custom resource, let’s
create it and see what happens. I would like to watch the logs of
the ExternalDNS Operator during this operation.</p>
<p>Earlier we saw that the name of the operator Pod is
<code>pod/external-dns-operator-594b465984-r2pc5</code>. This Pod has two
containers:</p>
<pre class="shell"><code>% oc get -o json -n external-dns-operator \
    pod/external-dns-operator-594b465984-r2pc5 \
    | jq &#39;.status.containerStatuses[].name&#39;
&quot;kube-rbac-proxy&quot;
&quot;operator&quot;</code></pre>
<p>The container named <code>operator</code> is the one we are interested in.
We can watch its log output like so:</p>
<pre class="shell"><code>% oc logs -n external-dns-operator --tail 2 --follow \
    external-dns-operator-594b465984-r2pc5 operator
2022-03-22T04:41:06.625Z        INFO    controller-runtime.manager.controller.external_dns_controller   Starting workers        {&quot;worker count&quot;: 1}
2022-03-22T04:41:06.626Z        INFO    controller-runtime.manager.controller.credentials_secret_controller     Starting workers        {&quot;worker count&quot;: 1}
... (waiting for more output)</code></pre>
<p>Now, in another terminal, create the <code>ExternalDNS</code> CR object:</p>
<pre class="shell"><code>% oc create -f externaldns-test.yaml
externaldns.externaldns.olm.openshift.io/test created</code></pre>
<p>Log output shows the ExternalDNS Operator responding to the
appearance of the <code>externaldns/test</code> CR:</p>
<pre><code>controller-runtime.webhook.webhooks     received request        {&quot;webhook&quot;: &quot;/validate-externaldns-olm-openshift-io-v1alpha1-externaldns&quot;, &quot;UID&quot;: &quot;cf2fb876-9ddd-45a8-88b8-5cc0344fb5cc&quot;, &quot;kind&quot;: &quot;externaldns.olm.openshift.io/v1alpha1, Kind=ExternalDNS&quot;, &quot;resource&quot;: {&quot;group&quot;:&quot;externaldns.olm.openshift.io&quot;,&quot;version&quot;:&quot;v1alpha1&quot;,&quot;resource&quot;:&quot;externaldnses&quot;}}
validating-webhook      validate create {&quot;name&quot;: &quot;test&quot;}
controller-runtime.webhook.webhooks     wrote response  {&quot;webhook&quot;: &quot;/validate-externaldns-olm-openshift-io-v1alpha1-externaldns&quot;, &quot;code&quot;: 200, &quot;reason&quot;: &quot;&quot;, &quot;UID&quot;: &quot;cf2fb876-9ddd-45a8-88b8-5cc0344fb5cc&quot;, &quot;allowed&quot;: true}
external_dns_controller reconciling externalDNS {&quot;externaldns&quot;: &quot;/test&quot;}
…</code></pre>
<p>And if we look in the <em>operand</em> namespace (<code>external-dns</code>) we see
a Pod running:</p>
<pre class="shell"><code>% oc get -n external-dns pod
NAME                                 READY   STATUS    RESTARTS   AGE
external-dns-test-865ffff756-45d44   1/1     Running   0          54s</code></pre>
<p>And if you want to see what an ExternalDNS <em>controller</em> is up to,
you can watch its logs:</p>
<pre class="shell"><code>% oc logs -n external-dns --tail 1 --follow \
    pod/external-dns-test-865ffff756-45d44
time=&quot;2022-03-23T12:26:18Z&quot; level=info msg=&quot;All records are already up to date&quot;
... (waiting for more output)</code></pre>
<h2 id="observing-record-creation">Observing record creation <a href="#observing-record-creation" class="section">§</a></h2>
<p>After creating the ExternalDNS instance, I found Google Cloud DNS
zone for my cluster and queried its records. How to interact with
the cloud provider depends on which cloud provider the cluster is
hosted on, so I won’t provide details. The existing records are:</p>
<pre><code>ci-ln-053y10k-72292.origin-ci-int-gce.dev.rhcloud.com.
  NS    21600  ns-gcp-private.googledomains.com.
ci-ln-053y10k-72292.origin-ci-int-gce.dev.rhcloud.com.
  SOA   21600  ns-gcp-private.googledomains.com.
api.ci-ln-053y10k-72292.origin-ci-int-gce.dev.rhcloud.com.
  A     60     10.0.0.2
api-int.ci-ln-053y10k-72292.origin-ci-int-gce.dev.rhcloud.com.
  A     60     10.0.0.2
*.apps.ci-ln-053y10k-72292.origin-ci-int-gce.dev.rhcloud.com.
  A     30     35.223.148.37</code></pre>
<div class="note">
<p>This is a <em>private</em> zone specific to my cluster. Some non-routable
addresses appear. I haven’t figured out how to update the records
in the public zone yet. I’m confident this is not a problem with
ExternalDNS. Rather, I put it down to my lack of familiarity with
how to configure it, and with Google Cloud DNS.</p>
</div>
<p>We can see that in addition to the expected <code>NS</code> and <code>SOA</code> records,
there are <code>A</code> records for the API server and a wildcard <code>A</code> record
for the main ingress controller.</p>
<p>Next I create the following Service:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Service</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> echo-tcp</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">labels</span><span class="kw">:</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">app</span><span class="kw">:</span><span class="at"> echo</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">type</span><span class="kw">:</span><span class="at"> LoadBalancer</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">selector</span><span class="kw">:</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">app</span><span class="kw">:</span><span class="at"> echo</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">ports</span><span class="kw">:</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> tcpecho</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">protocol</span><span class="kw">:</span><span class="at"> TCP</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">port</span><span class="kw">:</span><span class="at"> </span><span class="dv">12345</span></span></code></pre></div>
<p>Note that it has the <code>app: echo</code> label and has <code>type: LoadBalancer</code>,
satisfying the match criteria of the <code>externaldns/test</code> controller.
Create the service and observe its public IP address:</p>
<pre class="shell"><code>% oc create -f service-echo.yaml
service/echo-tcp created

% oc get service/echo-tcp \
    -o jsonpath=&#39;{.status.loadBalancer}&#39;
{&quot;ingress&quot;:[{&quot;ip&quot;:&quot;35.188.22.139&quot;}]}</code></pre>
<p>After creating the Service, two new records appeared in the zone:</p>
<pre><code>echo-tcp.ci-ln-053y10k-72292.origin-ci-int-gce.dev.rhcloud.com.
  A     300    35.188.22.139
external-dns-echo-tcp.ci-ln-053y10k-72292.origin-ci-int-gce.dev.rhcloud.com.
  TXT   300    &quot;heritage=external-dns,external-dns/owner=external-dns-test,external-dns/resource=service/test/echo-tcp&quot;</code></pre>
<p>The <code>A</code> record resolves the DNS name to the load balancer’s IP
address. Nothing surprising here.</p>
<p>The <code>TXT</code> record is the for the name <code>external-dns-echo-tcp.…</code> and
contains some metadata about the “owner” of the corresponding <code>A</code>
record. Specifically, it identifies the Service object that is the
<em>source</em> of the record. I am not 100% sure, but it seems to also
contain information about the ExternalDNS controller that created
the record.</p>
<p>When I first saw the TXT records, I theorised that the ExternalDNS
controller uses the TXT records to find “obsolete” records and
delete them. This would occur, for example, when the Service is
deleted. Indeed, deleting <code>service/echo-tcp</code> resulted in the
removal of both the <code>A</code> and <code>TXT</code> records.</p>
<h2 id="srv-records-for-loadbalancer-services">SRV records for <code>LoadBalancer</code> Services <a href="#srv-records-for-loadbalancer-services" class="section">§</a></h2>
<p>Kubernetes’ internal DNS system follows a <a href="https://github.com/kubernetes/dns/blob/master/docs/specification.md">DNS-based service
discovery</a> specification. In addition to <code>A</code>/<code>AAAA</code>
records, <code>SRV</code> records are created to locate service endpoints (port
and target DNS name) based on service name and transport protocol
(TCP or UDP). SRV records are an important part of several
protocols as used in the real world, including Kerberos, SIP, LDAP
and XMPP. <code>SRV</code> records have the following shape:</p>
<pre><code>_&lt;service&gt;._&lt;proto&gt;.&lt;domain&gt; &lt;ttl&gt;
    &lt;class&gt; SRV &lt;priority&gt; &lt;weight&gt; &lt;port&gt; &lt;target&gt;</code></pre>
<p>A record to locate an organisation’s LDAP server might look like:</p>
<pre><code>_ldap._tcp.example.net 300
    IN SRV 10 5 389 ldap.corp.example.net</code></pre>
<p>Although the current system has a critical deficiency for
applications that use SRV records and operate on both TCP and UDP
(see my <a href="2020-12-08-k8s-srv-limitation.html">previous blog post</a>)
for most applications it works well. Unfortunately, ExternalDNS
does not follow the DNS spec and does not create SRV records for
Services.</p>
<p>I am not sure why this is the case. Perhaps ExternalDNS even
pre-dates the SRV aspects of the Kubernetes DNS specification. Or
the need might not have been recognised or deemed sufficiently
critical to address this gap.</p>
<p>As it happens, there is <a href="https://github.com/kubernetes-sigs/external-dns/pull/1330">an abandoned pull request</a> from two years
ago that sought to add SRV record generation to ExternalDNS and
bring it in line with the spec. The maintainers seemed receptive,
but the PR author no longer needed the feature and closed it. So I
think there is reason to hope that the feature might eventually make
it into ExternalDNS. Perhaps our team will drive it… we need SRV
records, and it would probably be better to enhance ExternalDNS than
to build our own solution from scratch.</p>
<h2 id="srv-records-for-nodeport-services">SRV records for <code>NodePort</code> services <a href="#srv-records-for-nodeport-services" class="section">§</a></h2>
<p>I said that ExternalDNS does not support SRV records, but there is
one exception to that. ExternalDNS <em>does</em> create SRV records for
Services of <code>type: NodePort</code>. This is not an appropriate solution
for our application, but we can still play with it and get a feel
for how it might work similarly for <code>LoadBalancer</code> Services.</p>
<p>First, we have to modify <code>externaldns/test</code> to add <code>NodePort</code> to the
list of Service types. Update <code>externaldns-test.yaml</code>:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="at">…</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">service</span><span class="kw">:</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">serviceType</span><span class="kw">:</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> LoadBalancer</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> NodePort</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="at">…</span></span></code></pre></div>
<p>And apply updated configuration:</p>
<pre class="shell"><code>% oc replace -f externaldns-test.yaml
externaldns.externaldns.olm.openshift.io/test replaced</code></pre>
<p>Now create a new <code>NodePort</code> Service. <code>service-nodeport.yaml</code>:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Service</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> nodeport</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">labels</span><span class="kw">:</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">app</span><span class="kw">:</span><span class="at"> echo</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">type</span><span class="kw">:</span><span class="at"> NodePort</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">selector</span><span class="kw">:</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">app</span><span class="kw">:</span><span class="at"> echo</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">ports</span><span class="kw">:</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> nodeport</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">protocol</span><span class="kw">:</span><span class="at"> TCP</span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">port</span><span class="kw">:</span><span class="at"> </span><span class="dv">12345</span></span></code></pre></div>
<pre class="shell"><code>% oc create -f service-nodeport.yaml
service/nodeport created</code></pre>
<p>The ExternalDNS controller log output shows it generating an <code>SRV</code>
record for the Service (wrapped for clarity):</p>
<pre><code>…
time=&quot;…&quot; level=debug msg=&quot;Endpoints generated from service:
default/nodeport:
[ _nodeport._tcp.nodeport.ci-ln-8hkfrzk-72292.origin-ci-int-gce.dev.rhcloud.com 0
    IN SRV  0 50 30632
    nodeport.ci-ln-8hkfrzk-72292.origin-ci-int-gce.dev.rhcloud.com []
  nodeport.ci-ln-8hkfrzk-72292.origin-ci-int-gce.dev.rhcloud.com 0
    IN A  10.0.0.4;10.0.0.5;10.0.128.3;10.0.128.2;10.0.128.4;10.0.0.3 []
]&quot;
…</code></pre>
<p>Unfortunately, the <code>SRV</code> record didn’t actually make it to the
Google Cloud DNS zone. I haven’t worked out why, yet. The <code>A</code>
record does get created; it’s only the <code>SRV</code> record that is missing.
I’ll update this article if/when I work out why the <code>SRV</code> record
goes.</p>
<h2 id="conclusion">Conclusion <a href="#conclusion" class="section">§</a></h2>
<p>The ExternalDNS system is intended to automatically manage public
DNS records for Kubernetes-hosted applications. It can
automatically create <code>CNAME</code> records for OpenShift Routes and
<code>A</code>/<code>AAAA</code> records for Services, including <code>LoadBalancer</code> services.
For applications that use <code>A</code>/<code>AAAA</code> and <code>CNAME</code> records, it works
well.</p>
<p>Unfortunately, <code>SRV</code> records are not well supported. Certainly, it
does not meet the needs of typical applications that use <code>SRV</code>
records. Operators of such applications currently have one of two
options: either manage the records manually (do not want), or
implement the required automation yourselves (e.g. in the
application’s <em>operator</em> program).</p>
<p>The best way forward is to implement better support for <code>SRV</code>
records in ExternalDNS itself, so everyone can benefit through
shared effort and maintainership vested in the Kubernetes SIG. I
shall file a ticket and perhaps restart discussions in the
<a href="https://github.com/kubernetes-sigs/external-dns/pull/1330">abandoned pull request</a> with a view to getting this
critical feature on the ExternalDNS roadmap. The extent of
involvement of myself or my team in implementing or driving this
feature work will be determined later.</p>]]></summary>
</entry>
<entry>
    <title>Running Pods in user namespaces without privileged SCCs</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2022-02-02-openshift-user-ns-without-anyuid.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2022-02-02-openshift-user-ns-without-anyuid.html</id>
    <published>2022-02-02T00:00:00Z</published>
    <updated>2022-02-02T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="running-pods-in-user-namespaces-without-privileged-sccs">Running Pods in user namespaces without privileged SCCs</h1>
<p>In <a href="2021-07-22-openshift-systemd-workload-demo.html">previous posts</a> I demonstrated how to run workloads in an
isolated user namespace on OpenShift. There are still come caveats
to doing this. One of these relates to <em>Security Context
Constraints (SCCs)</em>, a security policy mechanism in OpenShift. In
particular, it appeared necessary to admit the Pod via the <code>anyuid</code>
SCC, or one with similar high privileges. This meant that although
the workload itself runs under unprivileged UIDs, the account that
creates the Pod would need privileges to create Pods that run under
arbitrary host UIDs. This is not a desirable situation.</p>
<p>I have investigated that matter further, and it turns out that you
<em>can</em> run a workload in a user namespace even via the default
<code>restricted</code> SCC. But the configuration is not intuitive, and the
reasons <em>why</em> it must be configured that way are convoluted. In
this post I explain the challenges that arise when running a user
namespaced Pod under the <code>restricted</code> SCC, and demonstrate the
solution.</p>
<div class="note">
<p>This post assumes a basic knowledge of Security Context Constraints.
If you are unfamiliar with SCCs, the DevConf.cz 2022 presentation
<em>Introduction to Security Context Constraints</em> (<a href="https://static.sched.com/hosted_files/devconfcz2022/d5/%5BDevConf.CZ%2022%5D%20SCCs%20Presentation.pdf">slides</a>,
<a href="https://www.youtube.com/watch?v=MrYSUmk-nr4">video</a>) by Alberto Losada and Mario Vázquez will bring you up to
speed.</p>
</div>
<h2 id="cluster-configuration">Cluster configuration <a href="#cluster-configuration" class="section">§</a></h2>
<p>I am testing on an OpenShift 4.10 (pre-release) cluster. Some
changes to worker node configuration are required. The following
<code>MachineConfig</code> object defines those changes:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> machineconfiguration.openshift.io/v1</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> MachineConfig</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">labels</span><span class="kw">:</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">machineconfiguration.openshift.io/role</span><span class="kw">:</span><span class="at"> worker</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> idm-4-10</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">kernelArguments</span><span class="kw">:</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> systemd.unified_cgroup_hierarchy=1</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> cgroup_no_v1=&quot;all&quot;</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> psi=1</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">config</span><span class="kw">:</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">ignition</span><span class="kw">:</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">version</span><span class="kw">:</span><span class="at"> </span><span class="fl">3.1.0</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">systemd</span><span class="kw">:</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">units</span><span class="kw">:</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;override-runc.service&quot;</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">enabled</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="fu">        contents</span><span class="kw">: </span><span class="ch">|</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>          [Unit]</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>          Description=Install runc override</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>          After=network-online.target rpm-ostreed.service</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>          [Service]</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>          ExecStart=/bin/sh -c &#39;rpm -q runc-1.0.3-992.rhaos4.10.el8.x86_64 || rpm-ostree override replace --reboot https://ftweedal.fedorapeople.org/runc-1.0.3-992.rhaos4.10.el8.x86_64.rpm&#39;</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>          Restart=on-failure</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>          [Install]</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>          WantedBy=multi-user.target</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">storage</span><span class="kw">:</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">files</span><span class="kw">:</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> </span><span class="fu">path</span><span class="kw">:</span><span class="at"> /etc/subuid</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">overwrite</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">contents</span><span class="kw">:</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">source</span><span class="kw">:</span><span class="at"> data:text/plain;charset=utf-8;base64,Y29yZToxMDAwMDA6NjU1MzYKY29udGFpbmVyczoyMDAwMDA6MjY4NDM1NDU2Cg==</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> </span><span class="fu">path</span><span class="kw">:</span><span class="at"> /etc/subgid</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">overwrite</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">contents</span><span class="kw">:</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">source</span><span class="kw">:</span><span class="at"> data:text/plain;charset=utf-8;base64,Y29yZToxMDAwMDA6NjU1MzYKY29udGFpbmVyczoyMDAwMDA6MjY4NDM1NDU2Cg==</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> </span><span class="fu">path</span><span class="kw">:</span><span class="at"> /etc/crio/crio.conf.d/99-crio-userns.conf</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">overwrite</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">contents</span><span class="kw">:</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">source</span><span class="kw">:</span><span class="at"> data:text/plain;charset=utf-8;base64,W2NyaW8ucnVudGltZS53b3JrbG9hZHMub3BlbnNoaWZ0LXVzZXJuc10KYWN0aXZhdGlvbl9hbm5vdGF0aW9uID0gImlvLm9wZW5zaGlmdC51c2VybnMiCmFsbG93ZWRfYW5ub3RhdGlvbnMgPSBbCiAgImlvLmt1YmVybmV0ZXMuY3JpLW8udXNlcm5zLW1vZGUiLAogICJpby5rdWJlcm5ldGVzLmNyaS1vLmNncm91cDItbW91bnQtaGllcmFyY2h5LXJ3IiwKICAiaW8ua3ViZXJuZXRlcy5jcmktby5EZXZpY2VzIgpdCg==</span></span></code></pre></div>
<p>The main parts of this <code>MachineConfig</code> are:</p>
<ul>
<li><p>The <strong><code>kernelArguments</code></strong> enable cgroupsv2, which are not strictly
required for this demo, but are required for running systemd-based
workloads.</p></li>
<li><p>The <strong><code>override-runc.service</code></strong> systemd unit installs a custom
version of runc that implements the new <a href="https://github.com/opencontainers/runtime-spec/blob/8958f93039ab90be53d803cd7e231a775f644451/config-linux.md#cgroup-ownership">OCI Runtime Specification
cgroup ownership semantics</a>.
This should be the default behaviour in future versions of
OpenShift, perhaps as soon as OpenShift 4.11.</p></li>
<li><p><strong><code>/etc/subuid</code></strong> and <strong><code>/etc/subgid</code></strong> provide a sub-id mapping range
for CRI-O to use when creating Pods with user namespaces.</p></li>
<li><p><strong><code>/etc/crio/crio.conf.d/99-crio-userns.conf</code></strong> defines the
<code>io.openshift.userns</code> workload type for CRI-O. It is also not
strictly necessary for this demo but is required for systemd-based
workloads to run successfully. The default CRI-O configuration in
OpenShift 4.10 provides the <code>io.openshift.builder</code> workload type,
which is sufficient if your workload does not need to manage
cgroups.</p></li>
</ul>
<p>Aside from the node configuration changes, I (as cluster admin) also
created project and user account to use for the subsequent steps:</p>
<pre class="shell"><code>% oc new-project test
Now using project &quot;test&quot; on server &quot;https://api.ci-ln-5rkyxfb-72292.origin-ci-int-gce.dev.rhcloud.com:6443&quot;.
…

% oc create user test
user.user.openshift.io/test created

% oc adm policy add-role-to-user edit test
clusterrole.rbac.authorization.k8s.io/edit added: &quot;test&quot;</code></pre>
<p>I did not assign any special SCCs to the <code>test</code> user account.</p>
<div class="note">
<p>Remember to wait for the Machine Config Operator to finish updating
the worker nodes before proceeding with Pod creation. You can use
<code>oc wait</code> to await this condition:</p>
<pre class="shell"><code>% oc wait mcp/worker \
    --for condition=updated --timeout=-1s</code></pre>
</div>
<h2 id="problem-demonstration">Problem demonstration <a href="#problem-demonstration" class="section">§</a></h2>
<p>The objective is to run a Pod in a user namespace, with that Pod
being admitted via the default <code>restricted</code> SCC. We will start with
the following Pod definition:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> fedora</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">annotations</span><span class="kw">:</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">io.openshift.userns</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;true&quot;</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">io.kubernetes.cri-o.userns-mode</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;auto:size=65536&quot;</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> fedora</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">image</span><span class="kw">:</span><span class="at"> registry.fedoraproject.org/fedora:35-x86_64</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">command</span><span class="kw">:</span><span class="at"> </span><span class="kw">[</span><span class="st">&quot;sleep&quot;</span><span class="kw">,</span><span class="at"> </span><span class="st">&quot;3600&quot;</span><span class="kw">]</span></span></code></pre></div>
<p>The <strong><code>io.openshift.userns</code></strong> annotation selects the CRI-O workload
profile that we added via the <code>MachineConfig</code> above. This profile
enables several other annotations, but does not automatically
execute the Pod in a user namespace. For that, you must <em>also</em>
supply the <strong><code>io.kubernetes.cri-o.userns-mode</code></strong> annotation. Its
argument tells CRI-O to automatically select unique host UID range
of size 65536 to map into the container’s user namespace.</p>
<p>I created the Pod as user <code>test</code>:</p>
<pre class="shell"><code>% oc --as test create -f pod-fedora.yaml
pod/fedora created</code></pre>
<p>Observe that it was admitted via the <code>restricted</code> SCC:</p>
<pre class="shell"><code>% oc get -o json pod/fedora \
    | jq &#39;.metadata.annotations.&quot;openshift.io/scc&quot;&#39;
&quot;restricted&quot;</code></pre>
<p>Unfortunately, the container is not running:</p>
<pre class="shell"><code>% oc get -o json pod/fedora \
  | jq &#39;.status.containerStatuses[].state&#39;
{
  &quot;waiting&quot;: {
    &quot;message&quot;: &quot;container create failed: time=\&quot;2022-02-02T05:43:34Z\&quot; level=error msg=\&quot;container_linux.go:380: starting container process caused: setup user: cannot set uid to unmapped user in user namespace\&quot;\n&quot;,
    &quot;reason&quot;: &quot;CreateContainerError&quot;
  }
}</code></pre>
<p>The core error message is: <strong><em>cannot set uid to unmapped user in
user namespace</em></strong>. This arises because, in the absense of a
<code>runAsUser</code> specification in the PodSpec, the <code>restricted</code> SCC has
defaulted it to a value from the UID range assigned to the project:</p>
<pre class="shell"><code>% oc get -o json pod/fedora \
  | jq &#39;.spec.containers[].securityContext.runAsUser&#39;
1000650000</code></pre>
<p>The project UID range allocation is recorded in the project and
namespace annotations:</p>
<pre class="shell"><code>% oc get -o json project/test namespace/test \
    | jq &#39;.items[].metadata.annotations.&quot;openshift.io/sa.scc.uid-range&quot;&#39;
&quot;1000650000/10000&quot;
&quot;1000650000/10000&quot;</code></pre>
<p>OpenShift allocated to project <code>test</code> a range of 10000 UIDs starting
at <code>1000650000</code>. The error arises because UID <code>1000650000</code> is not
mapped in the user namespace. The host UID range may be something
like <code>200000</code>–<code>265535</code>, whereas the sandbox’s UID range is
<code>0</code>–<code>65535</code>.</p>
<p>I deleted the Pod and will try something different:</p>
<pre class="shell"><code>% oc delete pod/fedora
pod &quot;fedora&quot; deleted</code></pre>
<p>Let’s say that we want to run the container process as UID <code>0</code> <em>in
the Pod’s user namespace</em>, as would be required for a systemd-based
workload. Instead of leaving it to the SCC machinery, I’ll set
<code>runAsUser: 0</code> in the PodSpec myself:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> fedora</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">annotations</span><span class="kw">:</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">io.openshift.userns</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;true&quot;</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">io.kubernetes.cri-o.userns-mode</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;auto:size=65536&quot;</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> fedora</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">image</span><span class="kw">:</span><span class="at"> registry.fedoraproject.org/fedora:35-x86_64</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">command</span><span class="kw">:</span><span class="at"> </span><span class="kw">[</span><span class="st">&quot;sleep&quot;</span><span class="kw">,</span><span class="at"> </span><span class="st">&quot;3600&quot;</span><span class="kw">]</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">securityContext</span><span class="kw">:</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">runAsUser</span><span class="kw">:</span><span class="at"> </span><span class="dv">0</span></span></code></pre></div>
<p>This time the <code>test</code> user cannot even create the Pod:</p>
<pre class="shell"><code>% oc --as test create -f pod-fedora.yaml
Error from server (Forbidden): error when creating &quot;pod-fedora.yaml&quot;…</code></pre>
<p>I’ve trimmed the rather long error message, but the core problem is:</p>
<pre><code>spec.containers[0].securityContext.runAsUser: Invalid value:
0: must be in the ranges: [1000650000, 1000659999]</code></pre>
<p>The <code>restricted</code> SCC only allows <code>runAsUser</code> values that fall in the
projects assigned UID range. And this is what we would expect. The
problem is that the admission machinery has no awareness of user
namespaces. It cannot discern that <code>runAsUser: 0</code> means that we
want to run as UID <code>0</code> <em>inside the user namespace</em>, whilst mapped to
an unprivileged UID on the host.</p>
<p>The problem is twofold. First, we are unable to control the UID
mapping that CRI-O gives us, so that it would coincide with the
project’s UID range. Second, the SCC admission checks and
defaulting is oblivious to user namespace. <code>runAsUser</code> is
interpreted as referring to host UIDs, and the <code>restricted</code> SCC
restricts (or defaults) us to values that are not mapped in the
Pod’s user namespace.</p>
<h2 id="solution">Solution <a href="#solution" class="section">§</a></h2>
<p>The <code>map-to-root</code> option in the <code>userns-mode</code> annotation provides a
solution to this dilemma. It takes whatever value <code>runAsUser</code> is,
and ensures that that host UID gets mapped to UID <code>0</code> in the Pod
user namespace. The updated PodSpec is:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> fedora</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">annotations</span><span class="kw">:</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">io.openshift.userns</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;true&quot;</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">io.kubernetes.cri-o.userns-mode</span><span class="kw">:</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="st">&quot;auto:size=65536;map-to-root=true&quot;</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">securityContext</span><span class="kw">:</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">runAsUser</span><span class="kw">:</span><span class="at"> </span><span class="dv">1000650000</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> fedora</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">image</span><span class="kw">:</span><span class="at"> registry.fedoraproject.org/fedora:35-x86_64</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">command</span><span class="kw">:</span><span class="at"> </span><span class="kw">[</span><span class="st">&quot;sleep&quot;</span><span class="kw">,</span><span class="at"> </span><span class="st">&quot;3600&quot;</span><span class="kw">]</span></span></code></pre></div>
<p>Now the Pod is able to run:</p>
<pre class="shell"><code>% oc --as test create -f pod-fedora.yaml
pod/fedora created

% oc get -o json pod/fedora \
  | jq &#39;.spec.nodeName, .status.containerStatuses[].state&#39;
&quot;ci-ln-fizz88k-72292-9phfc-worker-c-7s99v&quot;
{
  &quot;running&quot;: {
    &quot;startedAt&quot;: &quot;2022-02-02T06:20:49Z&quot;
  }
}</code></pre>
<p>We can observe the UID mapping:</p>
<pre class="shell"><code>% oc rsh pod/fedora cat /proc/self/uid_map
         1     265536      65535
         0 1000650000          1</code></pre>
<p>This shows that UID <code>0</code> in the Pod’s user namespace maps to UID
<code>10000650000</code> in the parent (host) user namespace. The remaining
UIDs <code>1</code>–<code>65536</code> in the Pod’s user namespace are mapped contiguously
from UID <code>265536</code> in the host user namespace.</p>
<p>Objective achieved.</p>
<h3 id="why-runasuser-must-be-specified">Why <code>runAsUser</code> must be specified <a href="#why-runasuser-must-be-specified" class="section">§</a></h3>
<p>Referring back to the PodSpec, why is it necessary to explicitly
specify <code>runAsUser</code>? Doesn’t the SCC admission machinery
automatically set the default value? Well… yes, and no. The SCC
machinery defaults <code>runAsUser</code> in each <em>container’s</em>
<code>securityContext</code> field. But it does not set it in the <em>Pod’s</em>
<code>securityContext</code>. And it is the <em>Pod</em> <code>securityContext</code> that CRI-O
examines when processing the <code>map-to-root</code> option. If it is unset,
<code>CRI-O</code> will not set the mapping up properly and container(s) will
fail to run.</p>
<p>The consequence of this is that the user or operator creating the
Pod must first examine the Project or Namespace object to learn what
its assigned UID range is. Then it must set the
<code>spec.securityContext.runAsUser</code> field to the start value of that
range. The range assignment will certainly differ from project to
project so it cannot be hardcoded. This is a bit annoying: more
work for the human operator, or more automation behaviour to
implement and maintain.</p>
<p>The simplest solution I can think of is to enhance the SCC
processing to also set <code>spec.securityContext.runAsUser</code> if it is
unset. Then CRI-O would see the value it needs to see.
Alternatively CRI-O could be enhanced to check the container
<code>securityContext</code> if the <code>runAsUser</code> is not specified in the Pod
<code>securityContext</code>. But to me this seems ill principled because
different containers (in the same Pod) could specify different
values, and there is no obvious “right” way to resolve the
ambiguities.</p>
<h2 id="using-multiple-uids">Using multiple UIDs <a href="#using-multiple-uids" class="section">§</a></h2>
<p>Although I have a nice range of 65536 UIDs mapped in the Pod’s user
namespace, I am not able to run processes as any UID other than <code>0</code>.
This is beacuse the <code>restricted</code> SCC forcibly omits <code>CAP_SETUID</code>
(among others) from the capability bounding set of the container
process. Complex workloads, including any based on systemd, will
fail to run properly under such a constraint.</p>
<p>The simplest workaround is to admit the Pod via the <code>anyuid</code> SCC.
But that undoes the good outcome achieved in this post!</p>
<p>An intermediate workaround is the create a new SCC that does not
forcibly deprive containers of <code>CAP_SETUID</code>. This entails
administrative overhead.</p>
<p>It also increases the attack surface. The <code>setuid(2)</code> system call
is restricted to UIDs mapped in the UID namespace of the calling
process. If the calling process is in an isolated user namespace
that maps to unprivileged host UIDs, it is safe (up to kernel bugs)
to grant <code>CAP_SETUID</code> to that process. But recall that user
namespaces are still opt-in; by default Pods use the host user
namespace. An SCC can use <code>MustRunAsRange</code> to restrict the
<em>initial</em> container process to running as a user in the project’s
assigned UID range. But if that SCC also lets containers use
<code>CAP_SETUID</code>, then it doesn’t really provide more protection than
<code>anyuid</code></p>
<p>A more robust solution would be to modify CRI-O to <em>reinstate</em>
<code>CAP_SETUID</code> and related capapbilities when the Pod runs in a user
namespace. I will raise the topic with the CRI-O maintainers, as
solving this problem is important for our use case, and probably
other “legacy” workloads too.</p>
<h2 id="conclusion">Conclusion <a href="#conclusion" class="section">§</a></h2>
<p>In this post I demonstrated how to run workloads in a user namespace
on OpenShift, under the default <code>restricted</code> SCC. The <code>map-to-root</code>
option is critical to accomplishing this. There is an unfortunate
“rough edge” in that the workload must specifically refer to the UID
range assigned to the namespace in which the Pod will live, which
means additional work for or complexity in the operator (human or
otherwise).</p>
<p>Despite this progress, if you need to run processes under different
UIDs in the container(s), the <code>restricted</code> UID won’t work because it
deprives the container process of the <code>CAP_SETUID</code> capability. You
must go back to admitting the workload via <code>anyuid</code> or a similar
SCC, which is a significant erosion of the security boundaries
between containers and the host. This issue will be the subject of
future investigations.</p>]]></summary>
</entry>
<entry>
    <title>Bare TCP and UDP ingress on Kubernetes</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2021-11-18-k8s-tcp-udp-ingress.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2021-11-18-k8s-tcp-udp-ingress.html</id>
    <published>2021-11-18T00:00:00Z</published>
    <updated>2021-11-18T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="bare-tcp-and-udp-ingress-on-kubernetes">Bare TCP and UDP ingress on Kubernetes</h1>
<p>Kubernetes and OpenShift have good solutions for routing HTTP/HTTPS
traffic to the right applications. But for ingress of bare TCP
(that is, not HTTP(S) or TLS with SNI) or UDP traffic, the situation
is more complicated. In this post I demonstrate how to use
<code>LoadBalancer</code> Service objects to route bare TCP and UDP traffic to
your Kubernetes applications.</p>
<h2 id="example-service">Example service <a href="#example-service" class="section">§</a></h2>
<p>For testing purposes I wrote a basic echo server. It listens on
both TCP and UDP port 12345, and merely upper-cases and returns the
data it receives:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> socketserver</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> threading</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> serve_tcp():</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">class</span> Handler(socketserver.StreamRequestHandler):</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> handle(<span class="va">self</span>):</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>                data <span class="op">=</span> <span class="va">self</span>.rfile.readline()</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="kw">not</span> data:</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">break</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.wfile.write(data.upper())</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> socketserver.TCPServer((<span class="st">&#39;&#39;</span>, <span class="dv">12345</span>), Handler) <span class="im">as</span> server:</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        server.serve_forever()</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> serve_udp():</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">class</span> Handler(socketserver.DatagramRequestHandler):</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> handle(<span class="va">self</span>):</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.wfile.write(<span class="va">self</span>.rfile.read().upper())</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> socketserver.UDPServer((<span class="st">&#39;&#39;</span>, <span class="dv">12345</span>), Handler) <span class="im">as</span> server:</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        server.serve_forever()</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&quot;__main__&quot;</span>:</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    threading.Thread(target<span class="op">=</span>serve_tcp).start()</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    threading.Thread(target<span class="op">=</span>serve_udp).start()</span></code></pre></div>
<p>The <code>Containerfile</code> adds this program to the official Fedora 35
container and declares the entry point:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode dockerfile"><code class="sourceCode dockerfile"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">FROM</span> fedora:35-x86_64</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="kw">COPY</span> echo.py .</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="kw">CMD</span> [ <span class="st">&quot;python3&quot;</span>, <span class="st">&quot;echo.py&quot;</span> ]</span></code></pre></div>
<p>I published the container <a href="https://quay.io/repository/ftweedal/udpecho.">image on Quay.io</a>. The Pod spec
references it:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> echo</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">labels</span><span class="kw">:</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">app</span><span class="kw">:</span><span class="at"> echo</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> server</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">image</span><span class="kw">:</span><span class="at"> quay.io/ftweedal/udpecho:latest</span></span></code></pre></div>
<p>I defined a new project namespace <code>echo</code> and created the Pod:</p>
<pre class="shell"><code>% oc new-project echo
Now using project &quot;echo&quot; on server
  &quot;https://api.ci-ln-4ixdypb-72292.origin-ci-int-gce.dev.rhcloud.com:6443&quot;.

…

% oc create -f pod-echo.yaml
pod/echo created</code></pre>
<h2 id="create-service-object">Create Service object <a href="#create-service-object" class="section">§</a></h2>
<p>My application is not talking HTTP, so I can’t use the normal
Ingress or Route facilities to get traffic to my app.</p>
<div class="note">
<p>HTTP and HTTPS traffic includes the <strong><code>Host</code></strong> header, which the
ingress system can inspect to route requests to a particular Pod.
Similarly, TLS with the <strong><em>Server Name (SNI)</em></strong> extension allows TLS
traffic to be routed to a particular Pod (the Pod will perform the
handshake). Neither approach works for UDP packets or “bare” TCP
connections.</p>
</div>
<p>Therefore, I define a <code>LoadBalancer</code> Service. The service
controller will ask the cloud provider to create a load balancer
that routes external traffic into the cluster. For example, on AWS
it will (by default) create an ELB (<em>Elastic Load Balancer</em>)
instance.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Service</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> echo</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">type</span><span class="kw">:</span><span class="at"> LoadBalancer</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">selector</span><span class="kw">:</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">app</span><span class="kw">:</span><span class="at"> echo</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">ports</span><span class="kw">:</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> tcpecho</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">protocol</span><span class="kw">:</span><span class="at"> TCP</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">port</span><span class="kw">:</span><span class="at"> </span><span class="dv">12345</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> udpecho</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">protocol</span><span class="kw">:</span><span class="at"> UDP</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">port</span><span class="kw">:</span><span class="at"> </span><span class="dv">12345</span></span></code></pre></div>
<p>OK, let’s create the Service:</p>
<pre class="shell"><code>% oc create -f service-echo.yaml 
The Service &quot;echo&quot; is invalid: spec.ports: Invalid value:
[]core.ServicePort{core.ServicePort{Name:&quot;tcpecho&quot;, Protocol:&quot;TCP&quot;,
AppProtocol:(*string)(nil), Port:12345,
TargetPort:intstr.IntOrString{Type:0, IntVal:12345, StrVal:&quot;&quot;},
NodePort:0}, core.ServicePort{Name:&quot;udpecho&quot;, Protocol:&quot;UDP&quot;,
AppProtocol:(*string)(nil), Port:12345,
TargetPort:intstr.IntOrString{Type:0, IntVal:12345, StrVal:&quot;&quot;},
NodePort:0}}: may not contain more than 1 protocol when type is
&#39;LoadBalancer&#39;</code></pre>
<p>Well, that’s unfortunate. Kubernetes does not support
<code>LoadBalancer</code> services with mixed <code>protocol</code>. <a href="https://github.com/kubernetes/enhancements/issues/1435">KEP 1435</a> is in
progress to address this. It is a gated “alpha” feature <a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md">since
Kubernetes 1.20</a>. Cloud provider support is
currently <a href="https://github.com/kubernetes/enhancements/issues/1435#issuecomment-969523031">mixed</a> but work is ongoing.</p>
<p>So for now, I have to create separate Service objects for UDP and
TCP ingress. As a consequence, there will be <strong>different public IP
addresses for TCP and UDP</strong>. Whether this is a problem depends on
the application. Applications that use <code>SRV</code> records to locate
servers can handle this scenario. Kerberos is such an application
(modern implementations, at least). Applications that use <code>A</code> or
<code>AAAA</code> records directly might have problems.</p>
<p>The other downside is cost. Cloud providers charge money for load
balancer instances. The more you use, the more you pay.</p>
<p>Below is the definition of my decomposed Service objects:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Service</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> echo-udp</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">type</span><span class="kw">:</span><span class="at"> LoadBalancer</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">selector</span><span class="kw">:</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">app</span><span class="kw">:</span><span class="at"> echo</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">ports</span><span class="kw">:</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> udpecho</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">protocol</span><span class="kw">:</span><span class="at"> UDP</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">port</span><span class="kw">:</span><span class="at"> </span><span class="dv">12345</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="pp">---</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Service</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> echo-tcp</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">type</span><span class="kw">:</span><span class="at"> LoadBalancer</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">selector</span><span class="kw">:</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">app</span><span class="kw">:</span><span class="at"> echo</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">ports</span><span class="kw">:</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> tcpecho</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">protocol</span><span class="kw">:</span><span class="at"> TCP</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">port</span><span class="kw">:</span><span class="at"> </span><span class="dv">12345</span></span></code></pre></div>
<p>Creating the objects now succeeds:</p>
<pre class="shell"><code>% oc create -f service-echo.yaml 
service/echo-udp created
service/echo-tcp created</code></pre>
<p>To find out the hostname or IP address of the load balancer ingress
endpoint, inspect the <code>status</code> field of the Service object:</p>
<pre class="shell"><code>% oc get -o json service \
    | jq -c &#39;.items[] | (.metadata.name, .status)&#39;
&quot;echo-tcp&quot;
{&quot;loadBalancer&quot;:{&quot;ingress&quot;:[{&quot;ip&quot;:&quot;34.136.55.93&quot;}]}}
&quot;echo-udp&quot;
{&quot;loadBalancer&quot;:{&quot;ingress&quot;:[{&quot;ip&quot;:&quot;34.71.82.205&quot;}]}}</code></pre>
<p>Most cloud providers report an IP address. That includes Google
Cloud (GCP) where this cluster was deployed. On the other hand, AWS
reports a DNS name. Below is the result of creating my service
objects on an cluster hosted on AWS:</p>
<pre class="shell"><code>% oc get -o json service \
    | jq -c &#39;.items[] | (.metadata.name, .status)&#39;
&quot;echo-tcp&quot;
{&quot;loadBalancer&quot;:{&quot;ingress&quot;:[{&quot;hostname&quot;:&quot;a095e8e1ebb9e4c64ae71e0f3c688ad4-608097611.us-east-2.elb.amazonaws.com&quot;}]}}
&quot;echo-udp&quot;
{&quot;loadBalancer&quot;:{}}</code></pre>
<p>ELB successfully created a load balancer for the TCP port. But
something is wrong with the UDP service. The events give more
information:</p>
<pre class="shell"><code>% oc get event --field-selector involvedObject.name=echo-udp
LAST SEEN   TYPE      REASON                   OBJECT             MESSAGE
94s         Normal    EnsuringLoadBalancer     service/echo-udp   Ensuring load balancer
94s         Warning   SyncLoadBalancerFailed   service/echo-udp   Error syncing load balancer: failed to ensure load balancer: Protocol UDP not supported by LoadBalancer</code></pre>
<p>Load balancer creation failed with the error:</p>
<blockquote>
<p>Error syncing load balancer: failed to ensure load balancer:
Protocol UDP not supported by LoadBalancer</p>
</blockquote>
<p>The workaround is to add an annotation to request a <em>Network Load
Balancer (NLB)</em> instance instead of ELB (the default):</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Service</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> echo-udp</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">annotations</span><span class="kw">:</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">service.beta.kubernetes.io/aws-load-balancer-type</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;nlb&quot;</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="at">  …</span></span></code></pre></div>
<p>After adding the annotation, both load balancers are configured:</p>
<pre class="shell"><code>% oc get -o json service \
    | jq -c &#39;.items[] | (.metadata.name, .status)&#39;
&quot;echo-tcp&quot;
{&quot;loadBalancer&quot;:{&quot;ingress&quot;:[{&quot;hostname&quot;:&quot;a473cf621de6b49dfabb6e933d0fab55-2099420434.us-east-2.elb.amazonaws.com&quot;}]}}
&quot;echo-udp&quot;
{&quot;loadBalancer&quot;:{&quot;ingress&quot;:[{&quot;hostname&quot;:&quot;af7f7ed0f44c9461dbb54a9a4aedca2c-0c5861432365c726.elb.us-east-2.amazonaws.com&quot;}]}}</code></pre>
<div class="note">
<p><code>aws-load-balancer-type</code> is one of several annotations for modifying
AWS load balancer configuration. See the <a href="https://cloud-provider-aws.sigs.k8s.io/service_controller/">AWS Cloud Provider
documentation</a> for the full list.</p>
</div>
<h2 id="testing-the-ingress">Testing the ingress <a href="#testing-the-ingress" class="section">§</a></h2>
<p>Using the IP address or DNS name from the <code>status</code> field, you can
use <code>nc(1)</code> to verify that the server is contactable.</p>
<pre class="shell"><code>% echo hello | nc 34.136.55.93 12345
HELLO

% nc --udp 34.71.82.205 12345
hello                             -- input
HELLO                             -- response
^D</code></pre>
<p>I was able to talk to my echo server via both TCP and UDP.</p>
<div class="note">
<p>If using TLS or DTLS, you could instead use OpenSSL’s <code>s_client(1)</code>
to test connectivity.</p>
</div>
<p>Use hostname instead of IP address if that is how the cloud provider
reports the ingress endpoint.</p>
<h2 id="reaching-the-service-via-dns">Reaching the service via DNS <a href="#reaching-the-service-via-dns" class="section">§</a></h2>
<p>The cloud provider has set up the load balancer and the ingress IP
addresses or hostnames are reported in the <code>status</code> field of the
Service object(s). Now you probably wish to set up DNS records so
that clients can use an established domain name to find the server.</p>
<p>I can’t go deep into this topic in this post, because I am still
exploring this problem space myself. But I can describe some
possible solutions at a high level.</p>
<p>One possibility is to teach your application controller to manage
the required DNS records. It would monitor the Service objects and
reconcile the external DNS configuration with what it sees. The
number and kind of records to be created will vary depending on
whether the cloud providers reports the ingress points as hostnames
or IP addresses:</p>
<table>
<thead>
<tr class="header">
<th>Ingress endpoint</th>
<th>Resolution method</th>
<th>Records needed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>hostname</code></td>
<td>direct</td>
<td><code>CNAME</code></td>
</tr>
<tr class="even">
<td><code>hostname</code></td>
<td>SRV</td>
<td><code>SRV</code></td>
</tr>
<tr class="odd">
<td><code>ip</code></td>
<td>direct</td>
<td><code>A</code>/<code>AAAA</code></td>
</tr>
<tr class="even">
<td><code>ip</code></td>
<td>SRV</td>
<td><code>A</code>/<code>AAAA</code> and <code>SRV</code></td>
</tr>
</tbody>
</table>
<p>Most applications have similar needs, so it would make sense to
encapsulate this behaviour in a controller that configures arbitrary
external DNS providers. That’s what the Kubernetes <a href="https://github.com/kubernetes-sigs/external-dns">ExternalDNS</a>
project is all about. <a href="https://github.com/kubernetes-sigs/external-dns#status-of-providers">Provider stability varies</a>; at
time of writing the only <em>stable</em> providers are Google Cloud DNS and
AWS Route 53.</p>
<p>Integration with OpenShift is via the <a href="https://github.com/openshift/external-dns-operator">ExternalDNS Operator</a>.
This is an active area of work and ExternalDNS will hopefully be an
officially supported part of OpenShift in a future release.</p>
<p>I haven’t actually played with ExternalDNS yet so can’t say much
more about it at this time. Only that it looks like a very useful
solution!</p>
<p>Finally, recall the caveats I mentioned earlier about applications
that require ingress of <strong>both TCP and UDP</strong> traffic. <a href="https://github.com/kubernetes/enhancements/issues/1435">KEP 1435</a>,
along with cloud provider support, should resolve this issue
eventually.</p>]]></summary>
</entry>
<entry>
    <title>Creating user namespaces inside containers</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2021-10-15-openshift-userns-in-container.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2021-10-15-openshift-userns-in-container.html</id>
    <published>2021-10-15T00:00:00Z</published>
    <updated>2021-10-15T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="creating-user-namespaces-inside-containers">Creating user namespaces inside containers</h1>
<p>Over the last year I have experimented with user namespace support in
OpenShift. That is, making OpenShift run workloads inside a
separate user namespace. We’re trying to drive this feature
forward, but some people have reservations. Does having processes
running as <code>root</code> inside a user namespace present an increased
security risk? What if there are kernel bugs…</p>
<p>If you’re worried about the security of user namespaces, OpenShift
or Kubernetes user namespace support doesn’t change the game at all.
As I demonstrate in this post, you can create and use user
namespaces <em>inside</em> your workloads right now.</p>
<h2 id="demo">Demo <a href="#demo" class="section">§</a></h2>
<p>I tested on OpenShift 4.9.0 in the default configuration. So, no
explicit user namespace support. I used a stock Fedora container
image with the following Pod spec:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> fedora</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> fedora</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">image</span><span class="kw">:</span><span class="at"> registry.fedoraproject.org/fedora:34-x86_64</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">command</span><span class="kw">:</span><span class="at"> </span><span class="kw">[</span><span class="st">&quot;sleep&quot;</span><span class="kw">,</span><span class="at"> </span><span class="st">&quot;3600&quot;</span><span class="kw">]</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">securityContext</span><span class="kw">:</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">capabilities</span><span class="kw">:</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">drop</span><span class="kw">:</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> CHOWN</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> DAC_OVERRIDE</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> FOWNER</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> FSETID</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> SETPCAP</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> NET_BIND_SERVICE</span></span></code></pre></div>
<p>The Pod will run under the <code>restricted</code> SCC. I explicitly drop a
number of default capabilities.</p>
<p>Next I created a project named <code>userns</code>, and new user <code>me</code>.</p>
<pre class="shell"><code>% oc new-project userns
Now using project &quot;userns&quot; on server &quot;https://api.ci-ln-cih2n32-f76d1.origin-ci-int-gce.dev.openshift.com:6443&quot;.

You can add applications to this project with the &#39;new-app&#39; command. For example, try:

    oc new-app rails-postgresql-example

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=k8s.gcr.io/serve_hostname

% oc create user me
user.user.openshift.io/me created

% oc adm policy add-role-to-user edit me
clusterrole.rbac.authorization.k8s.io/edit added: &quot;me&quot;</code></pre>
<p>Operating as <code>me</code> I created the pod:</p>
<pre class="shell"><code>% oc --as me create -f pod-fedora.yaml
pod/fedora created</code></pre>
<p>Soon after, the pod is running. I can see what node it is running
on, and its CRI-O container ID:</p>
<pre class="shell"><code>% oc get -o json pod/fedora \
    | jq &#39;.status.phase,
          .spec.nodeName,
          .status.containerStatuses[0].containerID&#39;
&quot;Running&quot;
&quot;ci-ln-cih2n32-f76d1-sjtwq-worker-a-qr5hr&quot;
&quot;cri-o://d164163951604b7fc9506b3a390ec6a14c76dc6077406fc7b5ffcbf81c406f68&quot;</code></pre>
<p>Next I started a shell in my container. I’ll leave it running for
now, and come back to it later:</p>
<pre class="shell"><code>% oc exec -it pod/fedora /bin/sh
sh-5.1$</code></pre>
<p>In another terminal, I opened a debug shell on the worker node.
Then I used <code>crictl</code> to find out the process ID (<code>pid</code>) of the main
container process.</p>
<pre class="shell"><code>% oc debug node/ci-ln-cih2n32-f76d1-sjtwq-worker-a-qr5hr
Starting pod/ci-ln-cih2n32-f76d1-sjtwq-worker-a-qr5hr-debug ...
To use host binaries, run `chroot /host`
Pod IP: 10.0.128.2
If you don&#39;t see a command prompt, try pressing enter.
sh-4.4# chroot /host
sh-4.4# crictl inspect d1641639 | jq .info.pid
18668</code></pre>
<p>Next I used <code>pgrep</code> to find all the processes that share the same
set of namespaces as process <code>18668</code>. In other words, processes
running in the same pod sandbox.</p>
<pre class="shell"><code>sh-4.4# pgrep --ns 18668 \
    | xargs ps -o user,pid,cmd --sort pid
USER         PID CMD
1000580+   18668 sleep 3600
1000580+   26490 /bin/sh</code></pre>
<p>There are two processes, running under an unpriviled UID. The UID
comes from a unique range allocated for the <code>userns</code> project. These
two processes are the main container process (<code>sleep</code>), and the
shell that I exected a few steps ago. As expected.</p>
<p>Now for the fun part. Back to the shell we opened in <code>pod/fedora</code>.
Observe that this shell process has an empty capability set:</p>
<pre class="shell"><code>sh-5.1$ grep Cap /proc/$$/status
CapInh: 0000000000000000
CapPrm: 0000000000000000
CapEff: 0000000000000000
CapBnd: 0000000000000000
CapAmb: 0000000000000000</code></pre>
<p>And yet, using <code>unshare(1)</code> I was able to create a new user
namespace. The <code>-r</code> option says to map <code>root</code> in the new user
namespace to the user that created the namespace. And that is
indeed what happens:</p>
<pre class="shell"><code>sh-5.1$ unshare -U -r
[root@fedora /]# id
uid=0(root) gid=0(root) groups=0(root),65534(nobody)</code></pre>
<p>I confirmed it via the node debug shell. I ran <code>pgrep</code> again, this
time restricting the search to processes in the same <code>pid</code> namespace
as process <code>18668</code>. The <code>--nslist</code> option gives the list of
namespaces to match (all namespaces when not specified).</p>
<pre class="shell"><code>sh-4.4# pgrep --ns 18668 --nslist pid \
    | xargs ps -o user,pid,cmd --sort pid
USER         PID CMD
1000580+   18668 sleep 3600
1000580+   26490 /bin/sh
1000580+   36704 -sh</code></pre>
<p>The new shell has pid <code>36704</code>. Observe that UID <code>0</code> in the
container maps to UID <code>1000580000</code>:</p>
<pre class="shell"><code>sh-4.4# cat /proc/36704/uid_map
         0 1000580000          1</code></pre>
<h2 id="discussion">Discussion <a href="#discussion" class="section">§</a></h2>
<p>You can create and use user namespaces inside your containers
without any special support from OpenShift or Kubernetes.
Therefore, the idea of a OpenShift or Kubernetes feature for running
a workload in an isolated user namespace <em>by default</em> does not lead
to an increased risk of container escapes or privilege escalation
related to processes running as uid 0 in a user namespace.</p>
<p>This is not to gloss over the fact that other parts of a “workloads
in user namespaces” feature have to be designed and implemented with
care. Particular aspects include pod admission and selection of the
unprivileged UIDs to map to. But on the question of the security of
the Linux user namespaces feature itself, a first class OpenShift of
Kubernetes feature doesn’t introduce any new risk. Whatever risk
there is, is there right now.</p>
<p>If some critical security with user namespaces emerges and you need
an urgent mitigation, the only option is to alter the container
runtime Seccomp policies to block the <code>unshare(2)</code> syscall. This is
an advanced topic, involving changes to node configuration. For
details, see <a href="https://docs.openshift.com/container-platform/4.8/security/seccomp-profiles.html"><em>Configuring seccomp profiles</em></a> in the
official OpenShift documentation.</p>]]></summary>
</entry>
<entry>
    <title>Demo: namespaced systemd workloads on OpenShift</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2021-07-22-openshift-systemd-workload-demo.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2021-07-22-openshift-systemd-workload-demo.html</id>
    <published>2021-07-22T00:00:00Z</published>
    <updated>2021-07-22T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="demo-namespaced-systemd-workloads-on-openshift">Demo: namespaced systemd workloads on OpenShift</h1>
<p>I have spent much of the last year diving deep into OpenShift’s
container runtime. The goal: work out how to run systemd-based
workloads in <em>user namespaces</em> on OpenShift nodes. The exploration
took many twists and turns. But finally, I have achieved the goal.</p>
<p>In this post I recap the journey so far, and
<a href="#demo"><strong>demonstrate</strong></a> what I have achieved. Then I will
summarise the path(s?) forward from here.</p>
<h2 id="the-journey-so-far">The journey so far <a href="#the-journey-so-far" class="section">§</a></h2>
<p>My <a href="2021-07-21-freeipa-on-openshift-update.html">previous post</a>
gives an overview of the FreeIPA on OpenShift project. In
particular, it explains our decision to use a “monolithic”
systemd-based container. That implementation approach exposed
capability gaps in OpenShift and led to a long running series of
investigations. I wrote up the results of these investigations
across several blog posts, summarised here:</p>
<h3 id="openshift-and-user-namespaces"><a href="2020-11-05-openshift-user-namespace.html"><em>OpenShift and user namespaces</em></a> <a href="#openshift-and-user-namespaces" class="section">§</a></h3>
<p>I observed that OpenShift (4.6 at the time) did not isolate
containers in user namespaces. I noted that <a href="https://github.com/kubernetes/enhancements/issues/127">KEP-127</a> proposes
user namespace support for Kubernetes (it is <a href="https://github.com/kubernetes/enhancements/pull/2101">still being worked
on</a>). CRI-O
had also recently <a href="https://github.com/cri-o/cri-o/pull/3944">added
support</a> for user
namespaces via annotations.</p>
<h3 id="user-namespaces-in-openshift-via-cri-o-annotations"><a href="2020-12-01-openshift-crio-userns.html"><em>User namespaces in OpenShift via CRI-O annotations</em></a> <a href="#user-namespaces-in-openshift-via-cri-o-annotations" class="section">§</a></h3>
<p>I tested CRI-O’s annotation-based user namespace support on
OpenShift 4.7 nightlies. I found that the runtime creates a sandbox
with a user namespace and the expected UID mappings. I also found
that it is necessary to override the <code>net.ipv4.ping_group_range</code>
sysctl. Also, the SCC enforcement machinery does not know about
user namespaces and therefore the account that creates the container
requires the <code>anyuid</code> SCC. These deficiencies still exist today.</p>
<h3 id="user-namespace-support-in-openshift-4.7"><a href="2021-03-03-openshift-4.7-user-namespaces.html"><em>User namespace support in OpenShift 4.7</em></a> <a href="#user-namespace-support-in-openshift-4.7" class="section">§</a></h3>
<p>I continued my investigation after the release of OpenShift 4.7.
With the aforementioned caveats, user namespaces work. I also noted
an inconsistent treatment of <code>securityContext</code>: specifying
<code>runAsUser</code> in the <code>PodSpec</code> maps the container’s UID <code>0</code> to host
UID <code>0</code>—a dangerous configuration.</p>
<p>More recently, I noticed that the <code>userns-mode</code> annotation I was
using included <code>map-to-root=true</code>. I now understand that it is this
configuration that causes this mapping behaviour. I no longer
consider it particularly serious. Ideally the SCC enforcement
should learn about user namespaces, and prevent unprivileged users
from creating containers that run as <code>root</code> (or other system
accounts) on the host.</p>
<h3 id="multiple-users-in-user-namespaces-on-openshift"><a href="2021-03-10-openshift-user-namespace-multi-user.html"><em>Multiple users in user namespaces on OpenShift</em></a> <a href="#multiple-users-in-user-namespaces-on-openshift" class="section">§</a></h3>
<p>I verified that workloads that run processes under a variety of user
accounts work as expected in user namespaces. I did not use a
<em>systemd</em>-based workload to verify this.</p>
<h3 id="systemd-containers-on-openshift-with-cgroups-v2"><a href="2021-03-30-openshift-cgroupv2-systemd.html"><em>systemd containers on OpenShift with cgroups v2</em></a> <a href="#systemd-containers-on-openshift-with-cgroups-v2" class="section">§</a></h3>
<p>I observed that systemd-based workloads run successfully in
OpenShift when executed as UID 0 <em>on the host</em>. Such containers can
only be created by accounts granted privileged SCCs (e.g. <code>anyuid</code>).
When running the container under other UIDs, <em>systemd</em> can’t run
because it does not have write permission on the container’s cgroup
directory.</p>
<h3 id="using-runc-to-explore-the-oci-runtime-specification"><a href="2021-05-27-oci-runtime-spec-runc.html"><em>Using <code>runc</code> to explore the OCI Runtime Specification</em></a> <a href="#using-runc-to-explore-the-oci-runtime-specification" class="section">§</a></h3>
<p>I investigated how <code>runc</code> (the OCI runtime used in OpenShift)
operates, and how it creates cgroups. I identified some potential
ways to change the ownership of the container cgroup to the
<em>container’s</em> UID 0.</p>
<h3 id="systemd-cgroups-and-subuid-ranges"><a href="2021-06-09-systemd-cgroups-subuid.html"><em>systemd, cgroups and subuid ranges</em></a> <a href="#systemd-cgroups-and-subuid-ranges" class="section">§</a></h3>
<p>I discovered that the systemd <em>transient unit API</em> (which <code>runc</code>
uses to create container cgroups) allows specifying a different
owner for the new cgroup. Unfortunately, the user must be “known”,
in the form of a <code>passwd</code> entity via NSSwitch. A <a href="https://github.com/systemd/systemd/issues/19781">proposal to relax
this requirement</a>
was provisionally rejected. Other approaches include writing an
NSSwitch module to synthesise <code>passwd</code> entities for subuids, or
modifying <code>runc</code> to <code>chown(2)</code> the container cgroup after systemd
creates it. I decided to experiment with the latter approach.</p>
<h2 id="modifying-runc-to-chown-the-container-cgroup">Modifying <code>runc</code> to <code>chown</code> the container cgroup <a href="#modifying-runc-to-chown-the-container-cgroup" class="section">§</a></h2>
<p>The main challenge in modifying <code>runc</code> was getting my head around
the unfamiliar codebase. The actual operations are straightforward.
There are two main aspects.</p>
<p>The first aspect is to compute the appropriate owner UID for the
cgroup, and tell it to the cgroup manager object. I <a href="2021-06-09-systemd-cgroups-subuid.html#determining-the-uid">described the
algorithm</a> in a previous post. The <code>config.HostRootUID()</code> method
already implements this computation. I was able to reuse it.</p>
<p>The second aspect is to actually <code>chown(2)</code> the relevant cgroup
files and directories. I previously observed systemd’s behaviour
when creating units owned by arbitrary users. systemd <code>chown</code>s the
container’s cgroup directory, and the <code>cgroup.procs</code>,
<code>cgroup.subtree_control</code> and <code>cgroup.threads</code> files within that
directory. <code>runc</code> will do the same. The cgroup manager object
already knows the path to the container cgroup directory. It
changes the owner of the directory and same three files as <em>systemd</em>
to the relevant user.</p>
<h2 id="demo">Demo <a href="#demo" class="section">§</a></h2>
<p>Following is a step-by-step demonstration starting with a fresh
deployment of OpenShift <code>4.7.20</code>.</p>
<pre class="shell"><code>% oc get clusterversion
NAME      VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
version   4.7.20    True        False         8m52s   Cluster version is 4.7.20</code></pre>
<div class="note">
<p>There is a <a href="https://github.com/cri-o/cri-o/issues/5077">regression</a>
in OpenShift 4.8.0 that prevents Pod annotations from being propagated
to container OCI configurations. As a consequence, <code>runc</code> does not
receive the annotations that trigger the experimental behaviour. I
filed a <a href="https://github.com/cri-o/cri-o/pull/5078">pull request</a>
that fixes the issue. The patch was accepted and the fix released
in OpenShift 4.8.4.</p>
</div>
<p>The latent credential is the cluster <code>admin</code> user. Where relevant,
I use the <code>oc --as USER</code> option to execute commands as other users.</p>
<pre class="shell"><code>% oc whoami
system:admin</code></pre>
<h3 id="install-modified-runc-package">Install modified <code>runc</code> package <a href="#install-modified-runc-package" class="section">§</a></h3>
<p>List the nodes in the cluster:</p>
<pre class="shell"><code>% oc get node
NAME                                       STATUS   ROLES    AGE   VERSION
ci-ln-jqbnbfk-f76d1-gnkkv-master-0         Ready    master   61m   v1.20.0+01c9f3f
ci-ln-jqbnbfk-f76d1-gnkkv-master-1         Ready    master   61m   v1.20.0+01c9f3f
ci-ln-jqbnbfk-f76d1-gnkkv-master-2         Ready    master   61m   v1.20.0+01c9f3f
ci-ln-jqbnbfk-f76d1-gnkkv-worker-a-vrbnv   Ready    worker   52m   v1.20.0+01c9f3f
ci-ln-jqbnbfk-f76d1-gnkkv-worker-b-dxk6k   Ready    worker   52m   v1.20.0+01c9f3f
ci-ln-jqbnbfk-f76d1-gnkkv-worker-c-db89w   Ready    worker   52m   v1.20.0+01c9f3f</code></pre>
<p>For each worker node, open a node debug shell and use <code>rpm-ostree override replace</code> to install the modified <code>runc</code> (one worker shown):</p>
<pre class="shell"><code>% oc debug node/ci-ln-jqbnbfk-f76d1-gnkkv-worker-a-vrbnv
Starting pod/ci-ln-jqbnbfk-f76d1-gnkkv-worker-a-vrbnv-debug ...
To use host binaries, run `chroot /host`
Pod IP: 10.0.32.2
If you don&#39;t see a command prompt, try pressing enter.
sh-4.2# chroot /host
sh-4.4# rpm-ostree override replace https://ftweedal.fedorapeople.org/runc-1.0.0-990.rhaos4.8.gitcd80260.el8.x86_64.rpm
Downloading &#39;https://ftweedal.fedorapeople.org/runc-1.0.0-990.rhaos4.8.gitcd80260.el8.x86_64.rpm&#39;... done!
Checking out tree 9767154... done
No enabled rpm-md repositories.
Importing rpm-md... done
Resolving dependencies... done
Applying 1 override
Processing packages... done
Running pre scripts... done
Running post scripts... done
Running posttrans scripts... done
Writing rpmdb... done
Writing OSTree commit... done
Staging deployment... done
Upgraded:
  runc 1.0.0-96.rhaos4.8.gitcd80260.el8 -&gt; 1.0.0-990.rhaos4.8.gitcd80260.el8
Run &quot;systemctl reboot&quot; to start a reboot</code></pre>
<div class="note">
<p>Instead of installing the modified <code>runc</code> on all worker nodes, you
could update one node and use <code>.spec.nodeAffinity</code> in the <code>PodSpec</code>
to force the pod to run on that node.</p>
</div>
<p>Don’t worry about the restart right now (it will happen in the next
step). Exit the debug shell:</p>
<pre class="shell"><code>sh-4.4# exit
sh-4.2# exit

Removing debug pod ...</code></pre>
<h3 id="enable-user-namespaces-and-cgroups-v2">Enable user namespaces and cgroups v2 <a href="#enable-user-namespaces-and-cgroups-v2" class="section">§</a></h3>
<p>The following <code>MachineConfig</code> enables cgroups v2 and CRI-O
annotation-based user namespace support:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> machineconfiguration.openshift.io/v1</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> MachineConfig</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">labels</span><span class="kw">:</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">machineconfiguration.openshift.io/role</span><span class="kw">:</span><span class="at"> worker</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> userns-cgv2</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">kernelArguments</span><span class="kw">:</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> systemd.unified_cgroup_hierarchy=1</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> cgroup_no_v1=&quot;all&quot;</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> psi=1</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">config</span><span class="kw">:</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">ignition</span><span class="kw">:</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">version</span><span class="kw">:</span><span class="at"> </span><span class="fl">3.1.0</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">storage</span><span class="kw">:</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">files</span><span class="kw">:</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> </span><span class="fu">path</span><span class="kw">:</span><span class="at"> /etc/crio/crio.conf.d/99-crio-userns.conf</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">overwrite</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">contents</span><span class="kw">:</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">source</span><span class="kw">:</span><span class="at"> data:text/plain;charset=utf-8;base64,W2NyaW8ucnVudGltZS5ydW50aW1lcy5ydW5jXQphbGxvd2VkX2Fubm90YXRpb25zPVsiaW8ua3ViZXJuZXRlcy5jcmktby51c2VybnMtbW9kZSJdCg==</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> </span><span class="fu">path</span><span class="kw">:</span><span class="at"> /etc/subuid</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">overwrite</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">contents</span><span class="kw">:</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">source</span><span class="kw">:</span><span class="at"> data:text/plain;charset=utf-8;base64,Y29yZToxMDAwMDA6NjU1MzYKY29udGFpbmVyczoyMDAwMDA6MjY4NDM1NDU2Cg==</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> </span><span class="fu">path</span><span class="kw">:</span><span class="at"> /etc/subgid</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">overwrite</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">contents</span><span class="kw">:</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">source</span><span class="kw">:</span><span class="at"> data:text/plain;charset=utf-8;base64,Y29yZToxMDAwMDA6NjU1MzYKY29udGFpbmVyczoyMDAwMDA6MjY4NDM1NDU2Cg==</span></span></code></pre></div>
<p>The file <code>/etc/crio/crio.conf.d/99-crio-userns.conf</code> enables CRI-O’s
annotation-based user namespace support. Its content
(base64-encoded in the <code>MachineConfig</code>) is:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode ini"><code class="sourceCode ini"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">[crio.runtime.runtimes.runc]</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="dt">allowed_annotations</span><span class="ot">=</span><span class="st">[&quot;io.kubernetes.cri-o.userns-mode&quot;]</span></span></code></pre></div>
<p>The <code>MachineConfig</code> also overrides <code>/etc/subuid</code> and <code>/etc/subgid</code>,
defining sub-id ranges for user namespaces. The content is the same
for both files:</p>
<pre><code>core:100000:65536
containers:200000:268435456</code></pre>
<p>Create the <code>MachineConfig</code>:</p>
<pre class="shell"><code>% oc create -f machineconfig-userns-cgv2.yaml
machineconfig.machineconfiguration.openshift.io/userns-cgv2 created</code></pre>
<p>Wait for the Machine Config Operator to apply the changes and reboot
the worker nodes:</p>
<pre class="shell"><code>% oc wait mcp/worker --for condition=updated --timeout=-1s
machineconfigpool.machineconfiguration.openshift.io/worker condition met</code></pre>
<p>It will take several minutes, as worker nodes get rebooted one a time.</p>
<h3 id="create-project-and-user">Create project and user <a href="#create-project-and-user" class="section">§</a></h3>
<p>Create a new project called <code>test</code>:</p>
<pre class="shell"><code>% oc new-project test
Now using project &quot;test&quot; on server &quot;https://api.ci-ln-jqbnbfk-f76d1.origin-ci-int-gce.dev.openshift.com:6443&quot;.

You can add applications to this project with the &#39;new-app&#39; command. For example, try:

    oc new-app ruby~https://github.com/sclorg/ruby-ex.git

to build a new example application in Python. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node</code></pre>
<p>The output shows the public domain name of this cluster:
<code>ci-ln-jqbnbfk-f76d1.origin-ci-int-gce.dev.openshift.com</code>. We need to know
this for creating the route in the next step.</p>
<p>Create a user called <code>test</code>. Grant it <code>admin</code> role on project
<code>test</code>, and the <code>anyuid</code> Security Context Constraint (SCC)
privilege:</p>
<pre class="shell"><code>% oc create user test
user.user.openshift.io/test created
% oc adm policy add-role-to-user admin test
clusterrole.rbac.authorization.k8s.io/admin added: &quot;test&quot;
% oc adm policy add-scc-to-user anyuid test
securitycontextconstraints.security.openshift.io/anyuid added to: [&quot;test&quot;]</code></pre>
<h3 id="create-service-and-route">Create service and route <a href="#create-service-and-route" class="section">§</a></h3>
<p>Create a service to provide HTTP access to pods matching the <code>app: nginx</code> selector:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Service</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> nginx</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">selector</span><span class="kw">:</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">app</span><span class="kw">:</span><span class="at"> nginx</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">ports</span><span class="kw">:</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> </span><span class="fu">protocol</span><span class="kw">:</span><span class="at"> TCP</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">port</span><span class="kw">:</span><span class="at"> </span><span class="dv">80</span></span></code></pre></div>
<pre class="shell"><code>% oc create -f service-nginx.yaml
service/nginx created</code></pre>
<p>The following route definition will provide HTTP ingress from
outside the cluster:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Route</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> nginx</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">host</span><span class="kw">:</span><span class="at"> nginx.apps.ci-ln-jqbnbfk-f76d1.origin-ci-int-gce.dev.openshift.com</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">to</span><span class="kw">:</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">kind</span><span class="kw">:</span><span class="at"> Service</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> nginx</span></span></code></pre></div>
<p>Note the <code>host</code> field. Its value is <code>nginx.apps.$CLUSTER_DOMAIN</code>.
Change it to the proper value for your cluster, then create the
route:</p>
<pre class="shell"><code>% oc create -f route-nginx.yaml
route.route.openshift.io/nginx created</code></pre>
<p>There is no pod to route the traffic to… yet.</p>
<h3 id="create-pod">Create pod <a href="#create-pod" class="section">§</a></h3>
<p>The pod specification is:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> nginx</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">labels</span><span class="kw">:</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">app</span><span class="kw">:</span><span class="at"> nginx</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">annotations</span><span class="kw">:</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">io.kubernetes.cri-o.userns-mode</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;auto:size=65536&quot;</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">securityContext</span><span class="kw">:</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">sysctls</span><span class="kw">:</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;net.ipv4.ping_group_range&quot;</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">value</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;0 65535&quot;</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> nginx</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">image</span><span class="kw">:</span><span class="at"> quay.io/ftweedal/test-nginx:latest</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">tty</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span></code></pre></div>
<p>Create the pod:</p>
<pre class="shell"><code>% oc --as test create -f pod-nginx.yaml
pod/nginx created</code></pre>
<p>After a few seconds, the pod is running:</p>
<pre class="shell"><code>% oc get -o json pod/nginx | jq .status.phase
&quot;Running&quot;</code></pre>
<p>Tail the pod’s log. Observe the final lines of systemd boot output
and the login prompt:</p>
<pre class="shell"><code>% oc logs --tail 10 pod/nginx
[  OK  ] Started The nginx HTTP and reverse proxy server.
[  OK  ] Reached target Multi-User System.
[  OK  ] Reached target Graphical Interface.
         Starting Update UTMP about System Runlevel Changes...
[  OK  ] Finished Update UTMP about System Runlevel Changes.

Fedora 33 (Container Image)
Kernel 4.18.0-305.3.1.el8_4.x86_64 on an x86_64 (console)

nginx login: %</code></pre>
<div class="note">
<p>Without <code>tty: true</code> in the <code>Container</code> spec, the pod won’t produce
any output and <code>oc logs</code> won’t have anything to show.</p>
</div>
<p>The log tail also shows that systemd started the <code>nginx</code> service.
We already set up a <code>route</code> in the previous step. Use <code>curl</code> to
issue an HTTP request and verify that the service is running
properly:</p>
<pre class="shell"><code>% curl --head \
    nginx.apps.ci-ln-jqbnbfk-f76d1.origin-ci-int-gce.dev.openshift.com
HTTP/1.1 200 OK
Server: nginx/1.18.0
Date: Wed, 21 Jul 2021 06:55:38 GMT
Content-Type: text/html
Content-Length: 5564
Last-Modified: Mon, 27 Jul 2020 22:20:49 GMT
ETag: &quot;5f1f5341-15bc&quot;
Accept-Ranges: bytes
Set-Cookie: 6cf5f3bc2fa4d24f45018c591d3617c3=f114e839b2eef9cdbe00856f18a06336; path=/; HttpOnly
Cache-control: private</code></pre>
<h3 id="verify-sandbox">Verify sandbox <a href="#verify-sandbox" class="section">§</a></h3>
<p>Now let’s verify that the container is indeed running in a user
namespace. Container UIDs must map to unprivileged UIDs on the
host. Query the worker node on which the pod is running, and its
CRI-O container ID:</p>
<pre class="shell"><code>% oc get -o json pod/nginx | jq \
    &#39;.spec.nodeName, .status.containerStatuses[0].containerID&#39;
&quot;ci-ln-jqbnbfk-f76d1-gnkkv-worker-c-db89w&quot;
&quot;cri-o://bf2b3d15cbd6944366e29927988ba30bc36d1efee00c28fb4c6d5b2036e462b0&quot;</code></pre>
<p>Start a debug shell on the node and query the PID of the container
init process:</p>
<pre class="shell"><code>% oc debug node/ci-ln-jqbnbfk-f76d1-gnkkv-worker-c-db89w
Starting pod/ci-ln-jqbnbfk-f76d1-gnkkv-worker-c-db89w-debug ...
To use host binaries, run `chroot /host`
Pod IP: 10.0.32.4
If you don&#39;t see a command prompt, try pressing enter.
sh-4.2# chroot /host
sh-4.4# crictl inspect bf2b3d | jq .info.pid
7759</code></pre>
<p>Query the UID map and process tree of the container:</p>
<pre class="shell"><code>sh-4.4# cat /proc/7759/uid_map
         0     200000      65536
sh-4.4# pgrep --ns 7759 | xargs ps -o user,pid,cmd --sort pid
USER         PID CMD
200000      7759 /sbin/init
200000      7796 /usr/lib/systemd/systemd-journald
200193      7803 /usr/lib/systemd/systemd-resolved
200000      7806 /usr/lib/systemd/systemd-homed
200000      7807 /usr/lib/systemd/systemd-logind
200081      7809 /usr/bin/dbus-broker-launch --scope system --audit
200000      7812 /sbin/agetty -o -p -- \u --noclear --keep-baud console 115200,38400,9600 xterm
200081      7813 dbus-broker --log 4 --controller 9 --machine-id 2f2fcc4033c5428996568ca34219c72a --max-bytes 5
200000      7815 nginx: master process /usr/sbin/nginx
200999      7816 nginx: worker process
200999      7817 nginx: worker process
200999      7818 nginx: worker process
200999      7819 nginx: worker process</code></pre>
<p>This confirms that the container has a user namespace. The
container’s UID range is <code>0</code>–<code>65535</code>, which maps to the host UID
range <code>200000</code>–<code>265535</code>. The <code>ps</code> output shows various services
running under systemd, running under unprivileged host UIDs in this
range.</p>
<p>So, everything is running as expected. One last thing: let’s look
at the cgroup ownership. Query the container’s <code>cgroupsPath</code>:</p>
<pre class="shell"><code>sh-4.4# crictl inspect bf2b3d | jq .info.runtimeSpec.linux.cgroupsPath
&quot;kubepods-besteffort-podc7f11ee7_e178_4dea_9d8c_c005ad648988.slice:crio:bf2b3d15cbd6944366e29927988ba30bc36d1efee00c28fb4c6d5b2036e462b0&quot;</code></pre>
<p>The value isn’t a filesystem path. <code>runc</code> interprets it relative to
an implementation-defined location. We expect the cgroup directory
and the three files mentioned earlier to be owned by the user that
maps to UID <code>0</code> in the container’s user namespace. In my case,
that’s <code>200000</code>. We also expect to see scopes and slices created by
systemd <strong>in the container</strong> to be owned by the same user.</p>
<pre class="shell"><code>sh-4.4# ls -ali /sys/fs/cgroup\
/kubepods.slice/kubepods-besteffort.slice\
/kubepods-besteffort-podc7f11ee7_e178_4dea_9d8c_c005ad648988.slice\
/crio-bf2b3d15cbd6944366e29927988ba30bc36d1efee00c28fb4c6d5b2036e462b0.scope \
    | grep 200000
14755 drwxr-xr-x.  5 200000 root   0 Jul 21 06:00 .
14757 -rw-r--r--.  1 200000 root   0 Jul 21 06:00 cgroup.procs
14760 -rw-r--r--.  1 200000 root   0 Jul 21 06:00 cgroup.subtree_control
14758 -rw-r--r--.  1 200000 root   0 Jul 21 06:00 cgroup.threads
14806 drwxr-xr-x.  2 200000 200000 0 Jul 21 06:00 init.scope
14835 drwxr-xr-x. 11 200000 200000 0 Jul 21 06:15 system.slice
14922 drwxr-xr-x.  2 200000 200000 0 Jul 21 06:00 user.slice</code></pre>
<p>Note the <em>inode</em> of the container cgroup directory: <code>14755</code>. We can query the
inode and ownership of <code>/sys/fs/cgroup</code> <em>within the pod</em>:</p>
<pre class="shell"><code>% oc exec pod/nginx -- ls -ldi /sys/fs/cgroup
14755 drwxr-xr-x. 5 root nobody 0 Jul 21 06:00 /sys/fs/cgroup</code></pre>
<p>The inode is the same; this is indeed the same cgroup. But within the
container’s user namespace, the owner appears as <code>root</code>.</p>
<p>This concludes the verification steps. With my modified version of
<code>runc</code>, systemd-based workloads are indeed working properly in user
namespaces.</p>
<h2 id="next-steps">Next steps <a href="#next-steps" class="section">§</a></h2>
<p>I submitted a <a href="https://github.com/opencontainers/runc/pull/3057">pull request</a> with these changes. It remains to be
seen if the general approach will be accepted, but initial feedback
is positive. Some implementation changes are needed. I might have
to hide the behaviour behind a feature gate (e.g. to be activated
via an annotation). I also need to write tests and documentation.</p>
<p>I also need to raise a ticket for the SCC issue. The requirement
for <code>RunAsAny</code> (which is granted by the <code>anyuid</code> SCC) should be
relaxed when the sandbox has a user namespace. The SCC enforcement
machinery needs to be enhanced to understand user namespaces, so
that unprivileged OpenShift user accounts can run workloads in them.</p>
<p>It would be nice to find a way to avoid the sysctl override to allow
the container user to use <code>ping</code>. This is a much lower priority.</p>
<p>Alongside these matters, I can begin testing the FreeIPA container
in the test environment. Although systemd is now working, I need to
see if the FreeIPA’s constituent services will run properly. I
anticipate that I will need to tweak the Pod configuration somewhat.
But are there more runtime capability gaps waiting to be discovered?
I don’t have a particular suspicion about it, but I do need to know
for certain, one way or the other. So expect another blog post
soon!</p>]]></summary>
</entry>
<entry>
    <title>FreeIPA on OpenShift: July 2021 update</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2021-07-21-freeipa-on-openshift-update.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2021-07-21-freeipa-on-openshift-update.html</id>
    <published>2021-07-21T00:00:00Z</published>
    <updated>2021-07-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="freeipa-on-openshift-july-2021-update">FreeIPA on OpenShift: July 2021 update</h1>
<p>Over the last year I’ve done a lot of investigations into OpenShift,
and container runtimes more generally. The driver of this work is
the FreeIPA on OpenShift project (known within Red Hat as IDMOCP).
I published the results of my investigations in numerous blog posts,
but I have not yet written much about <em>why</em> we are doing this at
all.</p>
<p>So it’s time to fix that. In this short post I discuss why we want
FreeIPA on OpenShift, and the major decision that put us on our
current implementation path.</p>
<p>FreeIPA is a centralised identity management system for the
enterprise. You enrol users, hosts and services, and configure
access policies and other security mechanisms. The system provides
authentication and policy enforcement mechanisms. It is similar to
Microsoft Active Directory (and indeed can integrate with AD).
FreeIPA is a complex system with lots of components including:</p>
<ul>
<li>LDAP server (389 DS / RHDS)</li>
<li>Kerberos KDC (MIT Kerberos)</li>
<li>Certificate authority (Dogtag / RHCS)</li>
<li>HTTP API (Apache httpd and a lot of Python code)</li>
<li>Host client daemon (SSSD)</li>
<li>several smaller supporting services</li>
<li>installation and administration tools</li>
</ul>
<p>FreeIPA is available on Fedora and RHEL. You install the RPMs and
the installation program configures the system. It is intended to
be deployed on a dedicated machine (VM or bare metal).</p>
<p>We are motivated to support FreeIPA on OpenShift for several
reasons, including:</p>
<ul>
<li><p>Easily providing identity services to applications running on
OpenShift.</p></li>
<li><p>Leveraging OpenShift and Kubernetes orchestration, scalaing and
management features to improve robustness and reduce management
overhead of FreeIPA deployments.</p></li>
<li><p>Offering FreeIPA, hosted on OpenShift, as a managed service.</p></li>
</ul>
<p>Understandably, moving such an application to OpenShift is a
non-trivial task. At the beginning of this effort, we had to decide
the main implementation approach. There were three options:</p>
<ol type="1">
<li><p>Put the whole system in a single “monolithic container”, with
systemd as the init process. At the time (and still today)
OpenShift only supports running systemd workloads in privileged
containers, which is not acceptable. The runtime needs to evolve
to support this use case. Work on <em>some</em> of the missing features
(such as user namespaces and cgroups v2) was already underway.</p></li>
<li><p>Deploy different parts of the FreeIPA system in different
containers, running unprivileged. This is a fundamental shift
from the current architecture and a huge up-front engineering
effort. Also, the current architecture has to be maintained and
supported for a long time (&gt;10 years). So this approach brings
a substantial ongoing cost in maintaining two architectures of
the same application. On a technical level, this approach is
feasible today.</p></li>
<li><p>Use a VM-based workload (Kata / OpenShift Sandboxed Containers).
This option probably has the lowest up-front and ongoing
engineering costs. But it requires a bare metal cluster or
nested virtualisation, which is not available from most cloud
providers. By extension, <a href="https://www.openshift.com/products/dedicated/">OpenShift Dedicated (OSD)</a> also
does not supported it. Red Hat managed services run on OSD.
Offering a managed service is one of the motivators of our
effort. So at this time, VM-based workloads are not an option
for us.</p></li>
</ol>
<p>As a small team, and considering the business reality of the
existing offering as part of RHEL, we decided to pursue the
“monolithic container” approach. We are depending on the OpenShift
runtime evolving to a point where it can support fully isolated
systemd-based workloads. And that is why I have invested much of
the last 12 months in understanding container runtimes and pushing
their limits.</p>
<p>Our approach is not “cloud native” and indeed many people have
expressed alarm or confusion when we tell them what we are doing.
Certainly, if we were designing FreeIPA from the ground up in
today’s world, it would look very different from the current
architecture. But this is the reality: if you want customers to
bring their mature, complex applications onto OpenShift, don’t
expect them to spend big money and assume big risk to rearchitect
the application to fit the new environment.</p>
<p>What customers actually need is to be able to bring the application
across more or less as-is. Then they can realise the benefits
(automation, monitoring, scaling, etc) <em>incrementally</em>, with lower
up-front costs and less risk.</p>
<p>If my claims are correct, then proper systemd workload support in
OpenShift will be a Very Big Deal. But even if I’m wrong, it is
still critical for our FreeIPA on OpenShift effort. And it is
achievable. In my next post I’ll demonstrate my working proof of
concept for user-namespaced systemd workloads on OpenShift.</p>]]></summary>
</entry>
<entry>
    <title>Live-testing changes in OpenShift clusters</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2021-06-29-openshift-live-changes.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2021-06-29-openshift-live-changes.html</id>
    <published>2021-06-29T00:00:00Z</published>
    <updated>2021-06-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="live-testing-changes-in-openshift-clusters">Live-testing changes in OpenShift clusters</h1>
<p>I have been hacking on the <a href="https://github.com/opencontainers/runc"><code>runc</code></a> container runtime. So how
do I test my changes in an OpenShift cluster?</p>
<p>One option is to compose a <code>machine-os-content</code> release via
<a href="https://github.com/coreos/coreos-assembler"><em>coreos-assembler</em></a>.
Then you can deploy or upgrade a cluster with that release. Indeed,
this approach is <em>necessary</em> for testing installation and upgrades.
It also seems useful for publishing modified versions for other
people to test. But it is a heavyweight and time consuming option.</p>
<p>For development I want a more lightweight approach. In this post
I’ll demonstrate how to use the <code>rpm-ostree usroverlay</code> and
<code>rpm-ostree override replace</code> commands to test changes in a live
OpenShift cluster.</p>
<h2 id="background">Background <a href="#background" class="section">§</a></h2>
<p>OpenShift runs on CoreOS. CoreOS uses <a href="https://en.wikipedia.org/wiki/OSTree"><em>OSTree</em></a> to manage
the filesystem. Most of the filesystem is immutable. When
upgrading, a new filesystem is prepared before rebooting the system.
The old filesystem is preserved, so it is easy to roll back.</p>
<p>So I can’t just log onto an OpenShift node and replace
<code>/usr/bin/runc</code> with my modified version. Nevertheless, I have seen
<a href="https://github.com/openshift/machine-config-operator/blob/master/docs/HACKING.md#directly-applying-changes-live-to-a-node">references</a> to the <code>rpm-ostree usroverlay</code> command. It is
supposed to provide a writable overlayfs on <code>/usr</code>, so that you can
test modifications. Changes are lost upon reboot, but that’s fine
for testing.</p>
<p>There’s also the <code>rpm-ostree override replace …</code> command. This
command works on the level of RPM packages. It allows you to
install new packages or replace or remove packages. Changes persist
across reboots, but it is easy to roll back to the <em>pristine</em> state
of the current CoreOS release.</p>
<p>The rest of this article explores how to use these two commands to
apply changes to the cluster.</p>
<h2 id="usroverlay-via-debug-container-doesnt-work"><code>usroverlay</code> via debug container (doesn’t work) <a href="#usroverlay-via-debug-container-doesnt-work" class="section">§</a></h2>
<p>I first attempted to use <code>rpm-ostree usroverlay</code> in a node debug
pod.</p>
<pre class="shell"><code>% oc debug node/worker-a
Starting pod/worker-a-debug ...
To use host binaries, run `chroot /host`
Pod IP: 10.0.128.2
If you don&#39;t see a command prompt, try pressing enter.
sh-4.2# chroot /host
sh-4.4# rpm-ostree usroverlay
Development mode enabled.  A writable overlayfs is now mounted on /usr.
All changes there will be discarded on reboot.
sh-4.4# touch /usr/bin/foo
touch: cannot touch &#39;/usr/bin/foo&#39;: Read-only file system</code></pre>
<p>The <code>rpm-ostree usroverlay</code> command succeeded. But <code>/usr</code> remained
read-only. The debug container has its own mount namespace, which
was unaffected. I guess that I need to log into the node directly
to use the writable <code>/usr</code> overlay. Perhaps it is also necessary to
execute <code>rpm-ostree usroverlay</code> as an unconfined user (in the
SELinux sense). I <strong>restarted the node</strong> to begin afresh:</p>
<pre class="shell"><code>sh-4.4# reboot

Removing debug pod ...</code></pre>
<h2 id="usroverlay-via-ssh"><code>usroverlay</code> via SSH <a href="#usroverlay-via-ssh" class="section">§</a></h2>
<p>For the next attempt, I logged into the worker node over SSH. The
first step was to add the SSH public key to the <code>core</code> user’s
<code>authorized_keys</code> file. Roberto Carratalá’s <a href="https://rcarrata.com/openshift/update-workers-ssh/">helpful blog post</a>
explains how to do this. I will recap the critical bits.</p>
<p>SSH keys can be added via <code>MachineConfig</code> objects, which must also
specify the machine role (e.g. <code>worker</code>). The Machine Config
Operator will only add keys to the <code>core</code> user. Multiple keys can
be specified, across multiple <code>MachineConfig</code> objects—all the keys
in matching objects will be included.</p>
<div class="note">
<p>I don’t have direct network access to the worker node. So how could
I log in over SSH? I generated a key <strong><em>in the node debug shell</em></strong>,
and will log in from there!</p>
<pre class="shell"><code>sh-4.4# ssh-keygen
Generating public/private rsa key pair.
Enter file in which to save the key (/root/.ssh/id_rsa):
Created directory &#39;/root/.ssh&#39;.
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /root/.ssh/id_rsa.
Your public key has been saved in /root/.ssh/id_rsa.pub.
The key fingerprint is:
SHA256:jAmv…NMnY root@worker-a
sh-4.4# cat ~/.ssh/id_rsa.pub
ssh-rsa AAAA…4OU= root@worker-a</code></pre>
</div>
<p>The following <code>MachineConfig</code> adds the SSH key for user <code>core</code>:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> machineconfiguration.openshift.io/v1</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> MachineConfig</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> ssh-authorized-keys-worker</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">labels</span><span class="kw">:</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">machineconfiguration.openshift.io/role</span><span class="kw">:</span><span class="at"> worker</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">config</span><span class="kw">:</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">ignition</span><span class="kw">:</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">version</span><span class="kw">:</span><span class="at"> </span><span class="fl">3.2.0</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">passwd</span><span class="kw">:</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">users</span><span class="kw">:</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> core</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">sshAuthorizedKeys</span><span class="kw">:</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> ssh-rsa AAAA…40U= root@worker-a</span></span></code></pre></div>
<p>I created the <code>MachineConfig</code>:</p>
<pre class="shell"><code>% oc create -f machineconfig-ssh-worker.yaml
machineconfig.machineconfiguration.openshift.io/ssh-authorized-keys created</code></pre>
<p>In the node debug shell, I observed that Machine Config Operator
applied the change after a few seconds. It did not restart the
worker node. My key was added alongside a key defined in some other
<code>MachineConfig</code>.</p>
<pre class="shell"><code>sh-4.4# cat /var/home/core/.ssh/authorized_keys
ssh-rsa AAAA…jjNV devenv

ssh-rsa AAAA…4OU= root@worker-a</code></pre>
<p>Now I could log in over SSH:</p>
<pre class="shell"><code>sh-4.4# ssh core@$(hostname)
The authenticity of host &#39;worker-a (10.0.128.2)&#39; can&#39;t be established.
ECDSA key fingerprint is SHA256:LUaZOleqVFunmLCp4/E1naIQ+E5BpmVp0gcsXHGacPE.
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added &#39;worker-a,10.0.128.2&#39; (ECDSA) to the list of known hosts.
Red Hat Enterprise Linux CoreOS 48.84.202106231817-0
  Part of OpenShift 4.8, RHCOS is a Kubernetes native operating system
  managed by the Machine Config Operator (`clusteroperator/machine-config`).

WARNING: Direct SSH access to machines is not recommended; instead,
make configuration changes via `machineconfig` objects:
  https://docs.openshift.com/container-platform/4.8/architecture/architecture-rhcos.html

---
[core@worker-a ~]$</code></pre>
<p>The user is unconfined and I can see the normal, read-only (<code>ro</code>)
<code>/usr</code> mount (but no overlay):</p>
<pre class="shell"><code>[core@worker-a ~]$ id -Z
unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023
[core@worker-a ~]$ mount |grep &quot;on /usr&quot;
/dev/sda4 on /usr type xfs (ro,relatime,seclabel,attr2,inode64,logbufs=8,logbsize=32k,prjquota)
overlay on /usr type overlay (rw,relatime,seclabel,lowerdir=usr,upperdir=/var/tmp/ostree-unlock-ovl.KZ4V50/upper,workdir=/var/tmp/ostree-unlock-ovl.KZ4V50/work)</code></pre>
<p>I executed <code>rpm-ostree usroverlay</code> via <code>sudo</code>. After that, a
read-write (<code>rw</code>) overlay filesystem is visible:</p>
<pre class="shell"><code>[core@worker-a ~]$ sudo rpm-ostree usroverlay
Development mode enabled.  A writable overlayfs is now mounted on /usr.
All changes there will be discarded on reboot.
[core@worker-a ~]$ mount |grep &quot;on /usr&quot;
/dev/sda4 on /usr type xfs (ro,relatime,seclabel,attr2,inode64,logbufs=8,logbsize=32k,prjquota)
overlay on /usr type overlay (rw,relatime,seclabel,lowerdir=usr,upperdir=/var/tmp/ostree-unlock-ovl.TCPM50/upper,workdir=/var/tmp/ostree-unlock-ovl.TCPM50/work)</code></pre>
<p>And it is indeed writable. I made a copy of the original <code>runc</code>
binary, then installed my modified version:</p>
<pre class="shell"><code>[core@worker-a ~]$ sudo cp /usr/bin/runc /usr/bin/runc.orig
[core@worker-a ~]$ sudo curl -Ss -o /usr/bin/runc \
    https://ftweedal.fedorapeople.org/runc</code></pre>
<h2 id="digression-use-a-buildroot">Digression: use a buildroot <a href="#digression-use-a-buildroot" class="section">§</a></h2>
<p>The <code>runc</code> executable I installed on the previous step didn’t work.
I had built it on my workstation, against a too-new version of
<em>glibc</em>. The OpenShift node (which was running RHCOS 4.8, based on
RHEL 8.4) was unable to link <code>runc</code>. Therefore it could not run
<em>any</em> container workloads. I was able to SSH in from another node
and reboot, discarding the transient change in the <code>usroverlay</code> and
restoring the node to a functional state.</p>
<p>All of this is obvious in hindsight. You have to build the program
for the environment in which it will be executed. In my case, it
was easiest to do this via Brew or Koji. I cloned the dist-git
repository (via the <code>fedpkg</code> or <code>rhpkg</code> tool), created patches and
updated the <code>runc.spec</code> file. Then I built the SRPM (<code>.src.rpm</code>)
and started a scratch build in Brew. After the build completed I
made the resulting <code>.rpm</code> publicly available, so that it can be
fetched from the OpenShift cluster.</p>
<h2 id="override-replace-via-node-debug-container"><code>override replace</code> via node debug container <a href="#override-replace-via-node-debug-container" class="section">§</a></h2>
<p>I now have my modified <code>runc</code> in an RPM package. So I can use
<code>rpm-ostree override replace</code> to install it. In a debug node on the
host:</p>
<pre class="shell"><code>sh-4.4# rpm-ostree override replace \
  https://ftweedal.fedorapeople.org/runc-1.0.0-98.rhaos4.8.gitcd80260.el8.x86_64.rpm
Downloading &#39;https://ftweedal.fedorapeople.org/runc-1.0.0-98.rhaos4.8.gitcd80260.el8.x86_64.rpm&#39;... done!
Checking out tree eb6dd3b... done
No enabled rpm-md repositories.
Importing rpm-md... done
Resolving dependencies... done
Applying 1 override
Processing packages... done
Running pre scripts... done
Running post scripts... done
Running posttrans scripts... done
Writing rpmdb... done
Writing OSTree commit... done
Staging deployment... done
Upgraded:
  runc 1.0.0-97.rhaos4.8.gitcd80260.el8 -&gt; 1.0.0-98.rhaos4.8.gitcd80260.el8
Run &quot;systemctl reboot&quot; to start a reboot</code></pre>
<p><code>rpm-ostree</code> downloaded the package and prepared the updated OS.
Per the advice, the update is not active yet; I need to reboot:</p>
<pre class="shell"><code>sh-4.4# rpm -q runc
runc-1.0.0-97.rhaos4.8.gitcd80260.el8.x86_64
sh-4.4# systemctl reboot
sh-4.4# exit
sh-4.2# 
Removing debug pod ...</code></pre>
<p>After reboot I started a node debug container and verified that the
modified version of <code>runc</code> is visible:</p>
<pre class="shell"><code>% oc debug node/worker-a
Starting pod/worker-a-debug ...
To use host binaries, run `chroot /host`
Pod IP: 10.0.128.2
If you don&#39;t see a command prompt, try pressing enter.
sh-4.2# chroot /host
sh-4.4# rpm -q runc
runc-1.0.0-98.rhaos4.8.gitcd80260.el8.x86_64</code></pre>
<p>And the fact that the debug container is working proves that the
modified version of runc isn’t <em>completely</em> broken! Testing the new
functionality is a topic for a different post, so I’ll leave it at
that.</p>
<h3 id="listing-and-resetting-overrides">Listing and resetting overrides <a href="#listing-and-resetting-overrides" class="section">§</a></h3>
<p><code>rpm-ostree status --booted</code> lists the current base image and any
overrides that have been applied:</p>
<pre class="shell"><code>sh-4.4# rpm-ostree status --booted
State: idle
BootedDeployment:
* pivot://quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9a23adde268dc8937ae293594f58fc4039b574210f320ebdac85a50ef40220dd
              CustomOrigin: Managed by machine-config-operator
                   Version: 48.84.202106231817-0 (2021-06-23T18:21:06Z)
      ReplacedBasePackages: runc 1.0.0-97.rhaos4.8.gitcd80260.el8 -&gt; 1.0.0-98.rhaos4.8.gitcd80260.el8</code></pre>
<p>To reset an override for a specific package, run <code>rpm-ostree override reset $PKG</code>:</p>
<pre class="shell"><code>sh-4.4# rpm-ostree override reset runc
Staging deployment... done
Freed: 1.1 GB (pkgcache branches: 0)
Downgraded:
  runc 1.0.0-98.rhaos4.8.gitcd80260.el8 -&gt; 1.0.0-97.rhaos4.8.gitcd80260.el8
Run &quot;systemctl reboot&quot; to start a reboot</code></pre>
<p>To reset <em>all</em> overrides, execute <code>rpm-ostree reset</code>:</p>
<pre class="shell"><code>sh-4.4# rpm-ostree reset
Staging deployment... done
Freed: 54.8 MB (pkgcache branches: 0)
Downgraded:
  runc 1.0.0-98.rhaos4.8.gitcd80260.el8 -&gt; 1.0.0-97.rhaos4.8.gitcd80260.el8
Run &quot;systemctl reboot&quot; to start a reboot</code></pre>
<h2 id="discussion">Discussion <a href="#discussion" class="section">§</a></h2>
<p>I achieved my goal of installed a modified <code>runc</code> executable on an
OpenShift node. There were two approaches:</p>
<ol type="1">
<li><p><code>rpm-ostree usroverlay</code> creates a writable overlay on <code>/usr</code>.
The overlay disappears at reboot, which is fine for my testing
needs. This technique doesn’t work from a node debug container;
you have to log in over SSH, which requires additional steps to
add SSH keys.</p></li>
<li><p><code>rpm-ostree override replace</code> overrides a particular package RPM.
The change takes effect after reboot and is persistent. It is
easy to rollback or reset the override. This technique does not
require SSH login; it works fine in a node debug container.</p></li>
</ol>
<p>Because I needed to build my package in a RHEL 8.4 / RHCOS 4.8
buildroot, I used Brew. The build artifacts are RPMs. Therefore
<code>rpm-ostree override replace</code> is the most convenient option for me.</p>
<p>Both options apply changes <em>per-node</em>. After confirming with CoreOS
developers, there is currently no way to roll out a package override
cluster-wide or to a defined group of nodes (e.g. to
<code>MachineConfigPool/worker</code> via a <code>MachineConfig</code>). So for now, you
either have to apply changes/overrides on specific nodes, or build
the whole <code>machine-os-content</code> image and upgrade the cluster. As a
container runtime developer, my sweet spot is in a gulf between the
existing options. I can tolerate this mild annoyance on the
assumption that it discourages messing around in production
environments.</p>
<p>In the meantime, now that I have worked out how to install my
modified <code>runc</code> onto worker nodes, I will get on with testing it!</p>]]></summary>
</entry>
<entry>
    <title>systemd, cgroups and subuid ranges</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2021-06-09-systemd-cgroups-subuid.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2021-06-09-systemd-cgroups-subuid.html</id>
    <published>2021-06-09T00:00:00Z</published>
    <updated>2021-06-09T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="systemd-cgroups-and-subuid-ranges">systemd, cgroups and subuid ranges</h1>
<p>In my <a href="2021-05-27-oci-runtime-spec-runc.html">previous post</a> I experimented with <code>runc</code> as a way of
understanding the behaviour of OCI runtimes. I ended up focusing on
cgroup creation and the interaction between <code>runc</code> and <em>systemd</em>.
The experiment revealed a critical deficiency: when using user
namespaces the container’s cgroup is not owned by the user executing
the container process. As a result, <em>systemd</em>-based workloads
cannot run.</p>
<p><code>runc</code> creates cgroups via systemd’s <em>transient unit API</em>. Could
a container runtime use this API to control the cgroup ownership?
Let’s find out.</p>
<h2 id="how-runc-talks-to-systemd">How <code>runc</code> talks to <em>systemd</em> <a href="#how-runc-talks-to-systemd" class="section">§</a></h2>
<p>The <em>Open Container Initiative (OCI)</em> <a href="https://github.com/opencontainers/runtime-spec">runtime spec</a> defines a
low-level container runtime interface. OCI runtimes must create the
Linux namespaces specified by an OCI config, including the cgroup
namespace.</p>
<p><code>runc</code> uses the systemd D-Bus API to ask systemd to create a cgroup
scope for the container. Then it creates a cgroup namespace with
the new cgroup scope as the root. We can see that <code>runc</code> invokes
the <code>StartTransientUnit</code> API method with a name for the new unit,
and a list of properties
(<a href="https://github.com/opencontainers/runc/blob/v1.0.0-rc95/vendor/github.com/coreos/go-systemd/v22/dbus/methods.go#L198-L200">source code</a>):</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode go"><code class="sourceCode go"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">// .../go-systemd/v22/dbus/methods.go</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="kw">func</span> <span class="op">(</span>c <span class="op">*</span>Conn<span class="op">)</span> StartTransientUnitContext<span class="op">(</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  ctx context<span class="op">.</span>Context<span class="op">,</span> name <span class="dt">string</span><span class="op">,</span> mode <span class="dt">string</span><span class="op">,</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  properties <span class="op">[]</span>Property<span class="op">,</span> ch <span class="kw">chan</span><span class="op">&lt;-</span> <span class="dt">string</span><span class="op">)</span> <span class="op">(</span><span class="dt">int</span><span class="op">,</span> <span class="dt">error</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> c<span class="op">.</span>startJob<span class="op">(</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    ctx<span class="op">,</span> ch<span class="op">,</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;org.freedesktop.systemd1.Manager.StartTransientUnit&quot;</span><span class="op">,</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    name<span class="op">,</span> mode<span class="op">,</span> properties<span class="op">,</span> <span class="bu">make</span><span class="op">([]</span>PropertyCollection<span class="op">,</span> <span class="dv">0</span><span class="op">))</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>Most of the unit configuration is passed as properties.</p>
<h2 id="the-user-property">The <code>User=</code> property <a href="#the-user-property" class="section">§</a></h2>
<p><a href="https://www.freedesktop.org/software/systemd/man/systemd.exec.html#User="><code>systemd.exec(5)</code></a> describes the properties that
configure a systemd unit (including transient units). Among the
properties are <code>User=</code> and <code>Group=</code>:</p>
<blockquote>
<p>Set the UNIX user or group that the processes are executed as,
respectively. Takes a single user or group name, or a numeric ID
as argument.</p>
</blockquote>
<p>This sounds promising. Further searching turned up a systemd
documentation page entitled <a href="https://systemd.io/CGROUP_DELEGATION/">Control Group APIs and
Delegation</a>. That document states:</p>
<blockquote>
<p>By turning on the <code>Delegate=</code> property for a scope or service you
get a few guarantees: … If your service makes use of the <code>User=</code>
functionality, then the sub-tree will be <code>chown()</code>ed to the
indicated user so that it can correctly create cgroups below it.</p>
</blockquote>
<p><code>runc</code> already supplies <code>Delegate=true</code>. The <code>User=</code> property seems
to be exactly what we need.</p>
<h2 id="determining-the-uid">Determining the UID <a href="#determining-the-uid" class="section">§</a></h2>
<p>The OCI configuration specifies the <a href="https://github.com/opencontainers/runtime-spec/blob/master/config.md#posix-platform-user"><code>user</code></a> that will
execute the container process (in the <strong>container’s user
namespace</strong>). It also specifies <a href=""><code>uidMappings</code></a>
between the host and container user namespaces. For example:</p>
<pre class="shell"><code>% jq -c &#39;.process.user, .linux.uidMappings&#39; &lt; config.json
{&quot;uid&quot;:0,&quot;gid&quot;:0}
[{&quot;containerID&quot;:0,&quot;hostID&quot;:100000,&quot;size&quot;:65536}]</code></pre>
<p><code>runc</code> has all the data it needs to compute the appropriate value
for the <code>User=</code> property. The algorithm, expressed as Python is:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>uid <span class="op">=</span> config[<span class="st">&quot;process&quot;</span>][<span class="st">&quot;user&quot;</span>][<span class="st">&quot;uid&quot;</span>]</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="bu">map</span> <span class="kw">in</span> config[<span class="st">&quot;linux&quot;</span>][<span class="st">&quot;uidMappings&quot;</span>]:</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    uid_min <span class="op">=</span> <span class="bu">map</span>[<span class="st">&quot;containerID&quot;</span>]</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    uid_max <span class="op">=</span> map_min <span class="op">+</span> <span class="bu">map</span>[<span class="st">&quot;size&quot;</span>] <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> uid_min <span class="op">&lt;=</span> uid <span class="op">&lt;=</span> uid_max:</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        offset <span class="op">=</span> uid <span class="op">-</span> uid_min</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">map</span>[<span class="st">&quot;hostID&quot;</span>] <span class="op">+</span> offset</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">raise</span> <span class="pp">RuntimeError</span>(<span class="st">&quot;user.uid is not mapped&quot;</span>)</span></code></pre></div>
<h2 id="testing-with-systemd-run">Testing with <code>systemd-run</code> <a href="#testing-with-systemd-run" class="section">§</a></h2>
<p><code>systemd-run(1)</code> uses the transient unit API to run programs via
transient scope or service units. You can use the <code>--property</code>/<code>-p</code>
option to pass additional properties. I used <code>systemd-run</code> to
observe how systemd handles the <code>Delegate=true</code> and <code>User=</code>
properties.</p>
<h3 id="create-and-inspect-transient-unit">Create and inspect transient unit <a href="#create-and-inspect-transient-unit" class="section">§</a></h3>
<p>First I will do a basic test, talking to my user account’s service
manager:</p>
<pre class="shell"><code>% id -u
1000

% systemd-run --user sleep 300
Running as unit: run-r8e3c22d2bb64491a85882d8303202dca.service

% systemctl --user status run-r8e3c22d2bb64491a85882d8303202dca.service
● run-r8e3c22d2bb64491a85882d8303202dca.service - /bin/sleep 300
     Loaded: loaded (/run/user/1000/systemd/transient/run-r8e3c22d2bb64491a85882d8303202dca.service; transient)
  Transient: yes
     Active: active (running) since Wed 2021-06-09 11:31:14 AEST; 9s ago
   Main PID: 11412 (sleep)
      Tasks: 1 (limit: 2325)
     Memory: 184.0K
        CPU: 3ms
     CGroup: /user.slice/user-1000.slice/user@1000.service/app.slice/run-r8e3c22d2bb64491a85882d8303202dca.service
             └─11412 /bin/sleep 300

Jun 09 11:31:14 f33-1.ipa.local systemd[863]: Started /bin/sleep 300.

% ls -nld /sys/fs/cgroup/user.slice/user-1000.slice/user@1000.service/app.slice/run-r8e3c22d2bb64491a85882d8303202dca.service
drwxr-xr-x. 2 1000 1000 0 Jun  9 11:31 /sys/fs/cgroup/user.slice/user-1000.slice/user@1000.service/app.slice/run-r8e3c22d2bb64491a85882d8303202dca.service</code></pre>
<p>We can see that:</p>
<ul>
<li>systemd-run creates the transient unit</li>
<li>the unit was started successfully, and is running</li>
<li>the unit has is own <code>CGroup</code></li>
<li>the cgroup is owned by user <code>1000</code></li>
</ul>
<p>As I try different ways of invoking <code>systemd-run</code>, I will repeat
this pattern of unit creation, inspection and cgroup ownership
checks.</p>
<h3 id="specify-user-user-service-manager">Specify <code>User=</code> (user service manager) <a href="#specify-user-user-service-manager" class="section">§</a></h3>
<p>Next I explicity specify <code>User=1000</code>:</p>
<pre class="shell"><code>% systemd-run --user -p User=1000 sleep 300
Running as unit: run-r651ff7d0d1214037b70def6d5694dcd6.service

% systemctl --no-pager --full --user status run-r651ff7d0d1214037b70def6d5694dcd6.service
× run-r651ff7d0d1214037b70def6d5694dcd6.service - /bin/sleep 300
     Loaded: loaded (/run/user/1000/systemd/transient/run-r651ff7d0d1214037b70def6d5694dcd6.service; transient)
  Transient: yes
     Active: failed (Result: exit-code) since Wed 2021-06-09 11:38:50 AEST; 1min 17s ago
    Process: 11432 ExecStart=/bin/sleep 300 (code=exited, status=216/GROUP)
   Main PID: 11432 (code=exited, status=216/GROUP)
        CPU: 4ms

Jun 09 11:38:50 f33-1.ipa.local systemd[863]: Started /bin/sleep 300.
Jun 09 11:38:50 f33-1.ipa.local systemd[11432]: run-r651ff7d0d1214037b70def6d5694dcd6.service: Failed to determine supplementary groups: Operation not permitted
Jun 09 11:38:50 f33-1.ipa.local systemd[11432]: run-r651ff7d0d1214037b70def6d5694dcd6.service: Failed at step GROUP spawning /bin/sleep: Operation not permitted
Jun 09 11:38:50 f33-1.ipa.local systemd[863]: run-r651ff7d0d1214037b70def6d5694dcd6.service: Main process exited, code=exited, status=216/GROUP
Jun 09 11:38:50 f33-1.ipa.local systemd[863]: run-r651ff7d0d1214037b70def6d5694dcd6.service: Failed with result &#39;exit-code&#39;.</code></pre>
<p>This unit failed to execute, because the user service manager does
not have permission to determine supplementary groups. Without
going into too much detail, this is because the user systemd
instance lacks the <code>CAP_SETGID</code> capability required by the
<code>setgroups(2)</code> system call used by <code>initgroups(3)</code>.</p>
<p>There doesn’t seem to be a way around this. For the rest of my
testing I’ll talk to the system service manager. That’s okay,
because <code>runc</code> on OpenShift also talks to the system service
manager.</p>
<h3 id="specify-user-system-service-manager">Specify <code>User=</code> (system service manager) <a href="#specify-user-system-service-manager" class="section">§</a></h3>
<pre class="shell"><code>% sudo systemd-run -p User=1000 sleep 300
Running as unit: run-r94725453119e4003af336d7294984085.service

% systemctl status run-r94725453119e4003af336d7294984085.service
● run-r94725453119e4003af336d7294984085.service - /usr/bin/sleep 300
     Loaded: loaded (/run/systemd/transient/run-r94725453119e4003af336d7294984085.service; transient)
  Transient: yes
     Active: active (running) since Wed 2021-06-09 11:50:10 AEST; 11s ago
   Main PID: 11517 (sleep)
      Tasks: 1 (limit: 2325)
     Memory: 184.0K
        CPU: 4ms
     CGroup: /system.slice/run-r94725453119e4003af336d7294984085.service
             └─11517 /usr/bin/sleep 300

Jun 09 11:50:10 f33-1.ipa.local systemd[1]: Started /usr/bin/sleep 300.

% ls -nld /sys/fs/cgroup/system.slice/run-r94725453119e4003af336d7294984085.service
drwxr-xr-x. 2 0 0 0 Jun  9 11:50 /sys/fs/cgroup/system.slice/run-r94725453119e4003af336d7294984085.service

% ps -o uid,pid,cmd --pid 11517
  UID     PID CMD
 1000   11517 /usr/bin/sleep 300</code></pre>
<p>The process is running as user <code>1000</code>, but the cgroup is owned by
<code>root</code>.</p>
<h3 id="specify-delegatetrue">Specify <code>Delegate=true</code> <a href="#specify-delegatetrue" class="section">§</a></h3>
<p>We need to specify <code>Delegate=true</code> to tell systemd to delegate the
cgroup to the specified <code>User</code>:</p>
<pre class="shell"><code>% sudo systemd-run -p Delegate=true -p User=1000 sleep 300
Running as unit: run-r518dbc963502423c9c67b1c72d3d4c12.service

% systemctl status run-r518dbc963502423c9c67b1c72d3d4c12.service
● run-r518dbc963502423c9c67b1c72d3d4c12.service - /usr/bin/sleep 300
     Loaded: loaded (/run/systemd/transient/run-r518dbc963502423c9c67b1c72d3d4c12.service; transient)
  Transient: yes
     Active: active (running) since Wed 2021-06-09 11:59:34 AEST; 1min 21s ago
   Main PID: 11579 (sleep)
      Tasks: 1 (limit: 2325)
     Memory: 184.0K
        CPU: 3ms
     CGroup: /system.slice/run-r518dbc963502423c9c67b1c72d3d4c12.service
             └─11579 /usr/bin/sleep 300

Jun 09 11:59:34 f33-1.ipa.local systemd[1]: Started /usr/bin/sleep 300.

% ls -nld /sys/fs/cgroup/system.slice/run-r518dbc963502423c9c67b1c72d3d4c12.service
drwxr-xr-x. 2 1000 1000 0 Jun  9 11:59 /sys/fs/cgroup/system.slice/run-r518dbc963502423c9c67b1c72d3d4c12.service</code></pre>
<p>systemd <code>chown()</code>ed the cgroup to the specified <code>User</code>. Note that
very few of the cgroup controls in the cgroup directory are
writable by user <code>1000</code>:</p>
<pre class="shell"><code>% ls -nl /sys/fs/cgroup/system.slice/run-r518dbc963502423c9c67b1c72d3d4c12.service \
    |grep 1000 
-rw-r--r--. 1 1000 1000 0 Jun  9 11:59 cgroup.procs
-rw-r--r--. 1 1000 1000 0 Jun  9 11:59 cgroup.subtree_control
-rw-r--r--. 1 1000 1000 0 Jun  9 11:59 cgroup.threads</code></pre>
<p>So the process cannot adjust its root cgroup’s <code>memory.max</code>,
<code>pids.max</code>, <code>cpu.weight</code> and so on. It <em>can</em> create cgroup
subtrees, manage resources within them, and move processes and
threads among those subtrees and its root cgroup.</p>
<h3 id="arbitrary-uids">Arbitrary UIDs <a href="#arbitrary-uids" class="section">§</a></h3>
<p>So far I have specified <code>User=1000</code>. User <code>1000</code> is a “known user”.
That is, the Name Service Switch (see <code>nss(5)</code>) returns information
about the user (name, home directory, shell, etc):</p>
<pre class="shell"><code>% getent passwd $(id -u)
ftweedal:x:1000:1000:ftweedal:/home/ftweedal:/bin/zsh</code></pre>
<p>However, when executing containers with user namespaces, we usually
map the namespace UIDs to unprivileged host UIDs from a <em>subordinate
ID</em> range. Subordinate UIDs and GID ranges are currently defined in
<code>/etc/subuid</code> and <code>/etc/subgid</code> respectively. The subuid range for
user <code>1000</code> is:</p>
<pre class="shell"><code>% grep $(id -un) /etc/subuid
ftweedal:100000:65536</code></pre>
<p>User <code>1000</code> has been allocated the range <code>100000</code>–<code>165535</code>. So
let’s try <code>systemd-run</code> with <code>User=100000</code>:</p>
<pre class="shell"><code>% sudo systemd-run -p Delegate=true -p User=100000 sleep 300
Running as unit: run-r1498304af7df406c9698da5c683ea79e.service

% systemctl --no-pager --full status run-r1498304af7df406c9698da5c683ea79e.service
× run-r1498304af7df406c9698da5c683ea79e.service - /usr/bin/sleep 300
     Loaded: loaded (/run/systemd/transient/run-r1498304af7df406c9698da5c683ea79e.service; transient)
  Transient: yes
     Active: failed (Result: exit-code) since Wed 2021-06-09 12:32:43 AEST; 14s ago
    Process: 11766 ExecStart=/usr/bin/sleep 300 (code=exited, status=217/USER)
   Main PID: 11766 (code=exited, status=217/USER)
        CPU: 2ms

Jun 09 12:32:43 f33-1.ipa.local systemd[1]: Started /usr/bin/sleep 300.
Jun 09 12:32:43 f33-1.ipa.local systemd[11766]: run-r1498304af7df406c9698da5c683ea79e.service: Failed to determine user credentials: No such process
Jun 09 12:32:43 f33-1.ipa.local systemd[11766]: run-r1498304af7df406c9698da5c683ea79e.service: Failed at step USER spawning /usr/bin/sleep: No such process
Jun 09 12:32:43 f33-1.ipa.local systemd[1]: run-r1498304af7df406c9698da5c683ea79e.service: Main process exited, code=exited, status=217/USER
Jun 09 12:32:43 f33-1.ipa.local systemd[1]: run-r1498304af7df406c9698da5c683ea79e.service: Failed with result &#39;exit-code&#39;.</code></pre>
<p>It failed. Cutting the noise, the cause is:</p>
<pre><code>Failed to determine user credentials: No such process</code></pre>
<p>The string <code>No such process</code> is a bit misleading. It is the string
associated with the <code>ESRCH</code> error value (see <code>errno(3)</code>). Here it
indicates that <code>getpwuid(3)</code> did not find a user record for uid
<code>100000</code>. systemd unconditionally fails in this scenario. And this
is a problem for us because without intervention, subordinate UIDs
do not have associated user records.</p>
<h3 id="arbitrary-uids-with-passwd-entry">Arbitrary UIDs (with <code>passwd</code> entry) <a href="#arbitrary-uids-with-passwd-entry" class="section">§</a></h3>
<p>So let’s make NSS return something for user <code>100000</code>. There are
several ways we could do this, including adding it to <code>/etc/passwd</code>,
or creating an NSS module that generates passwd records for ranges
declared in <code>/etc/subuid</code>.</p>
<p>Another way is to use <a href="https://www.freedesktop.org/software/systemd/man/nss-systemd.html">systemd’s NSS module</a>, which
returns passwd records for containers created by
<a href="https://www.freedesktop.org/software/systemd/man/systemd-machined.html"><code>systemd-machined</code></a>. And that’s what I did.
Given the root filesystem for a container in <code>./rootfs</code>,
<a href="https://www.freedesktop.org/software/systemd/man/systemd-nspawn.html"><code>systemd-nspawn</code></a> creates the container. The
<code>--private-users=100000</code> option tells it to create a user namespace
mapping to the host UID <code>100000</code> with default size 65536:</p>
<pre class="shell"><code>% sudo systemd-nspawn --directory rootfs --private-users=100000 /bin/sh
Spawning container rootfs on /home/ftweedal/go/src/github.com/opencontainers/runc/rootfs.
Press ^] three times within 1s to kill container.
Selected user namespace base 100000 and range 65536.
sh-5.0#</code></pre>
<p>On the host we can see the “machine” via
<a href="https://www.freedesktop.org/software/systemd/man/machinectl.html"><code>machinectl(1)</code></a>. We also observe that NSS now returns
results for UIDs in the mapped host range.</p>
<pre class="shell"><code>% getent passwd 100000 165535  
vu-rootfs-0:x:100000:65534:UID 0 of Container rootfs:/:/usr/sbin/nologin

% getent passwd 100000 165534
vu-rootfs-0:x:100000:65534:UID 0 of Container rootfs:/:/usr/sbin/nologin
vu-rootfs-65534:x:165534:65534:UID 65534 of Container rootfs:/:/usr/sbin/nologin</code></pre>
<p>The <code>passwd</code> records are constructed on demand by
<a href="https://www.freedesktop.org/software/systemd/man/nss-systemd.html"><code>nss-systemd(8)</code></a> using data registered by
<code>systemd-machined</code>.</p>
<p>Now let’s try <code>systemd-run</code> again:</p>
<pre class="shell"><code>% sudo systemd-run -p Delegate=true -p User=100000 sleep 300
Running as unit: run-r076a82c36fcd4934b13bba47fcc8462e.service

% systemctl status run-r076a82c36fcd4934b13bba47fcc8462e.service
● run-r076a82c36fcd4934b13bba47fcc8462e.service - /usr/bin/sleep 300
     Loaded: loaded (/run/systemd/transient/run-r076a82c36fcd4934b13bba47fcc8462e.service; transient)
  Transient: yes
     Active: active (running) since Wed 2021-06-09 14:14:34 AEST; 11s ago
   Main PID: 12045 (sleep)
      Tasks: 1 (limit: 2325)
     Memory: 180.0K
        CPU: 4ms
     CGroup: /system.slice/run-r076a82c36fcd4934b13bba47fcc8462e.service
             └─12045 /usr/bin/sleep 300

Jun 09 14:14:34 f33-1.ipa.local systemd[1]: Started /usr/bin/sleep 300.

% ls -nld /sys/fs/cgroup/system.slice/run-r076a82c36fcd4934b13bba47fcc8462e.service 
drwxr-xr-x. 2 100000 65534 0 Jun  9 14:14 /sys/fs/cgroup/system.slice/run-r076a82c36fcd4934b13bba47fcc8462e.service

% ps -o uid,gid,pid,cmd --pid 12045
  UID   GID     PID CMD
  100000 65534  12045 /usr/bin/sleep 300

% id -un 65534
nobody</code></pre>
<p>Now the cgroup is owned by <code>100000</code>. But the group ID (<code>gid</code>) under
which the process runs, and the group owner of the cgroup, is
<code>65534</code>. This is the host’s <code>nobody</code> account.</p>
<h3 id="specify-group">Specify <code>Group=</code> <a href="#specify-group" class="section">§</a></h3>
<p>In a user-namespaced container, ordinarily you would want both the
user <em>and</em> the group of the container process to be mapped into the
user namespace. Likewise, you would expect the cgroup to be owned
by a known (in the namespace) user. Setting the <code>Group=</code> property
should achieve this.</p>
<pre class="shell"><code>% sudo systemd-run -p Delegate=true -p User=100000 -p Group=100000 sleep 300      
Running as unit: run-re610d14cc0584a37a3d4099268df75d8.service

% systemctl status run-re610d14cc0584a37a3d4099268df75d8.service
● run-re610d14cc0584a37a3d4099268df75d8.service - /usr/bin/sleep 300
     Loaded: loaded (/run/systemd/transient/run-re610d14cc0584a37a3d4099268df75d8.service; transient)
  Transient: yes
     Active: active (running) since Wed 2021-06-09 14:24:58 AEST; 7s ago
   Main PID: 12131 (sleep)
      Tasks: 1 (limit: 2325)
     Memory: 184.0K
        CPU: 5ms
     CGroup: /system.slice/run-re610d14cc0584a37a3d4099268df75d8.service
             └─12131 /usr/bin/sleep 300

Jun 09 14:24:58 f33-1.ipa.local systemd[1]: Started /usr/bin/sleep 300.

% ls -nld /sys/fs/cgroup/system.slice/run-re610d14cc0584a37a3d4099268df75d8.service
drwxr-xr-x. 2 100000 100000 0 Jun  9 14:24 /sys/fs/cgroup/system.slice/run-re610d14cc0584a37a3d4099268df75d8.service

% ps -o uid,gid,pid,cmd --pid 12131
  UID   GID     PID CMD
100000 100000 12131 /usr/bin/sleep 300</code></pre>
<p>Finally, systemd is exhibiting the behaviour we desire.</p>
<h2 id="discussion-and-next-steps">Discussion and next steps <a href="#discussion-and-next-steps" class="section">§</a></h2>
<p>In summary, the findings from this investigation are:</p>
<ul>
<li><p>systemd changes the cgroup ownership of transient units according
to the <code>User=</code> and <code>Group=</code> properties, if and only if
<code>Delegate=true</code>.</p></li>
<li><p>systemd currently requires <code>User=</code> and <code>Group=</code> to refer to known
(via NSS) users and groups.</p></li>
<li><p>Unprivileged user systemd service manager instances lack the
privileges to set supplementary groups for the container process.
This is not a problem for the OpenShift use case, because it uses
the system service manager.</p></li>
</ul>
<p>As to the second point, I am curious why systemd behaves this way.
It does makes sense to query NSS to find out the shell, home
directory, and login name for setting up the execution environment.
But if there is no <code>passwd</code> record, why not synthesise one with
conservative defaults? Running processes as anonymous UIDs has a
valid use case—increasingly so, as adoption of user namespaces
increases. I <a href="https://github.com/systemd/systemd/issues/19781">filed an RFE (systemd#19781)</a> against
systemd to suggest relaxing this restriction, and inquire whether
this is a Bad Idea for some reason I don’t yet understand.</p>
<p>There are some alternative approaches that don’t require changing
systemd:</p>
<ul>
<li><p>Use <code>systemd-machined</code> to register a machine. It provides the
<code>org.freedesktop.machine1.Manager.RegisterMachine</code> D-Bus method
for this purpose. But <code>systemd-machined</code> is not used (or even
present) on OpenShift cluster nodes.</p></li>
<li><p>Implement, ship and configure an NSS module that synthesises
<code>passwd</code> records for user subordinate ID ranges. The <em>shadow</em>
project has <a href="https://github.com/shadow-maint/shadow/pull/321">defined an NSS interface</a> for subid
ranges. <em>libsubid</em>, part of <em>shadow</em>, will provide abstract subid
range lookups (forward and reverse). So a <em>libsubid</em>-based
solution to this should be possible. Unfortunately, <em>libsubid</em> is
not yet widely available as a shared library.</p>
<p>As an example, synthetic user records could have a username like
<code>subuid-{username}-{uid}</code>. The home directory and shell would be
<code>/</code> and <code>/sbin/nologin</code>, like the records synthesised by
<code>nss-systemd</code>.</p></li>
<li><p>Update the container runtime (<code>runc</code>) to <code>chown</code> the cgroup <em>after
systemd creates it</em>. In fact, this is what <code>systemd-nspawn</code> does.
This approach is nice because the only component to change is
<code>runc</code>—which had to change anyway, to add the logic to determine
the cgroup owner UID. To the best of my knowledge, on OpenShift
<code>runc</code> gets executed as <code>root</code> (on the node), so it should have
the permissions required to do this. Unless SELinux prevents it.</p></li>
</ul>
<p>Of these three options, modifying <code>runc</code> to <code>chown</code> the cgroup
directory seems the most promising. While I wait for feedback on
<a href="https://github.com/systemd/systemd/issues/19781">systemd#19781</a>, I will start hacking on <code>runc</code> and testing my
modifications.</p>]]></summary>
</entry>
<entry>
    <title>Using runc to explore the OCI Runtime Specification</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2021-05-27-oci-runtime-spec-runc.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2021-05-27-oci-runtime-spec-runc.html</id>
    <published>2021-05-27T00:00:00Z</published>
    <updated>2021-05-27T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="using-runc-to-explore-the-oci-runtime-specification">Using <code>runc</code> to explore the OCI Runtime Specification</h1>
<p>In recent posts I explored how to use user namespaces and cgroups v2
on OpenShift. My main objective is to run <em>systemd</em>-based workloads
in user namespaces that map to unprivileged users on the host. This
is a prerequisite to running <a href="https://www.freeipa.org/page/Main_Page">FreeIPA</a> <em>securely</em> in OpenShift,
and supporting multitenancy.</p>
<p>Independently, user namespaces and cgroups v2 already work well in
OpenShift. But for <em>systemd</em> support there is a critical gap: the
pod’s cgroup directory (mounted as <code>/sys/fs/cgroup/</code> in the
container) is owned by <code>root</code>—the <em>host’s</em> UID 0, which is unmapped
in the pod’s user namespace. As a consequence, the container’s main
process (<code>/sbin/init</code>, which is <em>systemd</em>) cannot manage cgroups,
and terminates.</p>
<p>To understand how to close this gap, I needed to become familiar
with the low-level container runtime behaviour. This post discusses
the relationship between various container runtime components and
demonstrates how to use <code>runc</code> directly to create and run
containers. I also outline some possible approaches to solving the
cgroup ownership issue.</p>
<h2 id="podman-kubernetes-cri-cri-o-runc-oh-my">Podman, Kubernetes, CRI, CRI-O, runc, oh my! <a href="#podman-kubernetes-cri-cri-o-runc-oh-my" class="section">§</a></h2>
<p>What actually happens when you “run a container”. Abstractly, a
container runtime sets up a <em>sandbox</em> and runs a process in it. The
sandbox consists of a set of namespaces (PID, UTS, mount, cgroup,
user, network, etc), and a restricted view of a filesystem (via
<code>chroot(2)</code> or similar mechanism).</p>
<p>There are several different container runtimes in widespread use.
In fact, there are several different <em>layers</em> of container runtime
with different purposes:</p>
<ul>
<li><p>End-user focused container runtimes include <a href="https://podman.io/"><em>Podman</em></a> and
<em>Docker</em>.</p></li>
<li><p>Kubernetes defines the <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md">Container Runtime Interface (CRI)</a>,
which it uses to run containers. Compliant implementations
include <em>containerd</em> and <a href="https://github.com/cri-o/cri-o"><em>CRI-O</em></a>.</p></li>
<li><p>The <em>Open Container Initiative (OCI)</em> <a href="https://github.com/opencontainers/runtime-spec">runtime spec</a> defines a
low-level container runtime interface. Implementations include
<a href="https://github.com/opencontainers/runc"><code>runc</code></a> and <a href="https://github.com/containers/crun"><em>crun</em></a>. OCI runtimes are designed to
be used by higher-level container runtimes. They are not friendly
for humans to use directly.</p></li>
</ul>
<p>Running a container usually involves a higher-level runtime <em>and</em> a
low-level runtime. For example, Podman uses an OCI runtime; crun by
default on Fedora but <code>runc</code> works fine too. OpenShift (which is
built on Kubernetes) uses CRI-O, which in turn uses <code>runc</code> (CRI-O
itself can use any OCI runtime).</p>
<h3 id="division-of-responsibilities">Division of responsibilities <a href="#division-of-responsibilities" class="section">§</a></h3>
<p>So, what are responsibilities of the higher-level runtime compared
to the OCI (or other low-level) runtime? In general the high-level
runtime is responsible for:</p>
<ul>
<li><p>Image management (pulling layers, preparing overlay filesystem)</p></li>
<li><p>Determining the mounts, environment, namespaces, resource limits
and security policies for the container</p></li>
<li><p>Network setup for the container</p></li>
<li><p>Metrics, accounting, etc.</p></li>
</ul>
<p>The steps performed by the low-level runtime include:</p>
<ul>
<li><p>Create and and enter required namespaces</p></li>
<li><p><code>chroot(2)</code> or <code>pivot_root(2)</code> to the specified root filesystem
path</p></li>
<li><p>Create requested mounts</p></li>
<li><p>Create cgroups and apply resource limits</p></li>
<li><p>Adjust capabilities and apply seccomp policy</p></li>
<li><p>Execute the container’s main process</p></li>
</ul>
<div class="note">
<p>I mentioned several features specific to Linux in the list above.
The OCI Runtime Specification also specifies Windows, Solaris and
VM-based workloads. This post assumes a Linux workload, so many
details are Linux-specific.</p>
</div>
<p>The above list is just a rough guide and not absolute. Depending on
use case the high-level runtime might perform some of the low-level
steps. For example, if container networking is required, Podman
might create the network namespace, setting up devices and routing.
Then, instead of asking the OCI runtime to create a network
namespace, it tells the runtime to enter the existing namespace.</p>
<h2 id="running-containers-via-runc">Running containers via <code>runc</code> <a href="#running-containers-via-runc" class="section">§</a></h2>
<p>Because our effort is targeting OpenShift, the rest of this post
mainly deals with <code>runc</code>.</p>
<div class="note">
<p>The functions demonstrated in this post were performed using <code>runc</code>
version 1.0.0-rc95+dev, which I built from source (commit
<code>19d75e1c</code>). The Fedora 33 and 34 repositories offer <code>runc</code> version
1.0.0-rc93, which <strong>does not work</strong>.</p>
</div>
<h3 id="clone-and-build">Clone and build <a href="#clone-and-build" class="section">§</a></h3>
<p>Install the Go compiler and <em>libseccomp</em> development headers:</p>
<pre class="shell"><code>% sudo dnf -y --quiet install libseccomp-devel

Installed:
  golang-1.16.3-1.fc34.x86_64
  golang-bin-1.16.3-1.fc34.x86_64
  golang-src-1.16.3-1.fc34.noarch
  libseccomp-devel-2.5.0-4.fc34.x86_64</code></pre>
<p>Clone the <code>runc</code> source code and build the program:</p>
<pre class="shell"><code>% mkdir -p ~/go/src/github.com/opencontainers
% cd ~/go/src/github.com/opencontainers
% git clone --quiet https://github.com/opencontainers/runc
% cd runc
% make --quiet
% ./runc --version
runc version 1.0.0-rc95+dev
commit: v1.0.0-rc95-31-g19d75e1c
spec: 1.0.2-dev
go: go1.16.3
libseccomp: 2.5.0</code></pre>
<h3 id="prepare-root-filesystem">Prepare root filesystem <a href="#prepare-root-filesystem" class="section">§</a></h3>
<p>I want to create a filesystem from my <em>systemd</em> based
<a href="https://quay.io/repository/ftweedal/test-nginx"><code>test-nginx</code></a> container image. To avoid
configuring overlay filesystems myself, I used Podman to create a
container, then exported the whole container filesystem, via
<code>tar(1)</code>, to a local directory:</p>
<pre class="shell"><code>% podman create --quiet quay.io/ftweedal/test-nginx
e97930b3…
% mkdir rootfs
% podman export e97930b3 | tar -xC rootfs
% ls rootfs
bin  dev home lib64      media opt  root sbin sys usr
boot etc lib  lost+found mnt   proc run  srv  tmp var</code></pre>
<h3 id="create-config.json">Create <code>config.json</code> <a href="#create-config.json" class="section">§</a></h3>
<p>OCI runtimes read the container configuration from <code>config.json</code> in
the <em>bundle</em> directory. (<code>runc</code> uses the current directory as the
default bundle directory). The <code>runc spec</code> command generates a
sample <code>config.json</code> which can serve as a starting point:</p>
<pre class="shell"><code>% ./runc spec --rootless
% file config.json
config.json: JSON data
% jq -c .process.args &lt; config.json
[&quot;sh&quot;]</code></pre>
<p>We can see that <code>runc</code> created the sample config. The command to
execute is <code>sh(1)</code>. Let’s change that to <code>/sbin/init</code>:</p>
<pre class="shell"><code>% mv config.json config.json.orig
% jq &#39;.process.args=[&quot;/sbin/init&quot;]&#39; config.json.orig \
    &gt; config.json</code></pre>
<div class="notes">
<p><code>jq(1)</code> cannot operate on JSON files in situ, so you first have to
copy or move the input file. The <a href="https://linux.die.net/man/1/sponge"><code>sponge(1)</code></a> command,
provided by the <em>moreutils</em> package, offers an alternative approach.</p>
</div>
<h3 id="run-container">Run container <a href="#run-container" class="section">§</a></h3>
<p>Now we can try and run the container:</p>
<pre class="shell"><code>% ./runc --systemd-cgroup run test
Mount failed for selinuxfs on /sys/fs/selinux:  No such file or directory
Another IMA custom policy has already been loaded, ignoring: Permission denied
Failed to mount tmpfs at /run: Operation not permitted
[!!!!!!] Failed to mount API filesystems.
Freezing execution.</code></pre>
<p>That didn’t work. systemd failed to mount a <code>tmpfs</code> (temporary,
memory-based filesystem) at <code>/tmp</code>, and halted. The container
itself was still running (but frozen). I was able to kill it from
another terminal:</p>
<pre class="shell"><code>% ./runc list --quiet
test
% ./runc kill test KILL
% ./runc list --quiet</code></pre>
<p>It turned out that in addition to the process to run, the config
requires several changes to successfully run a <em>systemd</em>-based
container. I will not repeat the whole process here, but I achieved
a working config through a combination of trial-and-error, and
comparison against OCI configurations produced by Podman. The
following <a href="https://stedolan.github.io/jq/manual/"><code>jq(1)</code></a> program performs the required modifications:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode numberSource json numberLines"><code class="sourceCode json"><span id="cb8-1"><a href="#cb8-1"></a><span class="er">.process.args</span> <span class="er">=</span> <span class="ot">[</span><span class="st">&quot;/sbin/init&quot;</span><span class="ot">]</span></span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="er">|</span> <span class="er">.process.env</span> <span class="er">|=</span> <span class="er">.</span> <span class="er">+</span> <span class="ot">[</span><span class="st">&quot;container=oci&quot;</span><span class="ot">]</span></span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="er">|</span> <span class="ot">[</span><span class="fu">{</span><span class="dt">&quot;containerID&quot;</span><span class="fu">:</span><span class="dv">1</span><span class="fu">,</span><span class="dt">&quot;hostID&quot;</span><span class="fu">:</span><span class="dv">100000</span><span class="fu">,</span><span class="dt">&quot;size&quot;</span><span class="fu">:</span><span class="dv">65536</span><span class="fu">}</span><span class="ot">]</span> <span class="er">as</span> <span class="er">$idmap</span></span>
<span id="cb8-4"><a href="#cb8-4"></a><span class="er">|</span> <span class="er">.linux.uidMappings</span> <span class="er">|=</span> <span class="er">.</span> <span class="er">+</span> <span class="er">$idmap</span></span>
<span id="cb8-5"><a href="#cb8-5"></a><span class="er">|</span> <span class="er">.linux.gidMappings</span> <span class="er">|=</span> <span class="er">.</span> <span class="er">+</span> <span class="er">$idmap</span></span>
<span id="cb8-6"><a href="#cb8-6"></a><span class="er">|</span> <span class="er">.linux.cgroupsPath</span> <span class="er">=</span> <span class="er">&quot;user.slice:runc:test&quot;</span></span>
<span id="cb8-7"><a href="#cb8-7"></a><span class="er">|</span> <span class="er">.linux.namespaces</span> <span class="er">|=</span> <span class="er">.</span> <span class="er">+</span> <span class="ot">[</span><span class="fu">{</span><span class="dt">&quot;type&quot;</span><span class="fu">:</span><span class="st">&quot;network&quot;</span><span class="fu">}</span><span class="ot">]</span></span>
<span id="cb8-8"><a href="#cb8-8"></a><span class="er">|</span> <span class="er">.process.capabilities</span><span class="ot">[]</span> <span class="er">=</span></span>
<span id="cb8-9"><a href="#cb8-9"></a>  <span class="ot">[</span> <span class="st">&quot;CAP_CHOWN&quot;</span><span class="ot">,</span> <span class="st">&quot;CAP_FOWNER&quot;</span><span class="ot">,</span> <span class="st">&quot;CAP_SETUID&quot;</span><span class="ot">,</span> <span class="st">&quot;CAP_SETGID&quot;</span><span class="ot">,</span></span>
<span id="cb8-10"><a href="#cb8-10"></a>    <span class="st">&quot;CAP_SETPCAP&quot;</span><span class="ot">,</span> <span class="st">&quot;CAP_NET_BIND_SERVICE&quot;</span> <span class="ot">]</span></span>
<span id="cb8-11"><a href="#cb8-11"></a><span class="er">|</span> <span class="fu">{</span><span class="dt">&quot;type&quot;</span><span class="fu">:</span> <span class="st">&quot;tmpfs&quot;</span><span class="fu">,</span></span>
<span id="cb8-12"><a href="#cb8-12"></a>   <span class="dt">&quot;source&quot;</span><span class="fu">:</span> <span class="st">&quot;tmpfs&quot;</span><span class="fu">,</span></span>
<span id="cb8-13"><a href="#cb8-13"></a>   <span class="dt">&quot;options&quot;</span><span class="fu">:</span> <span class="ot">[</span><span class="st">&quot;rw&quot;</span><span class="ot">,</span><span class="st">&quot;rprivate&quot;</span><span class="ot">,</span><span class="st">&quot;nosuid&quot;</span><span class="ot">,</span><span class="st">&quot;nodev&quot;</span><span class="ot">,</span><span class="st">&quot;tmpcopyup&quot;</span><span class="ot">]</span></span>
<span id="cb8-14"><a href="#cb8-14"></a>  <span class="fu">}</span> <span class="er">as</span> <span class="er">$tmpfs</span></span>
<span id="cb8-15"><a href="#cb8-15"></a><span class="er">|</span> <span class="er">.mounts</span> <span class="er">|=</span> <span class="ot">[</span><span class="fu">{</span><span class="dt">&quot;destination&quot;</span><span class="fu">:</span><span class="st">&quot;/var/log&quot;</span><span class="fu">}</span> <span class="er">+</span> <span class="er">$tmpfs</span><span class="ot">]</span> <span class="er">+</span> <span class="er">.</span></span>
<span id="cb8-16"><a href="#cb8-16"></a><span class="er">|</span> <span class="er">.mounts</span> <span class="er">|=</span> <span class="ot">[</span><span class="fu">{</span><span class="dt">&quot;destination&quot;</span><span class="fu">:</span><span class="st">&quot;/tmp&quot;</span><span class="fu">}</span> <span class="er">+</span> <span class="er">$tmpfs</span><span class="ot">]</span> <span class="er">+</span> <span class="er">.</span></span>
<span id="cb8-17"><a href="#cb8-17"></a><span class="er">|</span> <span class="er">.mounts</span> <span class="er">|=</span> <span class="ot">[</span><span class="fu">{</span><span class="dt">&quot;destination&quot;</span><span class="fu">:</span><span class="st">&quot;/run&quot;</span><span class="fu">}</span> <span class="er">+</span> <span class="er">$tmpfs</span><span class="ot">]</span> <span class="er">+</span> <span class="er">.</span></span></code></pre></div>
<p>This program performs the following actions:</p>
<ul>
<li><p>Set the container process to <code>/sbin/init</code> (which is <em>systemd</em>).</p></li>
<li><p>Set the <code>$container</code> environment variable, as <a href="https://systemd.io/CONTAINER_INTERFACE/#environment-variables">required by
systemd</a>.</p></li>
<li><p>Add UID and GID mappings for IDs <code>1</code>–<code>65536</code> in the container’s
user namespace. The host range (started at <code>100000</code>) is taken
from my user account’s assigned ranges in <code>/etc/subuid</code> and
<code>/etc/subgid</code>. <strong>You may need a different number.</strong> The mapping
for the container’s UID <code>0</code> to my user account already exists in
the config.</p></li>
<li><p>Set the container’s cgroup path. A non-absolute path is
interpreted relative to a runtime-determined location.</p></li>
<li><p>Tell the runtime to create a network namespace. Without this,
the container will have no network stack and <em>nginx</em> won’t run.</p></li>
<li><p>Set the <a href="https://linux.die.net/man/7/capabilities">capabilities</a> required by the container. <em>systemd</em>
requires all of these capabilities, although
<code>CAP_NET_BIND_SERVICE</code> is only required for network name
resolution (<em>systemd-resolved</em>). And <em>nginx</em>.</p></li>
<li><p>Tell the runtime to mount <code>tmpfs</code> filesystems at <code>/run</code>, <code>/tmp</code>
and <code>/var/log</code>.</p></li>
</ul>
<p>I ran the program to modify the config, then started the container:</p>
<pre class="shell"><code>% jq --from-file filter.jq config.json.orig &gt; config.json
% ./runc --systemd-cgroup run test
systemd v246.10-1.fc33 running in system mode. (+PAM …
Detected virtualization container-other.
Detected architecture x86-64.

Welcome to Fedora 33 (Container Image)!

…

[  OK  ] Started The nginx HTTP and reverse proxy server.
[  OK  ] Reached target Multi-User System.
[  OK  ] Reached target Graphical Interface.
         Starting Update UTMP about System Runlevel Changes.
[  OK  ] Finished Update UTMP about System Runlevel Changes.

Fedora 33 (Container Image)
Kernel 5.11.17-300.fc34.x86_64 on an x86_64 (console)

runc login:</code></pre>
<p>OK! <em>systemd</em> initialised the system properly and started <em>nginx</em>.
We can confirm <em>nginx</em> is running properly by running <code>curl</code> in the
container:</p>
<pre class="shell"><code>% ./runc exec test curl --silent --head localhost:80
HTTP/1.1 200 OK
Server: nginx/1.18.0
Date: Thu, 27 May 2021 02:29:58 GMT
Content-Type: text/html
Content-Length: 5564
Last-Modified: Mon, 27 Jul 2020 22:20:49 GMT
Connection: keep-alive
ETag: &quot;5f1f5341-15bc&quot;
Accept-Ranges: bytes</code></pre>
<p>At this point we cannot access <em>nginx</em> from outside the container.
That’s fine; I don’t need to work out how to do that. Not today,
anyhow.</p>
<h2 id="how-runc-creates-cgroups">How <code>runc</code> creates cgroups <a href="#how-runc-creates-cgroups" class="section">§</a></h2>
<p><code>runc</code> manages container cgroups via the host’s <em>systemd</em> service.
Specifically, it communicates with <em>systemd</em> over DBus to create a
<a href="https://www.freedesktop.org/wiki/Software/systemd/ControlGroupInterface/">transient scope</a> for the container. Then it binds the
container cgroup namespace to this new scope.</p>
<p>Observe that the inode of <code>/sys/fs/cgroup/</code> in the container is the
same as the scope created for the container by <em>systemd</em> on the
host:</p>
<pre class="shell"><code>% ./runc exec test ls -aldi /sys/fs/cgroup
64977 drwxr-xr-x. 5 root root 0 May 27 02:26 /sys/fs/cgroup

% ls -aldi /sys/fs/cgroup/user.slice/user-1000.slice/user@1000.service/user.slice/runc-test.scope 
64977 drwxr-xr-x. 5 ftweedal ftweedal 0 May 27 12:26 /sys/fs/cgroup/user.slice/user-1000.slice/user@1000.service/user.slice/runc-test.scope</code></pre>
<p>The mapping of <code>root</code> in the container’s user namespace to
<code>ftweedal</code> is confirmed by the UID map of the container process:</p>
<pre class="shell"><code>% id --user ftweedal
1000
% ./runc list -f json | jq &#39;.[]|select(.id=&quot;test&quot;).pid&#39;
186718
% cat /proc/186718/uid_map
         0       1000          1
         1     100000      65536</code></pre>
<h2 id="next-steps">Next steps <a href="#next-steps" class="section">§</a></h2>
<p><em>systemd</em> is running properly in the container, but <code>root</code> in the
container is mapped to my main user account. The container is not
as isolated as I would like it to be. A partial sandbox escape
could lead to the containerised process(es) messing with local
files, or other processes owned by my user (including other
containers).</p>
<p>User-namespaced containers in OpenShift (via CRI-O annotations) are
allocated non-overlapping host ID ranges. All the host IDs are
essentially anonymous. I confirmed this in <a href="2021-03-10-openshift-user-namespace-multi-user.html">a previous blog
post</a>. That is
good! But the container’s cgroup is owned by the <em>host’s</em> UID 0,
which is unmapped in the container. <em>systemd</em>-based workloads
cannot run because the container cannot write to its cgroupfs.</p>
<p>Therefore, the next steps in my investigation are:</p>
<ol type="1">
<li><p>Alter the ID mappings to use a single mapping of only “anonymous”
users. This is a simple change to the OCI config. The host IDs
still have to come from the user’s allocated sub-ID range.</p></li>
<li><p>Find (or implement) a way to change the ownership of the
container’s cgroup scope to the <strong>container’s</strong> UID 0.</p></li>
</ol>
<p>When using the <em>systemd</em> cgroup manager, <code>runc</code> uses the <a href="https://www.freedesktop.org/wiki/Software/systemd/ControlGroupInterface/"><em>transient
unit API</em></a> to ask <em>systemd</em> to create a new scope for the
container. I am still learning about this API. Perhaps there is a
way to specify a different ownership for the new scope or service.
If so, we should be able to avoid changes to higher-level container
runtimes like CRI-O. That would be the best outcome.</p>
<p>Otherwise, I will investigate whether we could use the OCI
<code>createRuntime</code> hook to <code>chown(2)</code> the container’s cgroup scope.
Unfortunately, the semantics of <code>createRuntime</code> is currently
underspecified. The specification is ambiguous about whether the
containers cgroup scope exists when this hook is executed. If this
approach is valid, we will have to update CRI-O to add the relevant
hook command to the OCI config.</p>
<p>Another possible approach is for the high-level runtime to perform
the ownership change itself. This would be done after it invokes
the OCI runtime’s <code>create</code> command, but before it invokes <code>start</code>.
(See also the OCI <a href="https://github.com/opencontainers/runtime-spec/blob/master/runtime.md#lifecycle">container lifecycle description</a>). However, on
OpenShift CRI-O runs as user <code>containers</code> and the container’s cgroup
scope is owned by <code>root</code>. So I have doubts about the viability of
this approach, as well as the OCI hook approach.</p>
<p>Whatever the outcome, there will certainly be more blog posts as I
continue this long-running investigation. I still have much to
learn as I struggle towards the goal of systemd-based workloads
running securely on OpenShift.</p>]]></summary>
</entry>

</feed>
