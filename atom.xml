<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Fraser's IdM Blog</title>
    <link href="https://frasertweedale.github.io/blog-redhat/atom.xml" rel="self" />
    <link href="https://frasertweedale.github.io/blog-redhat" />
    <id>https://frasertweedale.github.io/blog-redhat/atom.xml</id>
    <author>
        <name>Fraser Tweedale</name>
        <email>frase@frase.id.au</email>
    </author>
    <updated>2020-12-08T00:00:00Z</updated>
    <entry>
    <title>Kubernetes DNS Service Discovery limitations</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2020-12-08-k8s-srv-limitation.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2020-12-08-k8s-srv-limitation.html</id>
    <published>2020-12-08T00:00:00Z</published>
    <updated>2020-12-08T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="kubernetes-dns-service-discovery-limitations">Kubernetes DNS Service Discovery limitations</h1>
<p>Kubernetes <em>Service</em> objects expose applications running in Pods as network services. For each combination of service name, port and associated Pod, the Kubernetes DNS system creates a DNS <code>SRV</code> record that can be used for service discovery.</p>
<p>In this post I demonstrate a deficiency in this system that obstructs important, real-world use cases, and sketch potential solutions.</p>
<h2 id="overview-of-kubernetes-services-and-dns">Overview of Kubernetes Services and DNS <a href="#overview-of-kubernetes-services-and-dns">§</a></h2>
<p>The following Service definition defines an LDAP service:</p>
<pre><code>$ oc create -f service-test.yaml 
apiVersion: v1
kind: Service
metadata:
  name: service-test
  labels:
    app: service-test
spec:
  selector:
    app: service-test
  clusterIP: None
  ports:
  - name: ldap
    protocol: TCP
    port: 389

$ oc create -f service-test.yaml
service/service-test created</code></pre>
<p>The Service controller creates <em>Endpoint</em> objects to associating each of the Service <code>ports</code> with each Pod matching the Service <code>selector</code>. If there are no matching pods, there are no endpoints:</p>
<pre><code>$ oc get endpoints service-test
NAME           ENDPOINTS   AGE
service-test   &lt;none&gt;      8m1s</code></pre>
<p>If we add a matching pod:</p>
<pre><code>$ cat pod-service-test.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: service-test
  labels:
    app: service-test
spec:
  containers:
  - name: service-test
    image: freeipa/freeipa-server:fedora-31
    command: [&quot;sleep&quot;, &quot;3601&quot;]

$ oc create -f pod-service-test.yaml 
pod/service-test created</code></pre>
<p>Then the Service controller creates an endpoint that maps the Service to the Pod:</p>
<pre><code>$ oc get endpoints service-test
NAME           ENDPOINTS         AGE
service-test   10.129.2.13:389   16m

$ oc get -o yaml endpoints service-test
apiVersion: v1
kind: Endpoints
metadata:
  labels:
    app: service-test
    service.kubernetes.io/headless: &quot;&quot;
  ... 
subsets:
- addresses:
  - ip: 10.129.2.13
    nodeName: ft-47dev-2-27h8r-worker-0-f8bnl
    targetRef:
      kind: Pod
      name: service-test
      namespace: test
      resourceVersion: &quot;4556709&quot;
      uid: 296030f5-8dff-4f69-be96-ce6f0aa12653
  ports:
  - name: ldap
    port: 389
    protocol: TCP</code></pre>
<p>Cluster DNS systems (there are different implementations, e.g. <a href="https://github.com/kubernetes/dns">kubedns</a>, and the OpenShift <a href="https://github.com/openshift/cluster-dns-operator">Cluster DNS Operator</a>) use the Endpoints objects to manage DNS records for applications running in the cluster. In particular, it creates <code>SRV</code> records mapping each service <code>name</code> and <code>protocol</code> combination to the pod(s) that provide that service. The behaviour is defined in the <a href="">Kubernetes DNS-Based Service Discovery specification</a>.</p>
<p>The SRV record owner name has the form:</p>
<pre><code>_&lt;port&gt;._&lt;proto&gt;.&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;.</code></pre>
<p>where <code>ns</code> is the project namespace and <code>zone</code> is the cluster DNS zone. The objects created above result in the follow <code>SRV</code> and <code>A</code> records:</p>
<pre><code>$ oc rsh service-test

sh-5.0# dig +short SRV \
    _ldap._tcp.service-test.test.svc.cluster.local
0 100 389 10-129-2-13.service-test.test.svc.cluster.local.

sh-5.0# dig +short A \
    10-129-2-13.service-test.test.svc.cluster.local
10.129.2.13</code></pre>
<p>For more information above DNS <code>SRV</code> records, see <a href="https://tools.ietf.org/html/rfc2782">RFC 2782</a>.</p>
<h2 id="kubernetes-srv-limitation">Kubernetes SRV limitation <a href="#kubernetes-srv-limitation">§</a></h2>
<p>Some services operate over TCP, some over UDP. And some operate over <em>both</em> TCP and UDP. Two examples are DNS and Kerberos. <code>SRV</code> records are of particular importance for Kerberos; they are used (<a href="https://web.mit.edu/kerberos/krb5-devel/doc/admin/realm_config.html#hostnames-for-kdcs">widely</a>, by <a href="https://docs.microsoft.com/en-us/openspecs/windows_protocols/ms-adts/7fcdce70-5205-44d6-9c3a-260e616a2f04">multiple</a> <a href="https://www.freeipa.org/page/V4/DNS_Location_Mechanism">implementations</a>) for KDC discovery.</p>
<p>So to host a Kerberos KDC in Kubernetes and enable service discovery, we need two sets of SRV records: <code>_kerberos._tcp</code> and <code>_kerberos._udp</code>. And likewise for the <code>kpasswd</code> and <code>kerberos-master</code> service names. There could be (probably are) other protocols where a similar arrangement is required.</p>
<p>So, let’s update the Service object and add the <code>kerberos</code> ServicePort specs:</p>
<pre><code>$ cat service-test.yaml 
apiVersion: v1
kind: Service
metadata:
  name: service-test
  labels:
    app: service-test
spec:
  selector:
    app: service-test
  clusterIP: None
  ports:
  - name: ldap
    protocol: TCP
    port: 389
  - name: kerberos
    protocol: TCP
    port: 88
  - name: kerberos
    protocol: UDP
    port: 88

$ oc replace -f service-test.yaml
The Service &quot;service-test&quot; is invalid:
spec.ports[2].name: Duplicate value: &quot;kerberos&quot;</code></pre>
<p>Well, that’s a shame. Kerberos does not admit this important use case.</p>
<h3 id="endpoints-do-not-have-the-limitation">Endpoints do not have the limitation <a href="#endpoints-do-not-have-the-limitation">§</a></h3>
<p>Interestingly, the Endpoints type does not have this limitation. The Service controller automatically creates Endpoints objects for Services. The ServicePorts are (as far as I can tell) copied across to the Endpoints object.</p>
<p>I can manually replace the <code>endpoints/service-test</code> object (see above) with the following spec that includes the “duplicate” <code>kerberos</code> port:</p>
<pre><code>$ cat endpoints.yaml
apiVersion: v1
kind: Endpoints
metadata:
  creationTimestamp: &quot;2020-12-07T03:51:30Z&quot;
  labels:
    app: service-test
    service.kubernetes.io/headless: &quot;&quot;
  name: service-test
subsets:
- addresses:
  - ip: 10.129.2.13
    nodeName: ft-47dev-2-27h8r-worker-0-f8bnl
    targetRef:
      kind: Pod
      name: service-test
      namespace: test
      resourceVersion: &quot;5522680&quot;
      uid: 296030f5-8dff-4f69-be96-ce6f0aa12653
  ports:
  - name: ldap
    port: 389
    protocol: TCP
  - name: kerberos
    port: 88
    protocol: TCP
  - name: kerberos
    port: 88
    protocol: UDP

$ oc replace -f endpoints.yaml
endpoints/service-test replaced</code></pre>
<p>The object was accepted! Observe that the DNS system responds and creates <em>both</em> the <code>_kerberos._tcp</code> and <code>_kerberos._udp</code> <code>SRV</code> records:</p>
<pre><code>$ oc rsh service-test

sh-5.0# dig +short SRV \
    _kerberos._tcp.service-test.test.svc.cluster.local
0 100 88 10-129-2-13.service-test.test.svc.cluster.local.

sh-5.0# dig +short SRV \
    _kerberos._udp.service-test.test.svc.cluster.local
0 100 88 10-129-2-13.service-test.test.svc.cluster.local.</code></pre>
<p>Therefore it seems the scope of this problem is limited to validation and processing of the <code>Service</code> object. Other components of Kubernetes (Endpoint validation and the Cluster DNS Operator, at least) can already handle this use case.</p>
<h2 id="possible-resolutions">Possible resolutions <a href="#possible-resolutions">§</a></h2>
<p>I am not aware of any workarounds, but I see two possible approaches to resolving this issue.</p>
<p>One approach is to relax the uniqueness check. Instead of checking for uniqueness of ServicePort <code>name</code>, check for the uniqueness of the <code>name</code>/<code>protocol</code> pair. This is conceptually simple but I am not familiar enough with Kubernetes internals to judge the feasibility or technical tradeoffs of this approach. For users, nothing changes (except the example above would work!)</p>
<p>Another approach is to add a new ServicePort field to specify the actual DNS service label to use. For the sake of discussion I’ll call it <code>serviceName</code>. It would be optional, defaulting to the value of <code>name</code>. This means <code>name</code> can still be the “primary key”, but the approach requires <em>another</em> uniqueness check on the <code>serviceName</code>/<code>protocol</code> pair. In our use case the configuration would look like:</p>
<pre><code>...
ports:
- name: ldap
  protocol: TCP
  port: 389
- name: kerberos-tcp
  serviceName: kerberos
  protocol: TCP
  port: 88
- name: kerberos-udp
  serviceName: kerberos
  protocol: UDP
  port: 88</code></pre>
<p>From a UX perspective I prefer the first approach, because there are no changes or additions to the ServicePort configuration schema. But to maintain compatibility with programs that assume that <code>name</code> is unique (as is currently enforced), it might be necessary to introduce a new field.</p>
<h2 id="next-steps">Next steps <a href="#next-steps">§</a></h2>
<p>I <a href="https://github.com/kubernetes/kubernetes/issues/97149">filed a bug report</a> and submitted a <a href="">proof-of-concept pull request</a> to bring attention to the problem and solicit feedback from Kubernetes and OpenShift DNS experts. It might be necessary to submit a <a href="https://github.com/kubernetes/enhancements/blob/master/keps/README.md">Kubernetes Enhancement Proposal</a> (KEP), but that seems (as a Kubernetes outsider) a long and windy road to landing what is a conceptually small change.</p>]]></summary>
</entry>
<entry>
    <title>Pod hostnames and FQDNs</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2020-12-05-pod-hostname-fqdn.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2020-12-05-pod-hostname-fqdn.html</id>
    <published>2020-12-05T00:00:00Z</published>
    <updated>2020-12-05T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="pod-hostnames-and-fqdns">Pod hostnames and FQDNs</h1>
<p>Some complex or legacy applications make strict and pervasive assumptions about their execution environment. Relying on the host having a <em>fully qualified domain name (FQDN)</em> is an example of this kind of assumption. Indeed this is a particularly thorny kind of assumption because there are several ways an application can query the hostname, and they don’t always agree!</p>
<p>It is not surprising that we have hit this particular issue during our effort to containerise FreeIPA and operationalise it for OpenShift. Whereas container runtimes like Podman and Docker offer full control of a container’s FQDN, Kubernetes (and by extension OpenShift) is more strongly opinionated. By default, a Kubernetes pod has only a short name, not a fully qualified domain name. There are limited ways to configure a pod’s hostname and FQDN. Furthermore, there is currently no way to use a pod’s FQDN as the (Kernel) hostname.</p>
<p>In this post I will outline the challenges and document the attempted workarounds as we try to make FreeIPA run in OpenShift in spite of the Kubernetes hostname restriction.</p>
<h2 id="querying-the-fqdn">Querying the FQDN <a href="#querying-the-fqdn">§</a></h2>
<p>There are several ways an a program can query the host’s hostname.</p>
<ul>
<li>Read <code>/etc/hostname</code>. The name in this file may or may not be fully qualified.</li>
<li>Via the POSIX <code>uname(2)</code> system call. The <code>nodename</code> field in the <code>utsname</code> struct returned by this system call is intended to hold a network node name. Once again, it could be a short name or fully qualified. Furthermore, on most systems it is limited to 64 bytes. From userland you can use the <code>uname(1)</code> program or <code>uname(3)</code> library routine. The <code>gethostname(2)</code> and <code>gethostname(3)</code> are another way to retrieve this datum.</li>
<li>On systems that use <em>systemd</em> the <code>hostnamectl(1)</code> program can be used to get or set the hostname. Once again, the hostname is not necessarily fully qualified. <code>hostnamectl</code> distinguishes between the <em>static</em> hostname (set at boot by static configuration) and <em>transient</em> hostname (derived from network configuration). These can be queried separately.</li>
<li>A program could query DNS PTR records for its non-loopback IP addresses. This approach could yield zero, one or multiple FQDNs.</li>
<li>The <code>getaddrinfo(3)</code> routine when invoked with the <code>AI_CANONNAME</code> flag can return a FQDN for a given hostname (e.g. the name return by <code>gethostname(2)</code>. This allows any <em>Name Service Switch (NSS)</em> plugin to provide a canonical FQDN for a short name. NSS is usually configured to map hostnames using the data from <code>/etc/hosts</code>, but there are other plugins including for <em>systemd-resolved</em>, <em>dns</em> and <em>sss</em> (SSSD). From the command line, <code>hostname --fqdn</code> or <code>hostname --all-fqdns</code> will return result(s) from <code>getaddrinfo(3)</code>.</li>
</ul>
<p>Side-note: the “UTS” in <code>utsname</code> stands for <em>Unix Timesharing System</em>. Container runtimes can set a unique UTS hostname in each container because each container (or pod) has a unique <a href="https://www.man7.org/linux/man-pages/man7/uts_namespaces.7.html">UTS namespace</a>.</p>
<h2 id="auditing-freeipas-fqdn-query-behaviour">Auditing FreeIPA’s FQDN query behaviour <a href="#auditing-freeipas-fqdn-query-behaviour">§</a></h2>
<p>In order to decide how to proceed, we first needed to audit both FreeIPA and its dependencies to see how they query the hostname and host FQDN. I have published <a href="https://docs.google.com/document/d/e/2PACX-1vQzxjMw3eqkpuPfqaLbCW-GN8gwS1QvFjrs9TnPM02DMfNqBVSGapqITvAyZyxc2TN9jJShJrbqGayC/pub">the results of this audit</a>. It is perhaps not exhaustive, but hopefully fairly thorough.</p>
<h2 id="pod-hostname-configuration">Pod hostname configuration <a href="#pod-hostname-configuration">§</a></h2>
<p>We assume that the operator (human or machine) will create pods with deterministic FQDN. That is, it knows what the pod’s FQDN should be. Or to be more precise, the operator knows what it wants the application(s) running in the pod to recognise as the host FQDN. These are not necessarily the same thing (more on that in the next section).</p>
<p>First, let’s investigate how OpenShift configures pod hostnames. I created a standalone pod with no associated services and shelled into it to query the FQDN in various ways. The pod configuration:</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: test
spec:
  containers:
  - name: test
    image: freeipa/freeipa-server:fedora-31
    command: [&quot;sleep&quot;, &quot;3666&quot;]</code></pre>
<p>Interactive session:</p>
<pre><code>$ oc rsh test
sh-5.0$ uname -n
test
sh-5.0$ hostname
test
sh-5.0$ hostname --fqdn
test
sh-5.0$ cat /etc/hostname
test
sh-5.0$ hostnamectl get-hostname
System has not been booted with systemd as init system (PID 1).
Can&#39;t operate.
Failed to create bus connection: Host is down</code></pre>
<p>All the various ways of querying the hostname return <code>test</code>, except for <code>hostnamectl(1)</code> which fails because the container doesn’t use systemd.</p>
<p>What other ways can Kubernetes configure the hostname? <a href="">PodSpec</a> has a <code>hostname</code> field for configuring the pod hostname:</p>
<pre><code>$ grep -C2 hostname pod-test.yaml 
  name: test
spec:
  hostname: test.example.com
  containers:
  - name: test</code></pre>
<p>Unfortunately, <code>hostname</code> only accepts a short name:</p>
<pre><code>$ oc create -f pod-test.yaml
The Pod &quot;test&quot; is invalid: spec.hostname: Invalid value:
&quot;test.example.com&quot;: a DNS-1123 label must consist of lower case
alphanumeric characters or &#39;-&#39;, and must start and end with an
alphanumeric character (e.g. &#39;my-name&#39;,  or &#39;123-abc&#39;, regex used
for validation is &#39;[a-z0-9]([-a-z0-9]*[a-z0-9])?&#39;)</code></pre>
<p>Some container runtimes (e.g. Podman) do allow full control over the UTS hostname. But it seems Kubernetes is (for the time being) opinionated and only allows a short name.</p>
<p>Another <code>PodSpec</code> field of interest is <code>subdomain</code>. The documentation says:</p>
<blockquote>
<p>If specified, the fully qualified Pod hostname will be “&lt;hostname&gt;.&lt;subdomain&gt;.&lt;pod namespace&gt;.svc.&lt;cluster domain&gt;”. If not specified, the pod will not have a domainname at all.</p>
</blockquote>
<p>Sounds promising. Let’s give it a go.</p>
<pre><code>$ grep -C2 subdomain pod-test.yaml 
  name: test
spec:
  subdomain: subdomain
  containers:
  - name: test</code></pre>
<pre><code>$ oc rsh test
sh-5.0# uname -n 
test
sh-5.0# hostname
test
sh-5.0# hostname --fqdn
test.subdomain.test.svc.cluster.local</code></pre>
<p><code>hostname --fqdn</code> has returned a fully-qualified name. This works because the FQDN appears in <code>/etc/hosts</code> (associated with the IP address of the pod). My understanding is that <em>kubelet</em> uses a <code>ConfigMap</code> to inject this configuration into the pod.</p>
<pre><code>sh-5.0# grep subdomain /etc/hosts
10.129.3.84     test.subdomain.test.svc.cluster.local   test</code></pre>
<p>The preceding examples involve pods that I created directly. The configurations of pods that are created indirectly are under the (partial) control of the corresponding controllers. For example, pods created by the <code>StatefulSet</code> controller have their <code>subdomain</code> field set to the <code>name</code> of the <code>StatefulSet</code>.</p>
<h3 id="upcoming-changes">Upcoming changes <a href="#upcoming-changes">§</a></h3>
<p>An <a href="https://github.com/kubernetes/enhancements/issues/1797">upcoming Kubernetes enhancement</a> will allow pods to specify that its UTS hostname should be set to the pod FQDN (if the pod has an FQDN). This enhancement will introduces a new <code>setHostnameAsFQDN</code> field to the <code>PodSpec</code>. It is currently scheduled to land as <em>alpha</em> in Kubernetes v1.19, move to <em>beta</em> in v1.20 and become <em>stable</em> in v1.22.</p>
<h2 id="freeipa-changes">FreeIPA changes <a href="#freeipa-changes">§</a></h2>
<p>With sufficient craftiness, or code changes, or network configuration changes, or some combination thereof, it is possible to convince a program that it’s FQDN is a particular value. Although Kubernetes and OpenShift currently offer few ways to configure the pod (UTS) hostname, the operator could use some mechanism (e.g. pod environment variables or a <code>ConfigMap</code>, along with changes to application code) to ensure that each application instance “knows” its correct FQDN.</p>
<p>The hostname query audit revealed that FreeIPA asks for the host FQDN or the system hostname (in order to check that it is a FQDN) in lots of places and uses different query mechanisms. If we find all those places we can abstract away the check. In practice this means one common interface for FreeIPA’s C code and one for the Python code.</p>
<p>With hostname query logic abstracted behind these interfaces, we can perform the lookup in whatever way is appropriate for the deployment environment. For a traditional deployment, we use <code>gethostname(3)</code> and <code>getaddrinfo(3)</code> with <code>AI_CANONNAME</code>. But in an OpenShift deployment we can instead return a value supplied via a <code>ConfigMap</code> or other appropriate mechanism.</p>
<p>Upstream pull request <a href="https://github.com/freeipa/freeipa/pull/5107">#5107</a> implemented this change. It consolidated the hostname query behaviour into new C and Python routines. It did not implement alternative behaviour for other environments such as OpenShift, but abstracting the query behind a single interface (for each language) makes it easy to do this later. Whether we would use an environment variable, <code>ConfigMap</code>, or some other mechanism does not need to be decided at this time.</p>
<h2 id="next-steps">Next steps <a href="#next-steps">§</a></h2>
<p>The investigation into hostname/FQDN query behaviour of FreeIPA’s dependencies continues. In particular, we have not yet undertaken a thorough investigation of Samba, which is used for Active Directory trust support. Also, there are open questions about some other dependencies including Dogtag and Certmonger. It is possible that configuration or code changes will be required to make these programs work in environments</p>]]></summary>
</entry>
<entry>
    <title>User namespaces in OpenShift via CRI-O annotations</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2020-12-01-openshift-crio-userns.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2020-12-01-openshift-crio-userns.html</id>
    <published>2020-12-01T00:00:00Z</published>
    <updated>2020-12-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="user-namespaces-in-openshift-via-cri-o-annotations">User namespaces in OpenShift via CRI-O annotations</h1>
<p>In a recent post I covered the lack of user namespace support in OpenShift, and discussed the <a href="https://github.com/cri-o/cri-o/pull/3944">upcoming CRI-O feature</a> for user namespacing of containers, controlled by annotations.</p>
<p>I now have an OpenShift nightly cluster deployed. It uses a prerelease version of CRI-O v1.20, which includes this new feature. So it’s time to experiment! This post records my investigation of this feature.</p>
<h2 id="preliminaries">Preliminaries <a href="#preliminaries">§</a></h2>
<p>I’ll skip the details of deploying the nightly (4.7) cluster (because they are not important). What <em>is</em> important is that I created a <code>MachineConfig</code> to enable the CRI-O user namespace annotation feature, <a href="2020-11-30-openshift-machine-config-operator.html">as described in my previous post</a>.</p>
<p>As in the initial investigation, I created a new user account and project namespace for the experiments:</p>
<pre><code>% oc new-project test
Now using project &quot;test&quot; on server &quot;https://api.permanent.idmocp.lab.eng.rdu2.redhat.com:6443&quot;.

% oc create user test
user.user.openshift.io/test created

% oc adm policy add-role-to-user admin test
clusterrole.rbac.authorization.k8s.io/admin added: &quot;test&quot;</code></pre>
<h2 id="creating-a-user-namespaced-pod---attempt-1">Creating a user namespaced pod - Attempt 1 <a href="#creating-a-user-namespaced-pod---attempt-1">§</a></h2>
<p>I defined a pod that just runs <code>sleep</code>, but uses the new annotation to run it in a user namespace. The <code>map-to-root=true</code> directive says that the “beginning” of the host uid range assigned to the container should maps to uid 0 (i.e. <code>root</code>) in the container.</p>
<pre><code>$ cat userns-test.yaml
apiVersion: v1
kind: Pod
metadata:
  name: userns-test
  annotations:
    io.kubernetes.cri-o.userns-mode: &quot;auto:map-to-root=true&quot;
spec:
  containers:
  - name: userns-test
    image: freeipa/freeipa-server:fedora-31
    command: [&quot;sleep&quot;, &quot;3601&quot;]</code></pre>
<p>Create the pod:</p>
<pre><code>$ oc --as test create -f userns-test.yaml
pod/userns-test created</code></pre>
<p>After a few seconds, does everything look OK?</p>
<pre><code>$ oc get pod userns-test
NAME          READY   STATUS              RESTARTS   AGE
userns-test   0/1     ContainerCreating   0          14s</code></pre>
<p>Hm, 14 seconds seems a long time to be stuck at <code>ContainerCreating</code>. What does <code>oc describe</code> reveal?</p>
<pre><code>$ oc describe pod/userns-test
Name:         userns-test
Namespace:    test
Priority:     0
Node:         ft-47dev-2-27h8r-worker-0-j4jjn/10.8.1.106
Start Time:   Mon, 30 Nov 2020 12:41:34 +0000
Labels:       &lt;none&gt;
Annotations:  io.kubernetes.cri-o.userns-mode: auto:map-to-root=true
              openshift.io/scc: restricted
Status:       Pending

...

Events:
  Type     Reason                  Age                       From                                      Message
  ----     ------                  ----                      ----                                      -------
  Normal   Scheduled               &lt;unknown&gt;                                                           Successfully assigned test/userns-test to ft-47dev-2-27h8r-worker-0-j4jjn
  Warning  FailedCreatePodSandBox  &lt;invalid&gt; (x96 over 20m)  kubelet, ft-47dev-2-27h8r-worker-0-j4jjn  Failed to create pod sandbox: rpc error: code = Unknown desc = error creating pod sandbox with name &quot;k8s_userns-test_test_e4f69d50-e061-46ca-b933-000bcea3363a_0&quot;: could not find enough available IDs</code></pre>
<p>The node failed to create the pod sandbox. To spare you scrolling to read the unwrapped error message, I’ll reproduce it:</p>
<pre><code>Failed to create pod sandbox: rpc error: code = Unknown
desc = error creating pod sandbox with name
&quot;k8s_userns-test_test_e4f69d50-e061-46ca-b933-000bcea3363a_0&quot;:
could not find enough available IDs</code></pre>
<p>My initial reaction to this error is: <strong>this is good!</strong> It <em>seems</em> that CRI-O is attempting to create a user namespace for the container, but cannot. Another problem to solve, but we seem to be on the right track.</p>
<h2 id="etcsubuid"><code>/etc/subuid</code> <a href="#etcsubuid">§</a></h2>
<p>I had not yet done any host configuration related to user namespace mappings. But I had a feeling that the <code>/etc/subuid</code> and <code>/etc/subgid</code> files would come into play. According to <code>subuid(5)</code>:</p>
<blockquote>
<p>Each line in /etc/subuid contains a user name and a range of subordinate user ids that user is allowed to use.</p>
</blockquote>
<p>The description in <code>subgid(5)</code> is similar.</p>
<p>If the user that is attempting to create the containers doesn’t have an sufficient range of unused host uids and gids to use, it follows that it will not be able to create the user namespace for the pod.</p>
<p>I used a debug shell to observe the current contents of <code>/etc/subuid</code> and <code>/etc/subgid</code> on worker nodes:</p>
<pre><code>sh-4.4# cat /etc/subuid
core:100000:65536
sh-4.4# cat /etc/subgid
core:100000:65536</code></pre>
<p>The user <code>core</code> owns a uid and gid range of size 65536, starting at uid/gid 100000. There are no other ranges defined.</p>
<p>At this point, I have a strong feeling we need to define uid and gid ranges for the appropriate user, and then things will hopefully start working. The next question is: <em>who is the appropriate user</em>? That is, in OpenShift which user is responsible for creating the containers and, in this case, the user namespaces? Again on the worker node debug shell, I queried which user is running <code>crio</code>:</p>
<pre><code>sh-4.4# ps -o user,pid,cmd -p $(pgrep crio)
USER         PID CMD
root        1791 /usr/bin/crio --enable-metrics=true --metrics-port=9537</code></pre>
<p><code>crio</code> is running as the <code>root</code> user, which is not surprising. So we will need to add mappings for the <code>root</code> user to the mapping files.</p>
<h3 id="machineconfig-for-modifying-etcsubugid"><code>MachineConfig</code> for modifying <code>/etc/sub[ug]id</code> <a href="#machineconfig-for-modifying-etcsubugid">§</a></h3>
<p>I will create a <code>MachineConfig</code> to append the mappings <code>/etc/subuid</code> and <code>/etc/subgid</code>. First we need the base64 encoding of the line we want to add:</p>
<pre><code>$ echo &quot;root:200000:268435456&quot; | base64
cm9vdDoyMDAwMDA6MjY4NDM1NDU2Cg==</code></pre>
<p>The <code>MachineConfig</code> definition (note that it is scoped to the <code>worker</code> role):</p>
<pre><code>$ cat machineconfig-subuid-subgid.yaml 
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: subuid-subgid
spec:
  config:
    ignition:
      version: 3.1.0
    storage:
      files:
      - path: /etc/subuid
        append:
          - source: data:text/plain;charset=utf-8;base64,cm9vdDoyMDAwMDA6MjY4NDM1NDU2Cg==
      - path: /etc/subgid
        append:
          - source: data:text/plain;charset=utf-8;base64,cm9vdDoyMDAwMDA6MjY4NDM1NDU2Cg==</code></pre>
<p>Creating the <code>MachineConfig</code> object:</p>
<pre><code>$ oc create -f machineconfig-subuid-subgid.yaml
machineconfig.machineconfiguration.openshift.io/subuid-subgid created</code></pre>
<p>After a few moments, checking the <code>machineconfigpool/worker</code> object revealed that cluster is in a degraded state:</p>
<pre><code>$ oc get -o json mcp/worker |jq &#39;.status.conditions[-2:]&#39;
[
  {
    &quot;lastTransitionTime&quot;: &quot;2020-12-01T02:55:52Z&quot;,
    &quot;message&quot;: &quot;Node ft-47dev-2-27h8r-worker-0-f8bnl is reporting: \&quot;can&#39;t reconcile config rendered-worker-a37679c5cfcefb5b0af61bb3674dccc4 with rendered-worker-3cbd4cabeedd441500c83363dbf505fd: ignition file /etc/subuid includes append: unreconcilable\&quot;&quot;,
    &quot;reason&quot;: &quot;1 nodes are reporting degraded status on sync&quot;,
    &quot;status&quot;: &quot;True&quot;,
    &quot;type&quot;: &quot;NodeDegraded&quot;
  },
  {
    &quot;lastTransitionTime&quot;: &quot;2020-12-01T02:55:52Z&quot;,
    &quot;message&quot;: &quot;&quot;,
    &quot;reason&quot;: &quot;&quot;,
    &quot;status&quot;: &quot;True&quot;,
    &quot;type&quot;: &quot;Degraded&quot;
  }
]</code></pre>
<p>The error message is:</p>
<pre><code>Node ft-47dev-2-27h8r-worker-0-f8bnl is reporting: \&quot;can&#39;t
reconcile config rendered-worker-a37679c5cfcefb5b0af61bb3674dccc4
with rendered-worker-3cbd4cabeedd441500c83363dbf505fd: ignition
file /etc/subuid includes append: unreconcilable\&quot;&quot;,</code></pre>
<p>Upon further investigation, I learned that the Machine Config Operator does not support <code>append</code> operations. This is because are not idempotent. So I will try again with a new machine config that completely replaces the <code>/etc/subuid</code> and <code>/etc/subgid</code> files.</p>
<p>The new content shall be:</p>
<pre><code>core:100000:65536
root:200000:268435456</code></pre>
<p>The updated <code>MachineConfig</code> definition is:</p>
<pre><code>$ cat machineconfig-subuid-subgid.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: subuid-subgid
spec:
  config:
    ignition:
      version: 3.1.0
    storage:
      files:
      - path: /etc/subuid
        overwrite: true
        contents:
          source: data:text/plain;charset=utf-8;base64,Y29yZToxMDAwMDA6NjU1MzYKcm9vdDoyMDAwMDA6MjY4NDM1NDU2Cg==
      - path: /etc/subgid
        overwrite: true
        contents:
          source: data:text/plain;charset=utf-8;base64,Y29yZToxMDAwMDA6NjU1MzYKcm9vdDoyMDAwMDA6MjY4NDM1NDU2Cg==</code></pre>
<p>I replaced the <code>MachineConfig</code> object:</p>
<pre><code>$ oc replace -f machineconfig-subuid-subgid.yaml
machineconfig.machineconfiguration.openshift.io/subuid-subgid replaced</code></pre>
<p>After a few moments, the cluster is no longer degraded and the worker nodes will be updated over the next several minutes:</p>
<pre><code>$ oc get mcp/worker
NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
worker   rendered-worker-a37679c5cfcefb5b0af61bb3674dccc4   False     True       False      4              0                   0                     0                      3d20h</code></pre>
<p>After <code>READYMACHINECOUNT</code> reached <code>4</code> (all machines in the <code>worker</code> pool), I used a debug shell on one of the worker nodes to confirm that the changes had been applied:</p>
<pre><code>$ oc debug node/ft-47dev-2-27h8r-worker-0-j4jjn
Starting pod/ft-47dev-2-27h8r-worker-0-j4jjn-debug ...
To use host binaries, run `chroot /host`
Pod IP: 10.8.1.106
If you don&#39;t see a command prompt, try pressing enter.
sh-4.2# chroot /host
sh-4.4# cat /etc/subuid
core:100000:65536
root:200000:268435456
sh-4.4# cat /etc/subgid
core:100000:65536
root:200000:268435456</code></pre>
<p>Looks good!</p>
<h2 id="creating-a-user-namespaced-pod---attempt-2">Creating a user namespaced pod - Attempt 2 <a href="#creating-a-user-namespaced-pod---attempt-2">§</a></h2>
<p>It’s time to create the user namespaced pod again, and see if it succeeds this time.</p>
<pre><code>$ oc --as test create -f userns-test.yaml
pod/userns-test created</code></pre>
<p>Unfortunately, the same <code>FailedCreatePodSandBox</code> error occurred. My <code>subuid</code> remedy was either incorrect, or insufficient. I decided to use a debug shell on the worker node to examine the system journal. I searched for the error string <code>could not find enough available IDs</code>, and found the error in the output of the <code>hyperkube</code> unit. A few lines above that, there are some <code>crio</code> log messages, including:</p>
<pre><code>Cannot find mappings for user \&quot;containers\&quot;: No subuid
ranges found for user \&quot;containers\&quot; in /etc/subuid&quot;</code></pre>
<p>So, my mistake was defining ID map ranges for the <code>root</code> user. I should have used the <code>containers</code> user. I fixed the <code>MachineConfig</code> definition to use the file content:</p>
<pre><code>core:100000:65536
containers:200000:268435456</code></pre>
<p>Then I replaced the <code>subuid-subgid</code> object and again waited for Machine Config Operator to update the worker nodes.</p>
<h2 id="creating-a-user-namespaced-pod---attempt-3">Creating a user namespaced pod - Attempt 3 <a href="#creating-a-user-namespaced-pod---attempt-3">§</a></h2>
<p>Once again, the container remained at <code>ContainerCreating</code>. But the error was different (lines wrapped for readability):</p>
<pre><code>Failed to create pod sandbox: rpc error:
code = Unknown
desc = container create failed:
  time=&quot;2020-12-01T06:40:49Z&quot;
  level=warning
  msg=&quot;unable to terminate initProcess&quot;
  error=&quot;exit status 1&quot;

time=&quot;2020-12-01T06:40:49Z&quot;
level=error
msg=&quot;container_linux.go:366: starting container process caused:
  process_linux.go:472: container init caused:
    write sysctl key net.ipv4.ping_group_range:
      write /proc/sys/net/ipv4/ping_group_range: invalid argument&quot;</code></pre>
<p>After a bit of research, here is my understanding of the situation: CRI-O successfully created the pod sandbox (which includes the user namespace) and is now initialising it. One of the initialisation steps is to set the <code>net.ipv4.ping_group_range</code> sysctl (the subroutine is part of <code>runc</code>), and this is failing. This step is performed for all pods, but it is only failing when the pod is using a user namespace.</p>
<h2 id="net.ipv4.ping_group_range-and-user-namespaces"><code>net.ipv4.ping_group_range</code> and user namespaces <a href="#net.ipv4.ping_group_range-and-user-namespaces">§</a></h2>
<p>The <code>net.ipv4.ping_group_range</code> sysctl defines the range of group IDs that are allowed to send ICMP Echo packets. Setting it to the full gid range allows <code>ping</code> to be used in rootless containers, without setuid or the <code>CAP_NET_ADMIN</code> and <code>CAP_NET_RAW</code> capabilities.</p>
<p>The CRI-O config key <code>crio.runtime.default_sysctls</code> declares the default sysctls that will be set in all containers. The default OpenShift CRI-O configuration sets it to the full gid range:</p>
<pre><code>sh-4.4# cat /etc/crio/crio.conf.d/00-default \
    | grep -A2 default_sysctls
default_sysctls = [
    &quot;net.ipv4.ping_group_range=0 2147483647&quot;,
]</code></pre>
<p>My working hypothesis is that setting the sysctl in the user-namespaced container fails because the gid range in the sandbox is not <code>0–2147483647</code> but much smaller. This could explain the <code>invalid argument</code> part of the error message.</p>
<p>How to overcome this? I first thought to update the pod spec to specify a different value for the sysctl that reflects the actual gid range in the sandbox. And to do that, I have to calculate what that gid range is.</p>
<h3 id="computing-the-gid-range">Computing the gid range <a href="#computing-the-gid-range">§</a></h3>
<p>I will work on the assumption that I must refer to the range as it appears <em>in the namespace</em>. That assumption could be wrong, but that’s where I’m starting.</p>
<p>Because I am using <code>map-to-root=true</code>, the start value of the range should be <code>0</code>. The second number in the <code>ping_group_range</code> sysctl value is not the range size but the end gid (inclusive). CRI-O currently hard-codes a default user namespace size of <code>65536</code>.</p>
<p>Because the size of the uid range is a critical parameter, I shall from now on explicitly declare the desired size in the <code>userns-mode</code> annotation. This will protect the solution from change to the default range size. I probably won’t need 65536 uids/gids but I’ll stick with the default for now.</p>
<pre><code>io.kubernetes.cri-o.userns-mode: &quot;auto:size=65536;map-to-root=true&quot;</code></pre>
<p>With a range of <code>65536</code> starting at <code>0</code>, the desired sysctl setting is <code>net.ipv4.ping_group_range=0 65535</code>.</p>
<h3 id="configuring-the-sysctl">Configuring the sysctl <a href="#configuring-the-sysctl">§</a></h3>
<p>We need <code>ping</code> to continue working in containers that are not namespaced. Therefore, overriding or clearing the CRI-O <code>default_sysctls</code> config is not an option. Instead I need a way to optionally set the <code>net.ipv4.ping_group_range</code> sysctl to a specified value on a per-pod basis.</p>
<p>You can specify sysctls to be set in a pod via the <code>spec.securityContext.sysctls</code> array (see Kubernetes <a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#podsecuritycontext-v1-core">PodSecurityContext documentation</a>). I updated the pod definition to include the sysctl:</p>
<pre><code>$ cat userns-test.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: userns-test
  annotations:
    openshift.io/scc: restricted
    io.kubernetes.cri-o.userns-mode: &quot;auto:size=65536;map-to-root=true&quot;
spec:
  containers:
  - name: userns-test
    image: freeipa/freeipa-server:fedora-31
    command: [&quot;sleep&quot;, &quot;3601&quot;]
  securityContext:
    sysctls:
    - name: &quot;net.ipv4.ping_group_range&quot;
      value: &quot;0 65535&quot;</code></pre>
<p>As I write this, I don’t know yet how CRI-O behaves when both <code>default_sysctls</code> and the pod spec define the same sysctl. It might just set the value from the pod spec, which is the behaviour I need. Or it might first attempt to set the value from <code>default_sysctls</code>, and afterwards set it again to the value from the pod spec (this will fail as before).</p>
<p>Time to find out!</p>
<h2 id="creating-a-user-namespaced-pod---attempt-4">Creating a user namespaced pod - Attempt 4 <a href="#creating-a-user-namespaced-pod---attempt-4">§</a></h2>
<pre><code>$ oc --as test create -f userns-test.yaml
pod/userns-test created

# ... wait ...

$ oc get pod userns-test
NAME          READY   STATUS                 RESTARTS   AGE
userns-test   0/1     CreateContainerError   0          118s</code></pre>
<p>OK, progress was made! It did not get stuck at <code>ContainerCreating</code>; this time we got a <code>CreateContainerError</code>. This means that the CRI-O sysctl behaviour is what we were hoping for. As for the new error, <code>oc describe</code> gave the detail:</p>
<pre><code>Error: container create failed:
time=&quot;2020-12-01T12:38:45Z&quot;
level=error
msg=&quot;container_linux.go:366: starting container process caused:
  setup user: cannot set uid to unmapped user in user namespace&quot;</code></pre>
<p>My guess is that CRI-O is ignoring the fact that the pod is in a user namespace and is attempting to execute the process using the same uid as it would if the pod were not in a user namespace. The uid is outside the mapped range (<code>0</code>–<code>65535</code>). For my next attempt I will add <code>runAsUser</code> and <code>runAsGroup</code> to the <code>securityContext</code>.</p>
<p>But first some other quick notes and observations. First of all, a user namespace was indeed created for this pod!</p>
<pre><code>sh-4.4# lsns -t user
        NS TYPE  NPROCS    PID USER   COMMAND
4026531837 user     277      1 root   /usr/lib/systemd/systemd --switched-root --system --deserialize 16
4026532599 user       1 684279 200000 /usr/bin/pod</code></pre>
<p>We can examine the uid and gid maps for the namespace:</p>
<pre><code>sh-4.4# cat /proc/684279/uid_map
         0     200000      65536

sh-4.4# cat /proc/684279/gid_map
         1     200001      65535
         0 1000610000          1</code></pre>
<p>It surprised me that gid <code>0</code> is mapped to system user <code>1000610000</code>. I don’t know what consequences this might have; for now I am just noting it.</p>
<p>Because the pod sandbox does exist, I also decided to see if I could get a debug shell:</p>
<pre><code>$ oc debug pod/userns-test
Starting pod/userns-test-debug, command was: sleep 3601
Pod IP: 10.129.3.170
If you don&#39;t see a command prompt, try pressing enter.
sh-5.0$ id
uid=1000610000(1000610000) gid=0(root) groups=0(root),1000610000</code></pre>
<p>It worked! But the debug shell cannot be running in the user namespace; the uid (<code>1000610000</code>) is too high. Running <code>lsns</code> in my worker node debug shell confirms it; the namespace still has only one process running in it:</p>
<pre><code>sh-4.4# lsns -t user
        NS TYPE  NPROCS    PID USER   COMMAND
4026531837 user     282      1 root   /usr/lib/systemd/systemd --switched-root --system --deserialize 16
4026532599 user       1 684279 200000 /usr/bin/pod</code></pre>
<h2 id="creating-a-user-namespaced-pod---attempt-5">Creating a user namespaced pod - Attempt 5 <a href="#creating-a-user-namespaced-pod---attempt-5">§</a></h2>
<p>I once again deleted the <code>userns-test</code> pod. As proposed above, I modified the pod security context to specify that the entry point should be run as uid <code>0</code> and gid <code>0</code>:</p>
<pre><code>$ cat userns-test.yaml
apiVersion: v1
kind: Pod
metadata:
  name: userns-test
  annotations:
    openshift.io/scc: restricted
    io.kubernetes.cri-o.userns-mode: &quot;auto:size=65536;map-to-root=true&quot;
spec:
  containers:
  - name: userns-test
    image: freeipa/freeipa-server:fedora-31
    command: [&quot;sleep&quot;, &quot;3601&quot;]
  securityContext:
    runAsUser: 0
    runAsGroup: 0
    sysctls:
    - name: &quot;net.ipv4.ping_group_range&quot;
      value: &quot;0 65535&quot;</code></pre>
<p>Here we go:</p>
<pre><code>$ oc --as test create -f userns-test.yaml
Error from server (Forbidden): error when creating
&quot;userns-test.yaml&quot;: pods &quot;userns-test&quot; is forbidden: unable to
validate against any security context constraint:
[spec.containers[0].securityContext.runAsUser: Invalid value: 0:
must be in the ranges: [1000610000, 1000619999]]</code></pre>
<p><em>sad trombone</em></p>
<p>I don’t have a clear idea how I could proceed. The security context constraint (SCC) is prohibiting the use of uid <code>0</code> for the container process. Switching to a permissive SCC might allow me to proceed, but it would also mean using a more privileged OpenShift user account. Then that privileged account could then create containers running as <code>root</code> <em>in the system user namespace</em>. We want user namespaces in OpenShift so that we can <em>avoid</em> this exact scenario. So resorting to a permissive SCC (e.g. <code>anyuid</code>) feels like the wrong way to go.</p>
<p>It could be that it’s the only way to go for now, and that more nuanced security policy mechanisms must be implemented before user namespaces can be used in OpenShift to achieve the security objective. In any case, I’ll be reaching out to other engineers and OpenShift experts for their suggestions.</p>
<p>For now, I’m calling it a day! See you soon for the next episode.</p>]]></summary>
</entry>
<entry>
    <title>Using the OpenShift Machine Config Operator</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2020-11-30-openshift-machine-config-operator.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2020-11-30-openshift-machine-config-operator.html</id>
    <published>2020-11-30T00:00:00Z</published>
    <updated>2020-11-30T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="using-the-openshift-machine-config-operator">Using the OpenShift Machine Config Operator</h1>
<p>In a <a href="2020-11-05-openshift-user-namespace.html">recent post</a> I discussed how OpenShift and Kubernetes do not have user namespace isolation. An <a href="https://github.com/cri-o/cri-o/pull/3944">upcoming CRI-O enhancement</a> should allow pods to be run in separate user namespaces. This feature is controlled via <em>annotations</em>; no explicit Kubernetes support is required.</p>
<p>To experiment with this feature I deployed an OpenShift nightly (4.7) cluster, which uses a CRI-O v1.20 prerelease build. But having CRI-O v1.20 is not enough. The feature must be explicitly enabled in the CRI-O configuration. This leads to the question, <em>what is the proper way to manage machine configuration in an OpenShift cluster?</em> The answer is the <em>Machine Config Operator (MCO)</em>.</p>
<p>The <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.6/html/post-installation_configuration/post-install-machine-configuration-tasks">official OpenShift documentation</a> does a good job of introducing and explaining the MCO, so there’s no need to regurgitate it all here. Instead I’ll review the configuration, object definitions and procedure from my CRI-O use case.</p>
<h2 id="configuring-cri-o-via-the-machine-config-operator">Configuring CRI-O via the Machine Config Operator <a href="#configuring-cri-o-via-the-machine-config-operator">§</a></h2>
<p>CRI-O is configured via <code>/etc/crio/crio.conf</code> and additional files in the <code>/etc/crio/crio.conf.d/</code> directory. Directives from <code>crio.conf.d</code> files have higher precedence and files are processed in lexicographic order.</p>
<p>The follow configuration enables the user namespaces feature:</p>
<pre><code>[crio.runtime.runtimes.runc]
allowed_annotations=[&quot;io.kubernetes.cri-o.userns-mode&quot;]</code></pre>
<p>I used MCO to drop that configuration snippet into the file <code>/etc/crio/crio.conf.d/99-crio-userns.conf</code>. First I needed the base64 encoding of the configuration content:</p>
<pre><code>$ base64 --wrap=0 &lt;&lt;EOF
[crio.runtime.runtimes.runc]
allowed_annotations=[&quot;io.kubernetes.cri-o.userns-mode&quot;]
EOF
W2NyaW8ucnVudGltZS5ydW50aW1lcy5ydW5jXQphbGxvd2VkX2Fubm90YXRpb25zPVsiaW8ua3ViZXJuZXRlcy5jcmktby51c2VybnMtbW9kZSJdCg==</code></pre>
<p>Next I created <code>machineconfig-crio-userns.yaml</code>. This defines a <code>MachineConfig</code>, the primary resource type handled by the MCO. The base64 output from above is used in this file.</p>
<pre><code>apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: crio-userns
spec:
  config:
    ignition:
      version: 3.1.0
    storage:
      files:
      - path: /etc/crio/crio.conf.d/99-crio-userns.conf
        overwrite: true
        contents:
          source: data:text/plain;charset=utf-8;base64,W2NyaW8ucnVudGltZS5ydW50aW1lcy5ydW5jXQphbGxvd2VkX2Fubm90YXRpb25zPVsiaW8ua3ViZXJuZXRlcy5jcmktby51c2VybnMtbW9kZSJdCg==</code></pre>
<p>Note that the examples in the official documentation contain a lot of extraneous fields that can be omitted. <code>MachineConfig</code> objects use the <em>Ignition</em> configuration format. Read the <a href="https://github.com/coreos/ignition/blob/master/docs/configuration-v3_1.md">Ignition Configuration Specification</a> to see what fields are available or required (or not) for your use case.</p>
<p>There are just a few things about this <code>MachineConfig</code> that I’d like to highlight.</p>
<ul>
<li>For creating files, the <code>mode</code> field allows specifying the file access permissions. The default is <code>420</code> (<em>decimal!</em>, equivalent to <code>0644</code>); this was suitable for my use case so I omitted it. But there may be many cases where the default is not suitable and it will be necessary to specify the <code>mode</code>.</li>
<li>This config only needs to be applied on worker nodes. The <code>machineconfiguration.openshift.io/role: worker</code> label accomplishes this. The value <code>master</code> can be used for master-only configurations.</li>
<li>The file content is specified via a <a href="https://tools.ietf.org/html/rfc2397">"data" URI</a>. Other supported schemes include <code>https</code>, <code>s3</code> and <code>tftp</code>.</li>
</ul>
<p>Next I created the <code>MachineConfig</code> object:</p>
<pre><code>$ oc create -f machineconfig-crio-userns.yaml
machineconfig.machineconfiguration.openshift.io/crio-userns created</code></pre>
<p>Over the next several minutes, the Machine Config Operator applied the configuration change to all the worker nodes and restarted them.</p>
<h2 id="closing-thoughts">Closing thoughts <a href="#closing-thoughts">§</a></h2>
<p>Everything went smoothly and my impressions of MCO, from this first “hands on” experience, are very positive. It was a simple use case, I admit. But I am still very pleased that it was so easy and everything Just Worked. Hopefully other people have as good an experience with MCO as I did, even for more complex configuration changes.</p>]]></summary>
</entry>
<entry>
    <title>ACME Service Discovery</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2020-11-13-acme-service-discovery.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2020-11-13-acme-service-discovery.html</id>
    <published>2020-11-13T00:00:00Z</published>
    <updated>2020-11-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="acme-service-discovery">ACME Service Discovery</h1>
<p>Automated Certificate Management Environment (ACME) is a protocol for automated identifer validation certificate issuance. Over the past five years it gained widespread adoption thanks to <a href="https://letsencrypt.org/">Let's Encrypt</a>, the first publicly trusted CA that implemented it. ACME is supported by a plethora of server programs and service providers, Let’s Encrypt has now issued over <a href="https://letsencrypt.org/2020/02/27/one-billion-certs.html">1 billion certificates</a> and together with the ACME protocol itself is largely responsible for pushing the adoption of TLS from around 50% of page loads five years ago to well over 80% today. This is an amazing result!</p>
<p>So it’s no surprise that the ACME ecosystem is growing. Some other publicly trusted CAs now support the ACME protocol. Enterprise CAs are learning how to speak ACME. This includes <a href="https://www.dogtagpki.org/wiki/ACME">Dogtag</a>, and by extension FreeIPA. The upcoming FreeIPA 4.9 release will support ACME (I <a href="2020-05-06-ipa-acme-intro.html">blogged about this</a> a few months ago).</p>
<p>Having proved itself good for DNS certificates, <a href="https://tools.ietf.org/html/rfc8738">RFC 8738</a> introduced supported for IP addresses. Work to support email addresses (for S/MIME), <code>.onion</code> addresses (Tor services), and other identifer types is underway in the IETF <a href="https://datatracker.ietf.org/wg/acme/documents/">acme Working Group</a>. (ACME itself is defined in <a href="https://tools.ietf.org/html/rfc8555">RFC 8555</a>).</p>
<p>The outcome of all of this is that already today, and increasingly into the future, network environments will often have access to multiple ACME servers. These servers may differ in the kinds of certificates they issue and the validation methods (also called “challenge types”) they support. Also, it is desirable that a client (e.g. a printer or an IoT “thing”) would be able to opportunistically and automatically locate a suitable ACME server to acquire certificates without any operator (human or otherwise) intervention (and Let’s Encrypt or other public ACME servers may not be accessible in some environments).</p>
<p>So, what’s an ACME client to do?</p>
<h2 id="internet-draft">Internet-Draft <a href="#internet-draft">§</a></h2>
<p>I have <a href="https://datatracker.ietf.org/doc/draft-tweedale-acme-discovery/">published an Internet-Draft</a> defining a service discovery protocol for ACME. <em>Internet-Draft</em> is <a href="https://www.ietf.org/">IETF</a> jargon for a work-in-progress document that might one day become an <a href="https://www.ietf.org/standards/rfcs/">RFC</a>. An outline of how ACME Service Discovery works follows.</p>
<p>ACME Service Discovery is a profile of <em>DNS-based Service Discovery (DNS-SD)</em> (<a href="https://tools.ietf.org/html/rfc6763">RFC 6763</a>). Given a <em>parent domain</em>, <em>Service Instance Names</em> are listed by the PTR records of <code>_acme-server._tcp.$PARENT</code>. For example, the <code>corp.example.</code> parent domain advertises two service instances called <code>CorpCA</code> and <code>C4A</code>:</p>
<pre><code>$ORIGIN corp.example.

_acme-server._tcp PTR CorpCA._acme-server._tcp
_acme-server._tcp PTR C4A._acme-server._tcp</code></pre>
<p>Each Service Instance Name owns an SRV and TXT record that together describe the location, priority and capabilities of the server, as well as the path to the ACME directory object. Continuing with the example, <code>CorpCA</code> has the higher priority and supports the <code>ip</code> and <code>dns</code> identifer types, whereas <code>C4A</code> has a lower priority and only supports <code>dns</code> identifiers:</p>
<pre><code>$ORIGIN corp.example.

CorpCA._acme-server._tcp SRV 10 0 443 ca
CorpCA._acme-server._tcp TXT &quot;path=/acme&quot; &quot;i=ip,dns&quot;

C4A._acme-server._tcp    SRV 20 0 443 certs4all.example.
C4A._acme-server._tcp    TXT &quot;path=/acme/v2&quot; &quot;i=dns&quot;</code></pre>
<p>ACME clients are assumed to know (or deduce) one or more candidate parent domains. Possible sources for the candidate parent domain(s) are the DNS search domains, host FQDN or Kerberos realm. The client performs ACME Service Discovery on each parent domain, selecting and probing eligible service instances, until they find one that works. The probe step involves constructing a URL from the SRV target and port and TXT <code>path</code> attribute, performing an HTTP GET request for that resource, and checking that the response is a valid ACME directory object. In the example above, the directory URL for <code>CorpCA</code> is <code>https://ca.corp.example/acme</code>.</p>
<p>And that’s the main idea! There’s a fair bit more detail in the Internet-Draft but I won’t belabour it all here.</p>
<h2 id="enabling-acme-service-discovery-in-freeipa">Enabling ACME Service Discovery in FreeIPA <a href="#enabling-acme-service-discovery-in-freeipa">§</a></h2>
<p>To enable ACME Service Discovery in a FreeIPA environment using the integrated DNS service, add the PTR, SRV and TXT records for each service instance. This requires a <a href="https://github.com/freeipa/freeipa/pull/5239">recently merged patch</a> to allow PTR records to be created in arbitrary zones (PTR records were previously limited to <code>.arpa</code> reverse zones). The fix should be included in FreeIPA 4.9 and will also be backported to the 4.8.x branch.</p>
<p>The following DNS records advertise the FreeIPA CA itself:</p>
<pre><code>% ipa dnsrecord-add ipa.local ipa._acme-server._tcp \
    --srv-priority 10 --srv-weight 0 \
    --srv-port 443 --srv-target ipa-ca \
    --txt-rec &#39;&quot;path=/acme/directory&quot; &quot;i=dns&quot;&#39;
  Record name: ipa._acme-server._tcp
  SRV record: 10 0 443 ipa-ca
  TXT record: &quot;path=/acme/directory&quot; &quot;i=dns&quot;

% ipa dnsrecord-add ipa.local _acme-server._tcp \
    --ptr-rec &quot;ipa._acme-server._tcp.ipa.local.&quot;
  Record name: _acme-server._tcp
  PTR record: ipa._acme-server._tcp.ipa.local.</code></pre>
<p>The procedure to advertise additional ACME servers is similar.</p>
<p>If the ACME Service Discovery proposal gets traction we would ideally create these records to advertise the FreeIPA CA automatically (when it is enabled).</p>
<h2 id="certbot-plugin">Certbot plugin <a href="#certbot-plugin">§</a></h2>
<p>I wrote a <a href="https://certbot.eff.org/">Certbot</a> plugin to experiment with service discovery. It lives in a private branch at <a href="https://github.com/frasertweedale/certbot/tree/feature/discovery">https://github.com/frasertweedale/certbot/tree/feature/discovery</a>. I will probably submit a pull request soon, to invite feedback about the implementation and the service disovery proposal itself.</p>
<p>To install Certbot and the plugin under <code>~/.local/</code> (command output omitted):</p>
<pre><code># git clone https://github.com/certbot/certbot -b feature/discovery
# cd certbot/certbot
# pip install --user .
# cd ../certbot-discovery
# pip install --user .</code></pre>
<p>Run <code>certbot plugins</code> to verify that the plugin is installed:</p>
<pre><code># certbot plugins
Saving debug log to /var/log/letsencrypt/letsencrypt.log

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
* discovery
Description: ACME Service Discovery
Interfaces: IPlugin
Entry point: discovery = certbot_discovery:ACMEServiceDiscovery

* standalone
Description: Spin up a temporary webserver
Interfaces: IAuthenticator, IPlugin
Entry point: standalone = certbot._internal.plugins.standalone:Authenticator

* webroot
Description: Place files in webroot directory
Interfaces: IAuthenticator, IPlugin
Entry point: webroot = certbot._internal.plugins.webroot:Authenticator
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -</code></pre>
<p>Now register an account with the ACME server. Note the <code>--discovery</code> option:</p>
<pre><code># certbot --discovery register \
  --email ftweedal@redhat.com \
  --agree-tos --no-eff-email
Saving debug log to /var/log/letsencrypt/letsencrypt.log
Account registered.</code></pre>
<p>If service discovery fails, it will fail silently and use Let’s Encrypt (Certbot’s default). <code>--discovery=force</code> suppresses this fallback behaviour; if service discovery fails Certbot will abort.</p>
<p>Next request the certificate:</p>
<pre><code># certbot --discovery certonly \
    --domain $(hostname) --standalone
Saving debug log to /var/log/letsencrypt/letsencrypt.log
Plugins selected: Authenticator standalone, Installer None
Obtaining a new certificate
Performing the following challenges:
http-01 challenge for f33-0.ipa.local
Waiting for verification...
Cleaning up challenges

IMPORTANT NOTES:
 - Congratulations! Your certificate and chain have been saved at:
   /etc/letsencrypt/live/f33-0.ipa.local/fullchain.pem
   Your key file has been saved at:
   /etc/letsencrypt/live/f33-0.ipa.local/privkey.pem
   Your cert will expire on 2021-02-10. To obtain a new or tweaked
   version of this certificate in the future, simply run certbot
   again. To non-interactively renew *all* of your certificates, run
   &quot;certbot renew&quot;
 - If you like Certbot, please consider supporting our work by:

   Donating to ISRG / Let&#39;s Encrypt:   https://letsencrypt.org/donate
   Donating to EFF:                    https://eff.org/donate-le</code></pre>
<p>We can check that the certificate was issued by the FreeIPA CA, not Let’s Encrypt:</p>
<pre><code># openssl x509 -issuer -noout  \
    &lt; /etc/letsencrypt/live/f33-0.ipa.local/fullchain.pem
issuer=O = IPA.LOCAL 202011061623, CN = Certificate Authority</code></pre>
<p>You do have to supply the <code>--discovery</code> option to both the <code>register</code> and <code>certonly</code> commands (otherwise <code>certonly</code> will try to use Let’s Encrypt). Fortunately, for <em>renewal</em> (the <code>renew</code> command) Certbot does remember which server issued the certificate, and uses the same server for renewal.</p>
<p>What happens when service discovery fails? I’ll disable the ACME service on the FreeIPA server:</p>
<pre><code>% sudo ipa-acme-manage disable
The ipa-acme-manage command was successful</code></pre>
<p>Then, running <code>certbot register</code> again, this time with <code>--discovery=force</code> to prevent fallback to Let’s Encrypt:</p>
<pre><code># certbot --discovery=force register \
  --email ftweedal@redhat.com \
  --agree-tos --no-eff-email
usage:
  certbot [SUBCOMMAND] [options] [-d DOMAIN] [-d DOMAIN] ...

Certbot can obtain and install HTTPS/TLS/SSL certificates.  By default,
it will attempt to use a webserver both for obtaining and installing the
certificate.
certbot: error: service discovery failed (see /tmp/tmp6qq8pnks for info)</code></pre>
<p>The log file contains a transcript of the service discovery plugin’s activity:</p>
<pre><code># cat /tmp/tmp6qq8pnks
[INFO] processing parent domain ipa.local.
[INFO] enumerating service instances for _acme-server._tcp.ipa.local.
[INFO]   found service instances: [&lt;DNS name ipa._acme-server._tcp.ipa.local.&gt;]
[INFO] resolving service instance ipa._acme-server._tcp.ipa.local.
[INFO]   (&lt;DNS IN SRV rdata: 10 0 443 ipa-ca.ipa.local.&gt;, (b&#39;path=/acme/directory&#39;, b&#39;i=dns&#39;))
[INFO] eligible service instances:
[INFO]   (&lt;DNS IN SRV rdata: 10 0 443 ipa-ca.ipa.local.&gt;, (b&#39;path=/acme/directory&#39;, b&#39;i=dns&#39;))
[INFO] GET https://ipa-ca.ipa.local/acme/directory
[WARNING] failed to reach server: &lt;Response [503]&gt;</code></pre>
<p>We can see that the plugin found the service instance and requested the directory resource, but got a 503 response (as expected). So, when service discovery fails the plugin gives you some useful log output to debug the issue.</p>
<p>The log file is only persisted when service discovery fails, otherwise it is deleted. In the current implementation we cannot write to the “normal” Certbot log file because we don’t know where that is. The discovery plugin is actually doing all its work <em>inside the argument parsing</em>. It feels like a brutal hack but it’s the only way I found (in the limited time I had) to override the <code>--server</code> option whilst keeping the implementation as a plugin, fully separate from Certbot core. A nicer implementation is possible if service discovery were to be implemented in Certbot core (this would introduce a dependency on <em>dnspython</em>).</p>
<h2 id="next-steps">Next steps <a href="#next-steps">§</a></h2>
<p>I will present and demo this proposal during the <code>acme</code> Working Group meeting at IETF 109 (November 2020). From there I hope that it will be adopted, developed, and shepherded through to become an RFC. I will also seek feedback from Certbot developers about the proposal and my experimental implementation.</p>
<p>I also intend to submit another Internet-Draft proposing a mechanism for servers to advertise their capabilities in the ACME directory object. This could be useful to help clients choose from multiple servers (regardless of how they find out about the servers). And I think it’s good practice. When a protocol has many possible features that a server may or may not implement, servers should declare their capabilities for the benefit of clients.</p>
<p>Beyond that, I am starting to think about SRVName support in ACME. This would be useful in enterprise environments and on the open internet for protocols where SRV records are used to locate servers. Such protocols include Kerberos, LDAP, SIP and XMPP.</p>]]></summary>
</entry>
<entry>
    <title>OpenShift and user namespaces</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2020-11-05-openshift-user-namespace.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2020-11-05-openshift-user-namespace.html</id>
    <published>2020-11-05T00:00:00Z</published>
    <updated>2020-11-05T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="openshift-and-user-namespaces">OpenShift and user namespaces</h1>
<p>FreeIPA in its current form is very much not a “cloud native” application. Likewise the current FreeIPA container, which runs all the required services under systemd. My current team is working on operationalising FreeIPA for the OpenShift container platform. Our initial efforts are focused around this “monolithic” container, trying to get it to run in OpenShift, securely. Although we recognise we may eventually need to split up the container, it will be a major engineering effort to do so. We want to have a working proof of concept as early as possible, so that we (and others) can start the important integration work (e.g. with Keycloak / RHSSO).</p>
<p>This “lift and shift” of a complex traditional application to OpenShift results in a container that needs to run several processes as a variety of users, including <code>root</code>. OpenShift isolates containers (actually pods, which consist of one or more containers) in their own PID namespace. This is good, but if we are to run container processes as <code>root</code> (in the container), we do not want them to also be <code>root</code> on the host. Rather, they should map to an unprivileged account. If we want secure multitenancy of multiple IDM servers on a single worker node, we want the user accounts in different IDM pods to map to disjoint sets of unprivileged users on the host.</p>
<p>Linux <code>user_namespaces(7)</code> provide this kind of isolation. To what extent are user namespaces supported in OpenShift? We needed to find out, in order to decide how to proceed with the FreeIPA OpenShift effort. In this blog post I discuss my investigation and findings.</p>
<h2 id="investigating-current-openshift-behaviour">Investigating current OpenShift behaviour <a href="#investigating-current-openshift-behaviour">§</a></h2>
<p>To investigate the use (or not) of user namespaces I deployed pods on our team’s OpenShift cluster, running a simple command, and observed the effects on the worker node.</p>
<p>As cluster admin, I created a new project:</p>
<pre><code>% oc new-project test
Now using project &quot;test&quot; on server &quot;https://api.permanent.idmocp.lab.eng.rdu2.redhat.com:6443&quot;.
...</code></pre>
<p>To avoid the cluster admin user’s SCC bindings applying to pod creation, I created a user named <code>test</code> and granted it the <em>project</em> (not cluster) <code>admin</code> role. Subsequent pod creation operations were performed as user <code>test</code>.</p>
<pre><code>% oc create user test
user.user.openshift.io/test created

% oc adm policy add-role-to-user admin test
clusterrole.rbac.authorization.k8s.io/admin added: &quot;test&quot;</code></pre>
<p>Next I deployed a basic pod (as user <code>test</code>) and inspected it to find out which worker node it was scheduled on, and the CRI-O conatiner ID:</p>
<pre><code>% cat pod-test.yaml
apiVersion: v1
kind: Pod
metadata:
  name: test
spec:
  containers:
  - name: idm-test
    image: freeipa/freeipa-server:fedora-31
    command: [&quot;sleep&quot;, &quot;3600&quot;]

% oc --as test create -f pod-test.yaml
pod/test created

% oc get -o json pod test \
    | jq .spec.nodeName
&quot;permanent-bdd7p-worker-9r4b6&quot;

% oc get -o json pod test \
    | jq &quot;.status.containerStatuses[0].containerID&quot;
&quot;cri-o://a9c0cf0ac9c0c352b82a74cccf830dfa8c33aae28138808eb7bdd9d53aae2d1f&quot;</code></pre>
<p>Next, opening a debug shell on the worker node I inspected the container to find out the PID:</p>
<pre><code>% oc debug node/permanent-bdd7p-worker-9r4b6
Starting pod/permanent-bdd7p-worker-9r4b6-debug ...
To use host binaries, run `chroot /host`
Pod IP: 10.8.3.215
If you don&#39;t see a command prompt, try pressing enter.
sh-4.2# chroot /host
sh-4.4# crictl inspect a9c0cf0ac | jq .pid
1311115</code></pre>
<p>Next I looked at which user the process is running under, and the UID map of the process:</p>
<pre><code>sh-4.4# ls -l -d /proc/1311115
dr-xr-xr-x. 9 1000620000 root 0 Nov  5 05:34 /proc/1311115

sh-4.4# cat /proc/1311115/uid_map
         0          0 4294967295</code></pre>
<p>The process was running as user <code>1000620000</code>, and UID map has an offset of <code>0</code> and a size of <code>2^32</code>. Which is to say, this process is running in the same user namespace as the host. We can use the <code>lsns</code> command to confirm that everything on this node–including all container processes–shares the single user namespace:</p>
<pre><code>sh-4.4# lsns -t user
        NS TYPE  NPROCS PID USER COMMAND
4026531837 user     296   1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 18</code></pre>
<p>As a result, if we use <code>runAsUser</code> to specify a different user under which to run the container, the container will run as the specified user both in the container <strong>and on the host</strong>. The following transcript demonstrates this.</p>
<p>Delete the pod <code>test</code>:</p>
<pre><code>% oc delete pod test
pod &quot;test&quot; deleted</code></pre>
<p>Add the <code>anyuid</code> SCC to user <code>test</code>:</p>
<pre><code>% oc adm policy add-scc-to-user anyuid test
securitycontextconstraints.security.openshift.io/anyuid added to: [&quot;test&quot;]</code></pre>
<p>Create the pod (as user <code>test</code>):</p>
<pre><code>% oc --as test create -f pod-test.yaml
pod/test created</code></pre>
<p>Following the same procedure as earlier, find the PID (it was <code>1381728</code>) and observe that it is running as <code>root</code> (UID <code>0</code>) on the host:</p>
<pre><code>sh-4.4# ls -l -d /proc/1381728
dr-xr-xr-x. 9 root root 0 Nov  5 05:55 /proc/1381728</code></pre>
<h2 id="consequences-for-freeipa">Consequences for FreeIPA <a href="#consequences-for-freeipa">§</a></h2>
<p>Traditional applications sometimes assume they will run as <code>root</code> or some other “reserved” user. FreeIPA is such a case. Likewise, running systemd in a container means running as UID 0 (from the container’s point of view).</p>
<p>The lack of user namespace use in OpenShift means that for a process to run under a particular UID in the container, it must run as that user on the host too. If you application needs to be <code>root</code>, it will be <code>root</code> on the host. Other kinds of namespaces (e.g. <code>pid</code>, <code>mnt</code>, <code>uts</code> among others) do mitigate the security risk. But if a rogue process can escalate privileges and escape the other sandbox(es) the result could be catastrophic.</p>
<p>FreeIPA, being composed of many components, some of which are large complex projects in their own right, and several of which are implemented in C or leverage C libraries, has a large attack surface. In the absense of user namespaces the risk of container host or co-tenant compromise—even by accident—seems high.</p>
<p>This all assumes that containers do not have user namespace isolation and that FreeIPA continues to require running processes in the FreeIPA container as fixed UIDs (probably including <code>root</code>). I will now discuss possible ways to eliminate these assumptions.</p>
<h2 id="user-namespace-support-in-kubernetes">User namespace support in Kubernetes <a href="#user-namespace-support-in-kubernetes">§</a></h2>
<p>OpenShift is built on the Kubernetes container platform. <em>Kubernetes Enhancement Proposal</em> <a href="https://github.com/kubernetes/enhancements/issues/127">KEP-127</a> proposes user namespace support. The ticket has been open for 4 years and has since seen several efforts to formalise the proposal, the most recent of which is <a href="https://github.com/kubernetes/enhancements/pull/2101">kubernetes/enhancements#2101</a> (<a href="https://github.com/kubernetes/enhancements/blob/9726c1a4cc5051d8be7eaf4cb64313df60ae8751/keps/sig-node/127-usernamespaces-support/README.md">rendered</a>). There have also been several experimental implementations (e.g. <a href="https://github.com/kubernetes/kubernetes/pull/55707">#55707</a>, <a href="https://github.com/kubernetes/kubernetes/pull/64005">#64005</a>), none of which was accepted (yet).</p>
<p>There has been a recent resurgence of activity on this KEP, and related discussions and pull requests. But that has happened before. I believe that every new (or resurrected) discussion or experiment can move you closer to the goal, and that there can be several false starts before things happen. Maybe this time it will happen… or maybe not.</p>
<p>Right now there is no final proposal and no implementation plan. As a team we cannot proceed on the assumption that Kubernetes will support user namespaces. We will certainly present our case to OpenShift engineering internally at Red Hat, but we have to look at other options.</p>
<h2 id="user-namespace-support-in-cri-o">User namespace support in CRI-O <a href="#user-namespace-support-in-cri-o">§</a></h2>
<p>The <a href="https://cri-o.io/">CRI-O</a> container runtime <a href="https://github.com/cri-o/cri-o/pull/3944">recently implemented</a> support for running each pod in a separate user namespace, via <em>annotations</em> on the pod, e.g.:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb11-1"><a href="#cb11-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb11-2"><a href="#cb11-2"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb11-3"><a href="#cb11-3"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb11-4"><a href="#cb11-4"></a><span class="at">  </span><span class="fu">annotations</span><span class="kw">:</span></span>
<span id="cb11-5"><a href="#cb11-5"></a><span class="at">    </span><span class="fu">io.kubernetes.cri-o.userns-mode</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;auto&quot;</span></span>
<span id="cb11-6"><a href="#cb11-6"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb11-7"><a href="#cb11-7"></a><span class="at">  ...</span></span></code></pre></div>
<p>Using annotations means that no explicit support in Kubernetes is required. All that is required is that Kubernetes is using the CRI-O container runtime, and that CRI-O is configured to enable this feature. OpenShift 4.x does use CRI-O, so we’re halfway there. The remaining step is to enable the feature in <code>crio.conf</code>:</p>
<pre><code>allow_userns_annotation = true</code></pre>
<p>The developer Giuseppe Scrivano kindly published a <a href="https://asciinema.org/a/351396">screencast showing the feature in action</a> (2 minutes). This feature is not yet in a supported release but is available on the v1.20 branch and is included in OpenShift <a href="">nightly builds</a>.</p>
<h2 id="splitting-the-freeipa-container">Splitting the FreeIPA container <a href="#splitting-the-freeipa-container">§</a></h2>
<p>If Kubernetes or CRI-O user namespace support to does not solve our problem (in our desired timeframe) then there is more pressure to abandon the monolithic container and devote our efforts to a “split-service” FreeIPA/IDM application. In this scenario, the various services that make up FreeIPA (LDAP, KDC, HTTP, CA and others) would each run as an unprivileged process in its own container.</p>
<p>This would be a big engineering effort. Apart from FreeIPA as a whole, most of the constituent services are also “traditional” applications that make assumptions about their environment and execution context—assumptions that do not hold in the OpenShift container paradigm.</p>
<p>There is a general (albeit unevenly distributed) feeling in the team that in the long run this effort is inevitable. I do hold this view myself, but also recognise that the sooner we can have a working proof of concept, the better. That is the main reason we are initially pursuing the monolithic container approach.</p>
<h2 id="next-steps">Next steps <a href="#next-steps">§</a></h2>
<p>My next step will be to install an OpenShift cluster based on the nightly builds (which include CRI-O v1.20) and experiment with the annotation-based user namespace support. It seems to be what we want, or a big step in the right direction, but we need to confirm it. Expect a follow-up to this article with my findings, hopefully in the next week!</p>]]></summary>
</entry>
<entry>
    <title>Issuing certificates for long hostnames</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2020-10-20-ipa-cert-long-hostname.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2020-10-20-ipa-cert-long-hostname.html</id>
    <published>2020-10-20T00:00:00Z</published>
    <updated>2020-10-20T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="issuing-certificates-for-long-hostnames">Issuing certificates for long hostnames</h1>
<p>X.509, specified in <a href="https://tools.ietf.org/html/rfc5280">RFC 5280</a>, restricts the length of the <em>Common Name (CN)</em> attribute to 64 characters:</p>
<pre><code>X520CommonName ::= DirectoryName (SIZE (1..ub-common-name))
ub-common-name-length INTEGER ::= 64</code></pre>
<p>Although the use of the CN attribute to carry DNS names is deprecated, it is still common practice. Furthermore, FreeIPA still requires the CN to appear in a <em>Certificate Signing Request (CSR)</em> and validates that its value corresponds to the nominated <em>subject principal</em>.</p>
<p>As a consequence of this restriction, when a host or service has a DNS name longer than 64 characters, that name cannot be used as the CN. But it can still be included in the <em>Subject Alternative Name</em> extension, as a <em>dNSName</em> value.</p>
<p>How do we issue such a certificate in FreeIPA? The trick is to add a <em>principal alias</em> whose hostname is 64 characters or shorter. This shorter hostname will be the Common Name attribute value. The full hostname will appear in the Subject Alternative Name extension.</p>
<p>The following sections demonstrate the method. I conclude with an outline of what needs to be done to support certificates with empty Subject DN, which would avoid the problem and this workaround.</p>
<h2 id="creating-a-principal-with-a-long-hostname">Creating a principal with a long hostname <a href="#creating-a-principal-with-a-long-hostname">§</a></h2>
<p>To experiment and verify the workaround, I needed a principal with a hostname longer than 64 characters. The initial attempt failed:</p>
<pre><code>% ipa host-add --force \
    verylongverylongverylongverylongverylongverylonghostname.ipa.local
ipa: ERROR: invalid &#39;hostname&#39;: can be at most 64 characters</code></pre>
<p>FreeIPA has a default maximum hostname length of 64 characters, but this is configurable. After adjusting the limit, adding the host succeeded:</p>
<pre><code>% ipa config-mod --maxhostname 255
  Maximum username length: 32
  Maximum hostname length: 255
  ...

% ipa host-add --force \
    verylongverylongverylongverylongverylongverylonghostname.ipa.local
-------------------------------------------------------------------------------
Added host &quot;verylongverylongverylongverylongverylongverylonghostname.ipa.local&quot;
-------------------------------------------------------------------------------
  Host name: verylongverylongverylongverylongverylongverylonghostname.ipa.local
  Principal name: host/verylongverylongverylongverylongverylongverylonghostname.ipa.local@IPA.LOCAL
  Principal alias: host/verylongverylongverylongverylongverylongverylonghostname.ipa.local@IPA.LOCAL
  ...</code></pre>
<h2 id="adding-the-principal-alias">Adding the principal alias <a href="#adding-the-principal-alias">§</a></h2>
<p>For a host principal, use the <code>ipa host-add-principal</code> command to add a principal alias. The alias must also be a host principal, i.e. must have the form <code>host/$hostname</code>:</p>
<pre><code>% ipa host-add-principal \
    verylongverylongverylongverylongverylongverylonghostname.ipa.local \
    host/longhostname.ipa.local
----------------------------------------------------------------------------------------------
Added new aliases to host &quot;verylongverylongverylongverylongverylongverylonghostname.ipa.local&quot;
----------------------------------------------------------------------------------------------
Host name: verylongverylongverylongverylongverylongverylonghostname.ipa.local
Principal alias: host/verylongverylongverylongverylongverylongverylonghostname.ipa.local@IPA.LOCAL,
                 host/longhostname.ipa.local@IPA.LOCAL</code></pre>
<p>For a service principal, use the <code>ipa service-add-principal</code> command. Ensure the principal alias has the same service type as the subject principal’s <em>canonical name</em> (i.e. the value its <code>krbcanonicalname</code> attribute). For example, if the canonical principal name is <code>HTTP/$LONGHOSTNAME</code>, then the principal alias should be <code>HTTP/$SHORTHOSTNAME</code>.</p>
<p>I omitted the realm parts of principal names (the default realm will be added automatically). For the avoidance of doubt, the princpial alias must have the same realm as the canonical principal.</p>
<h2 id="creating-a-csr">Creating a CSR <a href="#creating-a-csr">§</a></h2>
<p>There are many different ways to create a CSR. I will give a single example using OpenSSL. The private key already exists (file <code>key.pem</code>).</p>
<p>The configuration file:</p>
<pre><code>% cat longhostname.conf
[ req ]
prompt = no
encrypt_key = no

distinguished_name = dn
req_extensions = exts

[ dn ]
commonName = &quot;longhostname.ipa.local

[ exts ]
subjectAltName=DNS:verylongverylongverylongverylongverylongverylonghostname.ipa.local</code></pre>
<p>Create the CSR:</p>
<pre><code>% openssl req -new -key key.pem \
    -config longhostname.conf -extensions exts \
    &gt; longhostname.csr</code></pre>
<h2 id="issuing-the-certificate">Issuing the certificate <a href="#issuing-the-certificate">§</a></h2>
<p>Now we can issue the certificate:</p>
<pre><code>% ipa cert-request longhostname.csr \
    --principal host/verylongverylongverylongverylongverylongverylonghostname.ipa.local
  Issuing CA: ipa
  Certificate: MIIE...
  Subject: CN=longhostname.ipa.local,O=IPA.LOCAL 202009291726
  Subject DNS name: verylongverylongverylongverylongverylongverylonghostname.ipa.local,
                    longhostname.ipa.local
  Issuer: CN=Certificate Authority,O=IPA.LOCAL 202009291726
  Not Before: Mon Oct 19 13:46:16 2020 UTC
  Not After: Thu Oct 20 13:46:16 2022 UTC
  Serial number: 11
  Serial number (hex): 0xB</code></pre>
<p>The CN attribute contains the shorter host name, and the SAN extension contains both the long and shorter hostnames. (We did not include the short hostname in the CSR SAN extension, but the <code>CommonNameToSANDefault</code> profile component copied it there).</p>
<h2 id="supporting-san-only-certificates">Supporting SAN-only certificates <a href="#supporting-san-only-certificates">§</a></h2>
<p>This workaround is straightforward but it is not the ideal solution. A better approach is to enhance FreeIPA and Dogtag to support issuing certificates with an empty Subject DN, using only the Subject Alternative Name extension to carry subject information.</p>
<p>RFC 5280 allows an empty Subject DN in a certificate, in which case the certificate must include the SAN extension, which must be marked as <em>critical</em>. <a href="https://tools.ietf.org/html/rfc6125#section-2.3">RFC 6125</a> further clarifies that such a certificate is acceptable for use with TLS.</p>
<p><a href="https://pagure.io/freeipa/issue/5706">Upstream ticket #5706</a> requests support for SAN-only certificates. The work will involve:</p>
<ul>
<li>Change the <code>ipa cert-request</code> command to accept empty subjects. When the subject is empty ensure a non-empty SAN extension is present in the CSR, and that it is marked criticial. This is straightforward.</li>
<li>On the Dogtag side we must implement new behaviour in the request processor to ensure that the certificate to be issued satisfies the X.509 requirements about empty/non-empty Subject DN and the presence and criticality of the SAN extension.</li>
<li>It may be necessary to define a new profile default or constraint component that allows an empty subject DN.</li>
<li>It is likely that FreeIPA will need to either modify the default profile (<code>caIPAserviceCert</code>) to allow for an empty Subject DN, or ship a separate profile that is suitable.</li>
</ul>]]></summary>
</entry>
<entry>
    <title>Dogtag, number ranges and VLV indices</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2020-09-17-dogtag-vlv-corruption.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2020-09-17-dogtag-vlv-corruption.html</id>
    <published>2020-09-17T00:00:00Z</published>
    <updated>2020-09-17T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="dogtag-number-ranges-and-vlv-indices">Dogtag, number ranges and VLV indices</h1>
<p>In a <a href="2019-07-26-dogtag-replica-ranges.html">previous post</a> I explained Dogtag’s identifier range management. This is how a Dogtag replica knows what range it should use to assign serial numbers, request IDs, etc. What that article did not cover is how Dogtag at startup works out <em>where it is up to</em> in the range. In this post I explain how uses LDAP <em>Virtual List View</em> to do that, how it can break, and how to fix it.</p>
<h2 id="ldap-virtual-list-view">LDAP Virtual List View <a href="#ldap-virtual-list-view">§</a></h2>
<p>The LDAP protocol has an optional extension called <em>Virtual List View (VLV)</em>, which is specified in an <a href="https://datatracker.ietf.org/doc/draft-ietf-ldapext-ldapv3-vlv/">expired Internet-Draft</a>. VLV supports result <em>paging</em> and is an extension of the <em>Server Side Sort (SSS)</em> control (<a href="https://tools.ietf.org/html/rfc2891">RFC 2891</a>). For a search that is covered by a VLV index, a client can specify a page size and offset and get just that portion of the result. It can also seek a specified attribute value and return nearby results.</p>
<p>In 389DS / RHDS, a VLV index is defined by two objects under <code>cn=config</code>. One of the VLV indices used in Dogtag is the search of all certificates sorted by serial number:</p>
<pre class="ldif"><code>dn: cn=allCerts-pki-tomcat,
    cn=ipaca, cn=ldbm database, cn=plugins, cn=config
objectClass: top
objectClass: vlvSearch
cn: allCerts-pki-tomcat
vlvBase: ou=certificateRepository,ou=ca,o=ipaca
vlvScope: 1
vlvFilter: (certstatus=*)

dn: cn=allCerts-pki-tomcatIndex, cn=allCerts-pki-tomcat,
    cn=ipaca, cn=ldbm database, cn=plugins, cn=config
objectClass: top
objectClass: vlvIndex
cn: allCerts-pki-tomcatIndex
vlvSort: serialno
vlvEnabled: 0
vlvUses: 0</code></pre>
<p>The first object defines the search base and filter. When performing a VLV search, these <strong>must match</strong>. The second object declares which attribute is the sort key. To perform a VLV search the client must use both the SSS control (which chooses the sort key) and the VLV control (which selects the page or the value of interest).</p>
<h2 id="dogtag-range-initialisation">Dogtag range initialisation <a href="#dogtag-range-initialisation">§</a></h2>
<p>When Dogtag is starting up, for each active identifier range it has to determine the first unused number. It uses VLV searches to do this. For serial numbers, it uses the VLV index shown above. For request IDs and other ranges, there are other indices. The VLV search targets the upper limit of the range, and requests the preceding values. It then looks for the highest value in the result that is also within the active range. This is the last number that was used; we increment it to get the next available number.</p>
<p>To make it a bit more concrete, we can perform a VLV search ourselves using <code>ldapsearch</code>:</p>
<pre><code># ldapsearch -LLL -D &quot;cn=Directory Manager&quot; -w $DM_PASS \
    -b ou=certificateRepository,ou=ca,o=ipaca -s one \
    -E &#39;sss=serialno&#39; -E &#39;vlv=1/0:09267911168&#39; \
    &#39;(certStatus=*)&#39; 1.1
dn: cn=397,ou=certificateRepository,ou=ca,o=ipaca

dn: cn=267911185,ou=certificateRepository,ou=ca,o=ipaca

# sortResult: (0) Success
# vlvResultpos=2 count=177 context= (0) Success</code></pre>
<p>In this search the target value (end of the active range) is <code>09267911168</code>. This is the integer <code>267911168</code> preceded by a two-digit length value. This is needed because the <code>serialno</code> attribute has <code>Directory String</code> syntax, which is sorted lexicographically. The <code>1/0</code> part of the control is asking for one value preceding the target value, and zero values following it.</p>
<p>The result contains two objects: <code>397</code> (which precedes the target) and <code>267911185</code> (which follows it). Why did we get a number following the target value? The target entry is the first entry whose sort attribute value is <em>greater than or equal</em> the target value. In this way, results greater than the target can appear in the result, as happened here.</p>
<p>The search above relates to the range <code>1..267911168</code>. The result shows us to initialise the repository with <code>397</code> as the “last used” number. The next certificate issued by this replica will have serial number <code>398</code>.</p>
<h2 id="vlv-index-corruption">VLV index corruption <a href="#vlv-index-corruption">§</a></h2>
<p>If a VLV index is corrupt or incomplete, Dogtag could initialise a repository with a too-low “last used” number. This could happen for serial numbers, request IDs or any other kind of managed range. When that happens, CA operations including certificate issuance or CSR submission could fail.</p>
<p>In fact, the <code>ldapsearch</code> above is from a customer case. A full search of the <code>ou=certificateRepository</code> showed thousands of certificates that were not included in the VLV index. If CA operations are failing due to LDAP “Object already exists” errors, you can perform this check to confirm or rule out VLV index corruption as the source of the problem. Keep in mind that VLV indices are maintained separately on each replica. Checks have to be performed on the replica where the problem is occurring.</p>
<h2 id="rebuilding-vlv-indices">Rebuilding VLV indices <a href="#rebuilding-vlv-indices">§</a></h2>
<p>389DS makes it easy to rebuild a VLV index. You create a <em>task</em> object and the DS takes care of it. For Dogtag, we even provide a template LDIF file for a task that reindexes <em>all</em> the VLV indices that Dogtag creates and uses.</p>
<p>First, copy and fill the template:</p>
<pre><code>$ /bin/cp /usr/share/pki/ca/conf/vlvtasks.ldif .
$ sed -i &quot;s/{instanceId}/pki-tomcat/g&quot; vlvtasks.ldif
$ sed -i &quot;s/{database}/ipaca/g&quot; vlvtasks.ldif</code></pre>
<p>Note that <code>{database}</code> should be replaced with <code>ipaca</code> in a FreeIPA instance, but for a standalone Dogtag deployment the correct value is usually <code>ca</code>. Now let’s look at the LDIF file:</p>
<pre class="ldif"><code>dn: cn=index1160589769, cn=index, cn=tasks, cn=config
objectclass: top
objectclass: extensibleObject
cn: index1160589769
ttl: 10
nsinstance: ipaca
nsindexVLVAttribute: allCerts-pki-tomcatIndex
# ... 33 more nsindexVLVAttribute values</code></pre>
<p>The <code>cn</code> is just a name for the task. I think you can put anything here. <code>ttl</code> specifies how many seconds 389DS will wait after the task finishes, before deleting it.</p>
<p>This task object refers to VLV indices in the Dogtag database. But you can see all that is needed to rebuild <em>any</em> VLV index is the <code>nsinstance</code> (name of the database) and the <code>nsindexVLVAttribute</code> (name of a VLV index).</p>
<p>Now we add the object, wait a few seconds, and have a look at it:</p>
<pre><code>$ ldapadd -x -D &quot;cn=Directory Manager&quot; -w $DM_PASS \
    -f vlvtasks.ldif
$ sleep 5
$ ldapsearch -x -D &quot;cn=Directory Manager&quot; -w $DM_PASS \
  -b &quot;cn=index1160589769,cn=index,cn=tasks,cn=config&quot;</code></pre>
<pre class="ldif"><code>dn: cn=index1160589769,cn=index,cn=tasks,cn=config
objectClass: top
objectClass: extensibleObject
cn: index1160589769
ttl: 10
nsinstance: ipaca
nsindexvlvattribute: allCerts-pki-tomcatIndex
# .. 33 more nsindexvlvattribute values
nsTaskCurrentItem: 0
nsTaskTotalItems: 1
nsTaskCreated: 20200916021128Z
nsTaskLog:: aXBhY2E6IEluZGV4aW #... (base64-encoded log)
nsTaskStatus: ipaca: Finished indexing.
nsTaskExitCode: 0</code></pre>
<p>We can see that the task finished successfully, and there is some (truncated) log output if we want more details. After a few more seconds, 389DS will delete the object. You can increase the <code>ttl</code> if you want to keep the objects for longer.</p>
<h2 id="discussion">Discussion <a href="#discussion">§</a></h2>
<p>This year I have encountered variations of this problem on several occasions. I don’t know what the cause(s) are, i.e. why VLV indices get corrupted or stop updating. Hopefully DS experts will be able to shed more light on the issue.</p>
<p>We are considering adding an automated check to the FreeIPA <em>Health Check</em> system, specifically for the range management VLVs. The <a href="">GitHub ticket</a> already contains some discussion and high level steps of how the check would work.</p>
<p>The proper fix for this issue is to move to UUIDs for all object identifiers. Serial numbers might need something different but it is the same idea. This work is on the roadmap. <em>So many problems</em> will go away when we make this change.</p>
<p>Historical commentary: I don’t know why the <code>serialno</code>, <code>requestId</code> and other attributes use Directory String syntax, which necessitates the length prefixing hack. Maybe SSS/VLV only work on strings (or it was thus in the past). The code predates our current VCS and the reasons are lost in time. The implication of this is that we can only handle numbers up to 99 decimal digits. Assumptions like this do bother me, but I think we are probably OK here. For my lifetime, anyway.</p>]]></summary>
</entry>
<entry>
    <title>Dynamic volume provisioning with OpenShift storage classes</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2020-08-13-openshift-storage-classes.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2020-08-13-openshift-storage-classes.html</id>
    <published>2020-08-13T00:00:00Z</published>
    <updated>2020-08-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="dynamic-volume-provisioning-with-openshift-storage-classes">Dynamic volume provisioning with OpenShift storage classes</h1>
<p>For containerised applications that require persistent storage, the Kubernetes <code>PersistentVolumeClaim</code> (PVC) object provides the link between a <code>PersistentVolume</code> (PV) and the pod. When scaling such an application or even deploying it the first time, the operator (human or otherwise) has to create the PVC; the pod specification can then refer to it.</p>
<p>For example, a <code>StatefulSet</code> object can optionally specify <code>volumeClaimTemplates</code> alongside the pod <code>template</code>. As the application creates pods, so will it create the associated PVCs according to the defined templates.</p>
<p>But PVCs need PVs to bind to. Can these also be created on the fly? And if so, how can we abstract over the details of the underlying storage provider(s), which may vary from cluster to cluster? In this post I provide an overview of <em>storage classes</em>, which solve these problems.</p>
<h2 id="creating-volumes">Creating volumes <a href="#creating-volumes">§</a></h2>
<p>A cluster can provide a variety of types of volumes: Ceph, NFS, <code>hostPath</code>, iSCSI and several more. Storage types of the infrastructure the cluster is deployed in may also be available, e.g. AWS EBS, Azure Disk, GCE PersistentDisk (PD), Cinder (OpenStack), etc.</p>
<p>Creating a <code>PersistentVolume</code> requires knowing about what volume types are supported, and possibly additional details about that storage type. For example, to create a PV based on a GCE PD:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb1-1"><a href="#cb1-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> PersistentVolume</span></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> pv-test</span></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="at">  </span><span class="fu">capacity</span><span class="kw">:</span></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="at">    </span><span class="fu">storage</span><span class="kw">:</span><span class="at"> 100Gi</span></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="at">  </span><span class="fu">accessModes</span><span class="kw">:</span></span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="at">  </span><span class="kw">-</span><span class="at"> ReadWriteOnce</span></span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="at">  </span><span class="fu">gcePersistentDisk</span><span class="kw">:</span></span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="at">    </span><span class="fu">pdName</span><span class="kw">:</span><span class="at"> my-data-disk</span></span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="at">    </span><span class="fu">fsType</span><span class="kw">:</span><span class="at"> ext4</span></span>
<span id="cb1-13"><a href="#cb1-13"></a><span class="at">  </span><span class="fu">nodeAffinity</span><span class="kw">:</span></span>
<span id="cb1-14"><a href="#cb1-14"></a><span class="at">    </span><span class="fu">required</span><span class="kw">:</span></span>
<span id="cb1-15"><a href="#cb1-15"></a><span class="at">      </span><span class="fu">nodeSelectorTerms</span><span class="kw">:</span></span>
<span id="cb1-16"><a href="#cb1-16"></a><span class="at">      </span><span class="kw">-</span><span class="at"> </span><span class="fu">matchExpressions</span><span class="kw">:</span></span>
<span id="cb1-17"><a href="#cb1-17"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">key</span><span class="kw">:</span><span class="at"> failure-domain.beta.kubernetes.io/zone</span></span>
<span id="cb1-18"><a href="#cb1-18"></a><span class="at">          </span><span class="fu">operator</span><span class="kw">:</span><span class="at"> In</span></span>
<span id="cb1-19"><a href="#cb1-19"></a><span class="at">          </span><span class="fu">values</span><span class="kw">:</span></span>
<span id="cb1-20"><a href="#cb1-20"></a><span class="at">          </span><span class="kw">-</span><span class="at"> us-central1-a</span></span>
<span id="cb1-21"><a href="#cb1-21"></a><span class="at">          </span><span class="kw">-</span><span class="at"> us-central1-b</span></span></code></pre></div>
<p>Creating this PV required:</p>
<ul>
<li>knowing that the cluster provides the GCE PD volume type</li>
<li>knowing the name and region/zones of the PD to use</li>
</ul>
<p>Having to know these details and encoding them into an application’s deployment manifests imposes a greater burden on administrators, or necessitates more complex operators, or results in a less portable application. Or some combination of those outcomes.</p>
<h2 id="storage-classes">Storage classes <a href="#storage-classes">§</a></h2>
<p>What we really want is to abstract over the storage implementations. We want to able to specify some high-level characteristics of the storage (e.g. block or file, fast or slow?). This is what <em>storage classes</em> provide. Then when we create a PVC, we can specify the desired capacity and class, and the cluster should <em>dynamically provision</em> an appropriate volume. As a result, applications are simpler to deploy and more portable.</p>
<p>To see the storage classes available in a cluster:</p>
<pre><code>ftweedal% oc get storageclass
NAME                 PROVISIONER            RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
standard (default)   kubernetes.io/cinder   Delete          WaitForFirstConsumer   true                   28d</code></pre>
<p>This cluster has only one storage class, called <code>standard</code>. It is also the default storage class for this cluster. To use dynamic provisioning, in the PVC spec instead of <code>volumeName</code> specify <code>storageClassName</code>:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb3-1"><a href="#cb3-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> PersistentVolumeClaim</span></span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> pvc-test</span></span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb3-6"><a href="#cb3-6"></a><span class="at">  </span><span class="fu">accessModes</span><span class="kw">:</span></span>
<span id="cb3-7"><a href="#cb3-7"></a><span class="at">    </span><span class="kw">-</span><span class="at"> ReadWriteOnce</span></span>
<span id="cb3-8"><a href="#cb3-8"></a><span class="at">  </span><span class="fu">resources</span><span class="kw">:</span></span>
<span id="cb3-9"><a href="#cb3-9"></a><span class="at">    </span><span class="fu">requests</span><span class="kw">:</span></span>
<span id="cb3-10"><a href="#cb3-10"></a><span class="at">      </span><span class="fu">storage</span><span class="kw">:</span><span class="at"> 10Gi</span></span>
<span id="cb3-11"><a href="#cb3-11"></a><span class="at">  </span><span class="fu">storageClassName</span><span class="kw">:</span><span class="at"> standard</span></span></code></pre></div>
<p>If you want to use the default storage class, you can even omit the <code>storageClassName</code> field:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb4-1"><a href="#cb4-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> PersistentVolumeClaim</span></span>
<span id="cb4-3"><a href="#cb4-3"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb4-4"><a href="#cb4-4"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> pvc-test</span></span>
<span id="cb4-5"><a href="#cb4-5"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb4-6"><a href="#cb4-6"></a><span class="at">  </span><span class="fu">accessModes</span><span class="kw">:</span></span>
<span id="cb4-7"><a href="#cb4-7"></a><span class="at">    </span><span class="kw">-</span><span class="at"> ReadWriteOnce</span></span>
<span id="cb4-8"><a href="#cb4-8"></a><span class="at">  </span><span class="fu">resources</span><span class="kw">:</span></span>
<span id="cb4-9"><a href="#cb4-9"></a><span class="at">    </span><span class="fu">requests</span><span class="kw">:</span></span>
<span id="cb4-10"><a href="#cb4-10"></a><span class="at">      </span><span class="fu">storage</span><span class="kw">:</span><span class="at"> 10Gi</span></span></code></pre></div>
<h2 id="dynamic-provisioning-in-action">Dynamic provisioning in action <a href="#dynamic-provisioning-in-action">§</a></h2>
<p>Let’s see what actually happens when we use dynamic provisioning. We will observe what objects are created and how their status changes as we create, use and delete a PVC that uses the default storage class.</p>
<p>First let’s see what PVs exist:</p>
<pre><code>ftweedal% oc get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                                             STORAGECLASS   REASON    AGE
pvc-d3bc7c81-8a24-4318-a914-296dbdc5ec3f   100Gi      RWO            Delete           Bound     openshift-image-registry/image-registry-storage   standard                 7d22h</code></pre>
<p>There is one PV, with a 100Gi capacity. It is used for the image registry.</p>
<p>Now, lets create <code>pvc-test</code> as specified above:</p>
<pre><code>ftweedal% oc create -f deploy/pvc-test.yaml
persistentvolumeclaim/pvc-test created

ftweedal% oc get pvc pvc-test
NAME       STATUS    VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-test   Pending                                       standard       11s

ftweedal% oc get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                                             STORAGECLASS   REASON    AGE
pvc-d3bc7c81-8a24-4318-a914-296dbdc5ec3f   100Gi      RWO            Delete           Bound     openshift-image-registry/image-registry-storage   standard                 7d22h

ftweedal% oc get pvc pvc-test -o yaml |grep storageClassName
storageClassName: standard</code></pre>
<p>The PVC <code>pvc-test</code> was created and has status <code>pending</code>. No new PV has been created yet. Finally note that the PVC has <code>storageClassName: standard</code> (which is the cluster default).</p>
<p>Now lets create a pod that uses <code>pvc-test</code>, mounting it at <code>/data</code>. The pod spec is:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb7-1"><a href="#cb7-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb7-3"><a href="#cb7-3"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb7-4"><a href="#cb7-4"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> pod-test</span></span>
<span id="cb7-5"><a href="#cb7-5"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb7-6"><a href="#cb7-6"></a><span class="at">  </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb7-7"><a href="#cb7-7"></a><span class="at">    </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> pod-test-container</span></span>
<span id="cb7-8"><a href="#cb7-8"></a><span class="at">      </span><span class="fu">image</span><span class="kw">:</span><span class="at"> freeipa/freeipa-server:fedora-31</span></span>
<span id="cb7-9"><a href="#cb7-9"></a><span class="at">      </span><span class="fu">volumeMounts</span><span class="kw">:</span></span>
<span id="cb7-10"><a href="#cb7-10"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">mountPath</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;/data&quot;</span></span>
<span id="cb7-11"><a href="#cb7-11"></a><span class="at">          </span><span class="fu">name</span><span class="kw">:</span><span class="at"> data</span></span>
<span id="cb7-12"><a href="#cb7-12"></a><span class="at">      </span><span class="fu">command</span><span class="kw">:</span></span>
<span id="cb7-13"><a href="#cb7-13"></a><span class="at">        </span><span class="kw">-</span><span class="at"> sleep</span></span>
<span id="cb7-14"><a href="#cb7-14"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="st">&quot;3600&quot;</span></span>
<span id="cb7-15"><a href="#cb7-15"></a><span class="at">  </span><span class="fu">volumes</span><span class="kw">:</span></span>
<span id="cb7-16"><a href="#cb7-16"></a><span class="at">    </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> data</span></span>
<span id="cb7-17"><a href="#cb7-17"></a><span class="at">      </span><span class="fu">persistentVolumeClaim</span><span class="kw">:</span></span>
<span id="cb7-18"><a href="#cb7-18"></a><span class="at">        </span><span class="fu">claimName</span><span class="kw">:</span><span class="at"> pvc-test</span></span></code></pre></div>
<p>After creating the pod we will write a file under <code>/data</code>, delete then re-create the pod, and observe that the file we wrote persists.</p>
<pre><code>ftweedal% oc create -f deploy/pod-test.yaml
pod/pod-test created

ftweedal% oc exec pod-test -- sh -c &#39;echo &quot;hello world&quot; &gt; /data/foo&#39;

ftweedal% oc delete pod pod-test
pod &quot;pod-test&quot; deleted

ftweedal% oc create -f deploy/pod-test.yaml
pod/pod-test created

ftweedal% oc exec pod-test -- cat /data/foo
hello world

ftweedal% oc delete pod pod-test
pod &quot;pod-test&quot; deleted</code></pre>
<p>This confirms that the PVC works as intended. Let’s check the status of the PVC and PVs to see what happened behind the scenes:</p>
<pre><code>ftweedal% oc get pvc pvc-test
NAME       STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-test   Bound     pvc-26d82d50-8e66-4938-bdee-f28ff2bcb49c   10Gi       RWO            standard       16m

ftweedal% oc get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                                             STORAGECLASS   REASON    AGE
pvc-26d82d50-8e66-4938-bdee-f28ff2bcb49c   10Gi       RWO            Delete           Bound     ftweedal-operator/pvc-test                        standard                 4m53s
pvc-d3bc7c81-8a24-4318-a914-296dbdc5ec3f   100Gi      RWO            Delete           Bound     openshift-image-registry/image-registry-storage   standard                 7d23h</code></pre>
<p>Before creating the pod <code>pvc-test</code> had status <code>Pending</code>. Now it is <code>Bound</code> to the volume <code>pvc-26d82d50-8e66-4938-bdee-f28ff2bcb49c</code> which was dynamically provisioned with capacity 10Gi as required by <code>pvc-test</code>.</p>
<p>Finally as we delete <code>pvc-test</code>, observe the automatic deletion of the dynamically provisioned volume:</p>
<pre><code>ftweedal% oc delete pvc pvc-test
persistentvolumeclaim &quot;pvc-test&quot; deleted

ftweedal% oc get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                                             STORAGECLASS   REASON    AGE
pvc-d3bc7c81-8a24-4318-a914-296dbdc5ec3f   100Gi      RWO            Delete           Bound     openshift-image-registry/image-registry-storage   standard                 7d23h</code></pre>
<p><code>pvc-26d82d50-8e66-4938-bdee-f28ff2bcb49c</code> went away, as expected.</p>
<h2 id="conclusion">Conclusion <a href="#conclusion">§</a></h2>
<p>As we work toward operationalising FreeIPA in OpenShift, I am interested in how we can use storage classes to make for a smooth deployment across different environments and especially those for which OpenShift Dedicated is available.</p>
<p>I also need to learn more about the best practices or common idioms for representing in storage classes the application suitability (e.g. file versus block storage) or performance characteristics of supported volume types in a cluster. To make it a bit more concrete, consider that for performance reasons we might require low-latency/high-throughput block storage for the 389 DS LDAP database storage. How can we express this abstract requirement such that we get a satisfactory result across a variety of “clouds” with no administrator effort? Hopefully storage classes are the answer. But if they are not the whole solution, from what I have learned so far I have a strong feeling that they will be a bit part of the solution.</p>]]></summary>
</entry>
<entry>
    <title>CRLs for Dogtag Lightweight CAs</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2020-06-19-dogtag-lightweight-ca-crl.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2020-06-19-dogtag-lightweight-ca-crl.html</id>
    <published>2020-06-19T00:00:00Z</published>
    <updated>2020-06-19T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="crls-for-dogtag-lightweight-cas">CRLs for Dogtag Lightweight CAs</h1>
<p>A few years ago I implemented <em>lightweight CAs</em> in Dogtag. This feature allows multiple CAs to be hosted in a single Dogtag server instance. For now these are restricted to sub-CAs of the <em>main CA</em> but this is not a fundamental restriction.</p>
<p>An important aspect of CA operation is <em>revocation</em>: the ability to revoke a certificate because of (suspected) key compromise, cessation of operation, it was superseded, etc. There are currently two main ways of conveying revocation status to clients: <em>Certificate Revocation Lists (CRLs)</em> and <em>Online Certificate Status Protocol (OCSP)</em>. CRLs and OCSP have their respective advantages and drawbacks. Suffice to say, for many security-conscious organisations CRLs are important (as is OCSP).</p>
<p>There is currently no support for lightweight CA certificates in CRLs produced by Dogtag. The purpose of this post is to discuss the challenges and possible approaches to closing this gap.</p>
<h2 id="overview-of-ocsp-and-crls">Overview of OCSP and CRLs <a href="#overview-of-ocsp-and-crls">§</a></h2>
<p>OCSP (defined in <a href="https://tools.ietf.org/html/rfc6960">RFC 6960</a>) is a network protocol for determining certificate revocation status. Any relying party (e.g. a web browser validating a server certificate) can ask the CA’s <em>OCSP responder</em> for a signed assertion of whether or not the certificate is revoked. For scalability and performance reasons, TLS servers can periodically obtain OCSP responses for their certificate and convey them to clients in the TLS handshake; this feature is called <a href="https://en.wikipedia.org/wiki/OCSP_stapling">OCSP stapling</a>.</p>
<p>On the other hand, CRLs are a more <em>passive</em> technology. X.509 CRLs are defined alongside X.509 certificates in <a href="https://tools.ietf.org/html/rfc5280">RFC 5280</a>. In the simple case a CRL is a signed, timestamped list of all revoked, non-exired certificates issued by a CA. The CA produces new CRLs on a fixed schedule (e.g. every 4 hours) and publishes them (e.g. on HTTP, in an LDAP directory, etc). Clients <em>somehow</em> obtain and refresh their CRL cache, and consult it when validating certificates. The CRL grows linearly in the number of revoked certificates so on a busy CA the CRL can become <em>huge</em>. Retrieving a large CRL takes time and bandwidth, storing it takes space, and consulting it takes time. The advantage is that validation requires no (additional) network traffic. The assumption is that the clients CRL cache is up to date.</p>
<p>One further downside of CRLs is that they are only as good as their most recent update. What if your CRL is 3 hours old, the certificate of interest was revoked 1 hour ago, and it is still 1 hour until the next CRL gets published? In practice, every approach to revocation suffers from such a delay. Also in practice, the delay duration is often much greater for CRLs than for OCSP.</p>
<h2 id="ocsp-support-for-lightweight-cas">OCSP support for Lightweight CAs <a href="#ocsp-support-for-lightweight-cas">§</a></h2>
<p>The initial release of the Dogtag lightweight CAs feature had OCSP support for certificates issued by lightweight CAs. It works properly and there is nothing more to be said about it.</p>
<h2 id="crl-support-for-lightweight-cas">CRL support for lightweight CAs <a href="#crl-support-for-lightweight-cas">§</a></h2>
<p>As mentioned in the introduction, certificates issued by lightweight CAs are not included in the CRLs produced by Dogtag. <a href="https://pagure.io/dogtagpki/issue/1627">Ticket #1627</a> in the upstream Pagure tracks this issue.</p>
<p>The reason this was not implemented in the initial release (or since) is that in the baseline case, a CRL can only include certificates from a single CA. Say we have the main CA <code>CN=MainCA</code> and lightweight sub-CA <code>CN=SubCA</code>. The CRL cannot include certificates from both CAs, because a CRL is just a list of serial numbers.</p>
<h3 id="indirect-crls">Indirect CRLs <a href="#indirect-crls">§</a></h3>
<p>There is a way around this limitation. The <a href="https://tools.ietf.org/html/rfc5280#section-5.3.3">Certificate Issuer</a> CRL entry extension, if some other extensions on both the certificate and CRL are set up <em>just right</em>, allows a CRL to include certificates from multiple issuers. Such CRLs are called <em>indirect CRLs</em>. Conforming applications are not required to support indirect CRLs, and the extension is <em>critical</em> so there is a risk of compatibility issues if we were to use indirect CRLs for conveying revocation status of certificates issued by lightweight CAs.</p>
<p>Apart from client support for the Certificate Issuer extension the other requirements for indirect CRLs to work are:</p>
<ul>
<li>The certificate’s <em>CRL Distribution Points (CRLDP)</em> extension must include the <code>cRLIssuer</code> field and its value must match the issuer of the CRL.</li>
<li>The CRL must include the <em>Issuing Distribution Point</em> CRL extension that asserts the <code>indirectCRL</code> boolean. This is a critical extension.</li>
<li>The trust anchor for the CRL must be the same as the trust anchor for the certificate. This means that indirect CRLs cannot work for lightweight CAs that do not chain to the same CA. This is only a potential problem if the lightweight CAs feature is enhanced to support hosting unrelated CAs (rather than sub-CAs).</li>
</ul>
<p>So to use indirect CRLs some minor changes to certificate profiles would be required. But the changes would be the same for all profiles and the content of the CRL Distribution Point extension would be the same regardless of which lightweight CA issues the certificate.</p>
<h3 id="separate-crls">Separate CRLs <a href="#separate-crls">§</a></h3>
<p>An alternative approach is to create a separate CRL for each lightweight CA. This would avoid compatibility issues caused by the use of critical extensions that clients are not required to support. It also avoids the trust anchor limitations that would arise when hosting a lightweight CA that does not share a common trust root with the CRL issuer.</p>
<p>From an implementation point of view there are two major challenges with this approach.</p>
<ol type="1">
<li>Dogtag does not generate CRLs implicitly but currently requires explicit configuration for each CRL. The configuration is not stored in LDAP but in the <code>CS.cfg</code> configuration file, so there is no way to dynamically configure new CRLs as new lightweight CAs are created.</li>
<li>The content of the CRL Distribution Point extension will differ according to the CA that is issuing the certificate. The CRLDP content is currently configured per-profile. New profile components or enhancements to the existing CRLDP profile component will be required.</li>
</ol>
<p>In my view it is not acceptable to have to define multiple profiles differing only the CRL Distribution Point extension. The CA issuing the certificate should, by default, set any extensions that relate specifically to itself, including the CRLDP (also <em>Authority Key Identifier</em> and <em>Authority Information Access</em>). For more specialised use cases, the CRLDP content could be <em>overridden</em> or <em>suppressed</em> on a per-profile basis.</p>
<h3 id="deciding-the-approach">Deciding the approach <a href="#deciding-the-approach">§</a></h3>
<p>Indirect CRLs is the lower-effort approach. But before choosing it, we ought to audit certificate verification libraries (especially OpenSSL, NSS and other libraries used in Fedora, RHEL and other Red Hat products) to see if they support indirect CRLs. If support is widespread, the approach is viable. If support is not widespread, it is not a good idea.</p>
<p>Thinking longer-term, this is a good opportunity to improve the administrator experience. Maybe now is a good time to implement useful features like automatic CRL generation for each CA in a Dogtag instance, and profile components that create a CRL Distribution Point extension that points to the CRL for the CA that is issuing the certificate. The current configuration approach is versatile and can handle all kinds of wild CRL scenarios. But it is <em>hostile</em> to getting things right for the common case.</p>
<p>This decision will probably not be mine to make because I will soon be leaving the Dogtag team. But I hope this post is useful to whoever is involved in the eventual decision.</p>
<h3 id="profile-changes">Profile changes <a href="#profile-changes">§</a></h3>
<p>Both of the discussed approaches require some changes to profile configuration. Required profile changes means upgrade steps to update them. This can be tricky especially in mixed-version topologies when new profile components (if any) are present on some servers but not others.</p>
<h3 id="the-do-nothing-option">The “do nothing” option <a href="#the-do-nothing-option">§</a></h3>
<p>Lightweight CAs have been available for nearly 4 years. I can only recall one or two queries about lightweight CA CRL support. To be clear, it is a fair ask. But it seems that OCSP is sufficient for most customers. Or perhaps there is a lack of awareness that CRLs do not include certificates issued by lightweight CAs. Whatever the case, the low demand aligns with my own opinion that although CRL support for lightweight CAs is a nice-to-have, it is not of critical importance to many users or customers.</p>
<h2 id="conclusion">Conclusion <a href="#conclusion">§</a></h2>
<p>In this post I identified two possible approaches to CRL support for lightweight CAs. Each approach has advantages, drawbacks and unique challenges. Never implementing it is also an option to be considered because demand, though it does exist, seems low.</p>
<p>I haven’t often discussed revocation in detail, so it is probably worth mentioning other approaches besides CRLs and OCSP.</p>
<p><em>Ephemeral PKI</em> avoids the problem by only issuing very short lived certificates, e.g. one week, one day or even less! Assuming keys are rotated just as frequently, when certificate lifetimes approach the “lag” time revocation solutions, the revocation solution is not needed.</p>
<p><em>CRLite</em> is an experimental revocation solution currently in development. It achieves fast and scalable revocation checking through cascading Bloom filters produced by an <em>aggregator</em> that records certificate revocations from one or more CAs. The target use case is in fact <em>all publicly trusted CAs</em> and Firefox Nightly already uses the system (non-enforcing, telemetry-only by default). Scott Helme wrote an <a href="https://scotthelme.co.uk/crlite-finally-a-fix-for-broken-revocation/">excellent blog post</a> about it and you can read the <a href="https://obj.umiacs.umd.edu/papers_for_stories/crlite_oakland17.pdf">original paper</a> for the gory details.</p>
<p>One final note. I found some compliance issues with how the CRL Distribution Point extension is configured in the default FreeIPA certificate profiles. A strict reading of <a href="https://tools.ietf.org/html/rfc5280">RFC 5280</a> suggests that the CRL Distribution Point extension data produced by the default FreeIPA profiles would lead to the certificate not being considered in scope of the CRLs produced by Dogtag. This issue is particular to FreeIPA configuration, not a general problem with FreeIPA. More investiation is required and I will probably write a separate post about this in the future.</p>]]></summary>
</entry>

</feed>
