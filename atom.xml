<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Fraser's IdM Blog</title>
    <link href="https://frasertweedale.github.io/blog-redhat/atom.xml" rel="self" />
    <link href="https://frasertweedale.github.io/blog-redhat" />
    <id>https://frasertweedale.github.io/blog-redhat/atom.xml</id>
    <author>
        <name>Fraser Tweedale</name>
        <email>frase@frase.id.au</email>
    </author>
    <updated>2021-10-15T00:00:00Z</updated>
    <entry>
    <title>Creating user namespaces inside containers</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2021-10-15-openshift-userns-in-container.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2021-10-15-openshift-userns-in-container.html</id>
    <published>2021-10-15T00:00:00Z</published>
    <updated>2021-10-15T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="creating-user-namespaces-inside-containers">Creating user namespaces inside containers</h1>
<p>Over the last year I have experiement with user namespace support in OpenShift. That is, making OpenShift run workloads inside a separate user namespace. We’re trying to drive this feature forward, but some people have reservations. Does having processes running as <code>root</code> inside a user namespace present an increased security risk? What if there are kernel bugs…</p>
<p>If you’re worried about the security of user namespaces, OpenShift or Kubernetes user namespace support doesn’t change the game at all. As I demonstrate in this post, you can create and use user namespaces <em>inside</em> your workloads right now.</p>
<h2 id="demo">Demo <a href="#demo" class="section">§</a></h2>
<p>I tested on OpenShift 4.9.0 in the default configuration. So, no explicit user namespace support. I used a stock Fedora container image with the following Pod spec:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> fedora</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">annotations</span><span class="kw">:</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">openshift.io/scc</span><span class="kw">:</span><span class="at"> restricted</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> fedora</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">image</span><span class="kw">:</span><span class="at"> registry.fedoraproject.org/fedora:34-x86_64</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">command</span><span class="kw">:</span><span class="at"> </span><span class="kw">[</span><span class="st">&quot;sleep&quot;</span><span class="kw">,</span><span class="at"> </span><span class="st">&quot;3600&quot;</span><span class="kw">]</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">securityContext</span><span class="kw">:</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">capabilities</span><span class="kw">:</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">drop</span><span class="kw">:</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> CHOWN</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> DAC_OVERRIDE</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> FOWNER</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> FSETID</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> SETPCAP</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> NET_BIND_SERVICE</span></span></code></pre></div>
<p>The Pod will run under the <code>restricted</code> SCC. I explicitly drop a number of default capabilities.</p>
<p>Next I created a project named <code>userns</code>, and new user <code>me</code>.</p>
<pre class="shell"><code>% oc new-project userns
Now using project &quot;userns&quot; on server &quot;https://api.ci-ln-cih2n32-f76d1.origin-ci-int-gce.dev.openshift.com:6443&quot;.

You can add applications to this project with the &#39;new-app&#39; command. For example, try:

    oc new-app rails-postgresql-example

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=k8s.gcr.io/serve_hostname

% oc create user me
user.user.openshift.io/me created

% oc adm policy add-role-to-user edit me
clusterrole.rbac.authorization.k8s.io/edit added: &quot;me&quot;</code></pre>
<p>Operating as <code>me</code> I created the pod:</p>
<pre class="shell"><code>% oc --as me create -f pod-fedora.yaml
pod/fedora created</code></pre>
<p>Soon after, the pod is running. I can see what node it is running on, and its CRI-O container ID:</p>
<pre class="shell"><code>% oc get -o json pod/fedora \
    | jq &#39;.status.phase,
          .spec.nodeName,
          .status.containerStatuses[0].containerID&#39;
&quot;Running&quot;
&quot;ci-ln-cih2n32-f76d1-sjtwq-worker-a-qr5hr&quot;
&quot;cri-o://d164163951604b7fc9506b3a390ec6a14c76dc6077406fc7b5ffcbf81c406f68&quot;</code></pre>
<p>Next I started a shell in my container. I’ll leave it running for now, and come back to it later:</p>
<pre class="shell"><code>% oc exec -it pod/fedora /bin/sh
sh-5.1$</code></pre>
<p>In another terminal, I opened a debug shell on the worker node. Then I used <code>crictl</code> to find out the process ID (<code>pid</code>) of the main container process.</p>
<pre class="shell"><code>% oc debug node/ci-ln-cih2n32-f76d1-sjtwq-worker-a-qr5hr
Starting pod/ci-ln-cih2n32-f76d1-sjtwq-worker-a-qr5hr-debug ...
To use host binaries, run `chroot /host`
Pod IP: 10.0.128.2
If you don&#39;t see a command prompt, try pressing enter.
sh-4.4# chroot /host
sh-4.4# crictl inspect d1641639 | jq .info.pid
18668</code></pre>
<p>Next I used <code>pgrep</code> to find all the processes that share the same set of namespaces as process <code>18668</code>. In other words, processes running in the same pod sandbox.</p>
<pre class="shell"><code>sh-4.4# pgrep --ns 18668 \
    | xargs ps -o user,pid,cmd --sort pid
USER         PID CMD
1000580+   18668 sleep 3600
1000580+   26490 /bin/sh</code></pre>
<p>There are two processes, running under an unpriviled UID. The UID comes from a unique range allocated for the <code>userns</code> project. These two processes are the main container process (<code>sleep</code>), and the shell that I exected a few steps ago. As expected.</p>
<p>Now for the fun part. Back to the shell we opened in <code>pod/fedora</code>. Observe that this shell process has an empty capability set:</p>
<pre class="shell"><code>sh-5.1$ grep Cap /proc/$$/status
CapInh: 0000000000000000
CapPrm: 0000000000000000
CapEff: 0000000000000000
CapBnd: 0000000000000000
CapAmb: 0000000000000000</code></pre>
<p>And yet, using <code>unshare(1)</code> I was able to create a new user namespace. The <code>-r</code> option says to map <code>root</code> in the new user namespace to the user that created the namespace. And that is indeed what happens:</p>
<pre class="shell"><code>sh-5.1$ unshare -U -r
[root@fedora /]# id
uid=0(root) gid=0(root) groups=0(root),65534(nobody)</code></pre>
<p>I confirmed it via the node debug shell. I ran <code>pgrep</code> again, this time restricting the search to processes in the same <code>pid</code> namespace as process <code>18668</code>. The <code>--nslist</code> option gives the list of namespaces to match (all namespaces when not specified).</p>
<pre class="shell"><code>sh-4.4# pgrep --ns 18668 --nslist pid \
    | xargs ps -o user,pid,cmd --sort pid
USER         PID CMD
1000580+   18668 sleep 3600
1000580+   26490 /bin/sh
1000580+   36704 -sh</code></pre>
<p>The new shell has pid <code>36704</code>. Observe that UID <code>0</code> in the container maps to UID <code>1000580000</code>:</p>
<pre class="shell"><code>sh-4.4# cat /proc/36704/uid_map
         0 1000580000          1</code></pre>
<h2 id="discussion">Discussion <a href="#discussion" class="section">§</a></h2>
<p>You can create and use user namespaces inside your containers without any special support from OpenShift or Kubernetes. Therefore, the idea of a OpenShift or Kubernetes feature for running a workload in an isolated user namespace <em>by default</em> does not lead to an increased risk of container escapes or privilege escalation related to processes running as uid 0 in a user namespace.</p>
<p>This is not to gloss over the fact that other parts of a “workloads in user namespaces” feature have to be designed and implemented with care. Particular aspects include pod admission and selection of the unprivileged UIDs to map to. But on the question of the security of the Linux user namespaces feature itself, a first class OpenShift of Kubernetes feature doesn’t introduce any new risk. Whatever risk there is, is there right now.</p>
<p>If some critical security with user namespaces emerges and you need an urgent mitigation, the only option is to alter the container runtime Seccomp policies to block the <code>unshare(2)</code> syscall. This is an advanced topic, involving changes to node configuration. For details, see <a href="https://docs.openshift.com/container-platform/4.8/security/seccomp-profiles.html"><em>Configuring seccomp profiles</em></a> in the official OpenShift documentation.</p>]]></summary>
</entry>
<entry>
    <title>Demo: namespaced systemd workloads on OpenShift</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2021-07-22-openshift-systemd-workload-demo.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2021-07-22-openshift-systemd-workload-demo.html</id>
    <published>2021-07-22T00:00:00Z</published>
    <updated>2021-07-22T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="demo-namespaced-systemd-workloads-on-openshift">Demo: namespaced systemd workloads on OpenShift</h1>
<p>I have spent much of the last year diving deep into OpenShift’s container runtime. The goal: work out how to run systemd-based workloads in <em>user namespaces</em> on OpenShift nodes. The exploration took many twists and turns. But finally, I have achieved the goal.</p>
<p>In this post I recap the journey so far, and <a href="#demo"><strong>demonstrate</strong></a> what I have achieved. Then I will summarise the path(s?) forward from here.</p>
<h2 id="the-journey-so-far">The journey so far <a href="#the-journey-so-far" class="section">§</a></h2>
<p>My <a href="2021-07-21-freeipa-on-openshift-update.html">previous post</a> gives an overview of the FreeIPA on OpenShift project. In particular, it explains our decision to use a “monolithic” systemd-based container. That implementation approach exposed capability gaps in OpenShift and led to a long running series of investigations. I wrote up the results of these investigations across several blog posts, summarised here:</p>
<h3 id="openshift-and-user-namespaces"><a href="2020-11-05-openshift-user-namespace.html"><em>OpenShift and user namespaces</em></a> <a href="#openshift-and-user-namespaces" class="section">§</a></h3>
<p>I observed that OpenShift (4.6 at the time) did not isolate containers in user namespaces. I noted that <a href="https://github.com/kubernetes/enhancements/issues/127">KEP-127</a> proposes user namespace support for Kubernetes (it is <a href="https://github.com/kubernetes/enhancements/pull/2101">still being worked on</a>). CRI-O had also recently <a href="https://github.com/cri-o/cri-o/pull/3944">added support</a> for user namespaces via annotations.</p>
<h3 id="user-namespaces-in-openshift-via-cri-o-annotations"><a href="2020-12-01-openshift-crio-userns.html"><em>User namespaces in OpenShift via CRI-O annotations</em></a> <a href="#user-namespaces-in-openshift-via-cri-o-annotations" class="section">§</a></h3>
<p>I tested CRI-O’s annotation-based user namespace support on OpenShift 4.7 nightlies. I found that the runtime creates a sandbox with a user namespace and the expected UID mappings. I also found that it is necessary to override the <code>net.ipv4.ping_group_range</code> sysctl. Also, the SCC enforcement machinery does not know about user namespaces and therefore the account that creates the container requires the <code>anyuid</code> SCC. These deficiencies still exist today.</p>
<h3 id="user-namespace-support-in-openshift-4.7"><a href="2021-03-03-openshift-4.7-user-namespaces.html"><em>User namespace support in OpenShift 4.7</em></a> <a href="#user-namespace-support-in-openshift-4.7" class="section">§</a></h3>
<p>I continued my investigation after the release of OpenShift 4.7. With the aforementioned caveats, user namespaces work. I also noted an inconsistent treatment of <code>securityContext</code>: specifying <code>runAsUser</code> in the <code>PodSpec</code> maps the container’s UID <code>0</code> to host UID <code>0</code>—a dangerous configuration.</p>
<p>More recently, I noticed that the <code>userns-mode</code> annotation I was using included <code>map-to-root=true</code>. I now understand that it is this configuration that causes this mapping behaviour. I no longer consider it particularly serious. Ideally the SCC enforcement should learn about user namespaces, and prevent unprivileged users from creating containers that run as <code>root</code> (or other system accounts) on the host.</p>
<h3 id="multiple-users-in-user-namespaces-on-openshift"><a href="2021-03-10-openshift-user-namespace-multi-user.html"><em>Multiple users in user namespaces on OpenShift</em></a> <a href="#multiple-users-in-user-namespaces-on-openshift" class="section">§</a></h3>
<p>I verified that workloads that run processes under a variety of user accounts work as expected in user namespaces. I did not use a <em>systemd</em>-based workload to verify this.</p>
<h3 id="systemd-containers-on-openshift-with-cgroups-v2"><a href="2021-03-30-openshift-cgroupv2-systemd.html"><em>systemd containers on OpenShift with cgroups v2</em></a> <a href="#systemd-containers-on-openshift-with-cgroups-v2" class="section">§</a></h3>
<p>I observed that systemd-based workloads run successfully in OpenShift when executed as UID 0 <em>on the host</em>. Such containers can only be created by accounts granted privileged SCCs (e.g. <code>anyuid</code>). When running the container under other UIDs, <em>systemd</em> can’t run because it does not have write permission on the container’s cgroup directory.</p>
<h3 id="using-runc-to-explore-the-oci-runtime-specification"><a href="2021-05-27-oci-runtime-spec-runc.html"><em>Using <code>runc</code> to explore the OCI Runtime Specification</em></a> <a href="#using-runc-to-explore-the-oci-runtime-specification" class="section">§</a></h3>
<p>I investigated how <code>runc</code> (the OCI runtime used in OpenShift) operates, and how it creates cgroups. I identified some potential ways to change the ownership of the container cgroup to the <em>container’s</em> UID 0.</p>
<h3 id="systemd-cgroups-and-subuid-ranges"><a href="2021-06-09-systemd-cgroups-subuid.html"><em>systemd, cgroups and subuid ranges</em></a> <a href="#systemd-cgroups-and-subuid-ranges" class="section">§</a></h3>
<p>I discovered that the systemd <em>transient unit API</em> (which <code>runc</code> uses to create container cgroups) allows specifying a different owner for the new cgroup. Unfortunately, the user must be “known”, in the form of a <code>passwd</code> entity via NSSwitch. A <a href="https://github.com/systemd/systemd/issues/19781">proposal to relax this requirement</a> was provisionally rejected. Other approaches include writing an NSSwitch module to synthesise <code>passwd</code> entities for subuids, or modifying <code>runc</code> to <code>chown(2)</code> the container cgroup after systemd creates it. I decided to experiment with the latter approach.</p>
<h2 id="modifying-runc-to-chown-the-container-cgroup">Modifying <code>runc</code> to <code>chown</code> the container cgroup <a href="#modifying-runc-to-chown-the-container-cgroup" class="section">§</a></h2>
<p>The main challenge in modifying <code>runc</code> was getting my head around the unfamiliar codebase. The actual operations are straightforward. There are two main aspects.</p>
<p>The first aspect is to compute the appropriate owner UID for the cgroup, and tell it to the cgroup manager object. I <a href="2021-06-09-systemd-cgroups-subuid.html#determining-the-uid">described the algorithm</a> in a previous post. The <code>config.HostRootUID()</code> method already implements this computation. I was able to reuse it.</p>
<p>The second aspect is to actually <code>chown(2)</code> the relevant cgroup files and directories. I previously observed systemd’s behaviour when creating units owned by arbitrary users. systemd <code>chown</code>s the container’s cgroup directory, and the <code>cgroup.procs</code>, <code>cgroup.subtree_control</code> and <code>cgroup.threads</code> files within that directory. <code>runc</code> will do the same. The cgroup manager object already knows the path to the container cgroup directory. It changes the owner of the directory and same three files as <em>systemd</em> to the relevant user.</p>
<h2 id="demo">Demo <a href="#demo" class="section">§</a></h2>
<p>Following is a step-by-step demonstration starting with a fresh deployment of OpenShift <code>4.7.20</code>.</p>
<pre class="shell"><code>% oc get clusterversion
NAME      VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
version   4.7.20    True        False         8m52s   Cluster version is 4.7.20</code></pre>
<div class="note">
<p>There is a <a href="https://github.com/cri-o/cri-o/issues/5077">regression</a> in OpenShift 4.8.0 that prevents Pod annotations from being propagated to container OCI configurations. As a consequence, <code>runc</code> does not receive the annotations that trigger the experimental behaviour. I filed a <a href="https://github.com/cri-o/cri-o/pull/5078">pull request</a> that fixes the issue. The patch was accepted and the fix released in OpenShift 4.8.4.</p>
</div>
<p>The latent credential is the cluster <code>admin</code> user. Where relevant, I use the <code>oc --as USER</code> option to execute commands as other users.</p>
<pre class="shell"><code>% oc whoami
system:admin</code></pre>
<h3 id="install-modified-runc-package">Install modified <code>runc</code> package <a href="#install-modified-runc-package" class="section">§</a></h3>
<p>List the nodes in the cluster:</p>
<pre class="shell"><code>% oc get node
NAME                                       STATUS   ROLES    AGE   VERSION
ci-ln-jqbnbfk-f76d1-gnkkv-master-0         Ready    master   61m   v1.20.0+01c9f3f
ci-ln-jqbnbfk-f76d1-gnkkv-master-1         Ready    master   61m   v1.20.0+01c9f3f
ci-ln-jqbnbfk-f76d1-gnkkv-master-2         Ready    master   61m   v1.20.0+01c9f3f
ci-ln-jqbnbfk-f76d1-gnkkv-worker-a-vrbnv   Ready    worker   52m   v1.20.0+01c9f3f
ci-ln-jqbnbfk-f76d1-gnkkv-worker-b-dxk6k   Ready    worker   52m   v1.20.0+01c9f3f
ci-ln-jqbnbfk-f76d1-gnkkv-worker-c-db89w   Ready    worker   52m   v1.20.0+01c9f3f</code></pre>
<p>For each worker node, open a node debug shell and use <code>rpm-ostree override replace</code> to install the modified <code>runc</code> (one worker shown):</p>
<pre class="shell"><code>% oc debug node/ci-ln-jqbnbfk-f76d1-gnkkv-worker-a-vrbnv
Starting pod/ci-ln-jqbnbfk-f76d1-gnkkv-worker-a-vrbnv-debug ...
To use host binaries, run `chroot /host`
Pod IP: 10.0.32.2
If you don&#39;t see a command prompt, try pressing enter.
sh-4.2# chroot /host
sh-4.4# rpm-ostree override replace https://ftweedal.fedorapeople.org/runc-1.0.0-990.rhaos4.8.gitcd80260.el8.x86_64.rpm
Downloading &#39;https://ftweedal.fedorapeople.org/runc-1.0.0-990.rhaos4.8.gitcd80260.el8.x86_64.rpm&#39;... done!
Checking out tree 9767154... done
No enabled rpm-md repositories.
Importing rpm-md... done
Resolving dependencies... done
Applying 1 override
Processing packages... done
Running pre scripts... done
Running post scripts... done
Running posttrans scripts... done
Writing rpmdb... done
Writing OSTree commit... done
Staging deployment... done
Upgraded:
  runc 1.0.0-96.rhaos4.8.gitcd80260.el8 -&gt; 1.0.0-990.rhaos4.8.gitcd80260.el8
Run &quot;systemctl reboot&quot; to start a reboot</code></pre>
<div class="note">
<p>Instead of installing the modified <code>runc</code> on all worker nodes, you could update one node and use <code>.spec.nodeAffinity</code> in the <code>PodSpec</code> to force the pod to run on that node.</p>
</div>
<p>Don’t worry about the restart right now (it will happen in the next step). Exit the debug shell:</p>
<pre class="shell"><code>sh-4.4# exit
sh-4.2# exit

Removing debug pod ...</code></pre>
<h3 id="enable-user-namespaces-and-cgroups-v2">Enable user namespaces and cgroups v2 <a href="#enable-user-namespaces-and-cgroups-v2" class="section">§</a></h3>
<p>The following <code>MachineConfig</code> enables cgroups v2 and CRI-O annotation-based user namespace support:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> machineconfiguration.openshift.io/v1</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> MachineConfig</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">labels</span><span class="kw">:</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">machineconfiguration.openshift.io/role</span><span class="kw">:</span><span class="at"> worker</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> userns-cgv2</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">kernelArguments</span><span class="kw">:</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> systemd.unified_cgroup_hierarchy=1</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> cgroup_no_v1=&quot;all&quot;</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> psi=1</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">config</span><span class="kw">:</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">ignition</span><span class="kw">:</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">version</span><span class="kw">:</span><span class="at"> </span><span class="fl">3.1.0</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">storage</span><span class="kw">:</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">files</span><span class="kw">:</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> </span><span class="fu">path</span><span class="kw">:</span><span class="at"> /etc/crio/crio.conf.d/99-crio-userns.conf</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">overwrite</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">contents</span><span class="kw">:</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">source</span><span class="kw">:</span><span class="at"> data:text/plain;charset=utf-8;base64,W2NyaW8ucnVudGltZS5ydW50aW1lcy5ydW5jXQphbGxvd2VkX2Fubm90YXRpb25zPVsiaW8ua3ViZXJuZXRlcy5jcmktby51c2VybnMtbW9kZSJdCg==</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> </span><span class="fu">path</span><span class="kw">:</span><span class="at"> /etc/subuid</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">overwrite</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">contents</span><span class="kw">:</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">source</span><span class="kw">:</span><span class="at"> data:text/plain;charset=utf-8;base64,Y29yZToxMDAwMDA6NjU1MzYKY29udGFpbmVyczoyMDAwMDA6MjY4NDM1NDU2Cg==</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> </span><span class="fu">path</span><span class="kw">:</span><span class="at"> /etc/subgid</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">overwrite</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">contents</span><span class="kw">:</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">source</span><span class="kw">:</span><span class="at"> data:text/plain;charset=utf-8;base64,Y29yZToxMDAwMDA6NjU1MzYKY29udGFpbmVyczoyMDAwMDA6MjY4NDM1NDU2Cg==</span></span></code></pre></div>
<p>The file <code>/etc/crio/crio.conf.d/99-crio-userns.conf</code> enables CRI-O’s annotation-based user namespace support. Its content (base64-encoded in the <code>MachineConfig</code>) is:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode ini"><code class="sourceCode ini"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">[crio.runtime.runtimes.runc]</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="dt">allowed_annotations</span><span class="ot">=</span><span class="st">[&quot;io.kubernetes.cri-o.userns-mode&quot;]</span></span></code></pre></div>
<p>The <code>MachineConfig</code> also overrides <code>/etc/subuid</code> and <code>/etc/subgid</code>, defining sub-id ranges for user namespaces. The content is the same for both files:</p>
<pre><code>core:100000:65536
containers:200000:268435456</code></pre>
<p>Create the <code>MachineConfig</code>:</p>
<pre class="shell"><code>% oc create -f machineconfig-userns-cgv2.yaml
machineconfig.machineconfiguration.openshift.io/userns-cgv2 created</code></pre>
<p>Wait for the Machine Config Operator to apply the changes and reboot the worker nodes:</p>
<pre class="shell"><code>% oc wait mcp/worker --for condition=updated --timeout=-1s
machineconfigpool.machineconfiguration.openshift.io/worker condition met</code></pre>
<p>It will take several minutes, as worker nodes get rebooted one a time.</p>
<h3 id="create-project-and-user">Create project and user <a href="#create-project-and-user" class="section">§</a></h3>
<p>Create a new project called <code>test</code>:</p>
<pre class="shell"><code>% oc new-project test
Now using project &quot;test&quot; on server &quot;https://api.ci-ln-jqbnbfk-f76d1.origin-ci-int-gce.dev.openshift.com:6443&quot;.

You can add applications to this project with the &#39;new-app&#39; command. For example, try:

    oc new-app ruby~https://github.com/sclorg/ruby-ex.git

to build a new example application in Python. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node</code></pre>
<p>The output shows the public domain name of this cluster: <code>ci-ln-jqbnbfk-f76d1.origin-ci-int-gce.dev.openshift.com</code>. We need to know this for creating the route in the next step.</p>
<p>Create a user called <code>test</code>. Grant it <code>admin</code> role on project <code>test</code>, and the <code>anyuid</code> Security Context Constraint (SCC) privilege:</p>
<pre class="shell"><code>% oc create user test
user.user.openshift.io/test created
% oc adm policy add-role-to-user admin test
clusterrole.rbac.authorization.k8s.io/admin added: &quot;test&quot;
% oc adm policy add-scc-to-user anyuid test
securitycontextconstraints.security.openshift.io/anyuid added to: [&quot;test&quot;]</code></pre>
<h3 id="create-service-and-route">Create service and route <a href="#create-service-and-route" class="section">§</a></h3>
<p>Create a service to provide HTTP access to pods matching the <code>app: nginx</code> selector:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Service</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> nginx</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">selector</span><span class="kw">:</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">app</span><span class="kw">:</span><span class="at"> nginx</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">ports</span><span class="kw">:</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> </span><span class="fu">protocol</span><span class="kw">:</span><span class="at"> TCP</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">port</span><span class="kw">:</span><span class="at"> </span><span class="dv">80</span></span></code></pre></div>
<pre class="shell"><code>% oc create -f service-nginx.yaml
service/nginx created</code></pre>
<p>The following route definition will provide HTTP ingress from outside the cluster:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Route</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> nginx</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">host</span><span class="kw">:</span><span class="at"> nginx.apps.ci-ln-jqbnbfk-f76d1.origin-ci-int-gce.dev.openshift.com</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">to</span><span class="kw">:</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">kind</span><span class="kw">:</span><span class="at"> Service</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> nginx</span></span></code></pre></div>
<p>Note the <code>host</code> field. Its value is <code>nginx.apps.$CLUSTER_DOMAIN</code>. Change it to the proper value for your cluster, then create the route:</p>
<pre class="shell"><code>% oc create -f route-nginx.yaml
route.route.openshift.io/nginx created</code></pre>
<p>There is no pod to route the traffic to… yet.</p>
<h3 id="create-pod">Create pod <a href="#create-pod" class="section">§</a></h3>
<p>The pod specification is:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> nginx</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">labels</span><span class="kw">:</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">app</span><span class="kw">:</span><span class="at"> nginx</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">annotations</span><span class="kw">:</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">openshift.io/scc</span><span class="kw">:</span><span class="at"> restricted</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">io.kubernetes.cri-o.userns-mode</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;auto:size=65536&quot;</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">securityContext</span><span class="kw">:</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">sysctls</span><span class="kw">:</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;net.ipv4.ping_group_range&quot;</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">value</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;0 65535&quot;</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> nginx</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">image</span><span class="kw">:</span><span class="at"> quay.io/ftweedal/test-nginx:latest</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">tty</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span></code></pre></div>
<p>Create the pod:</p>
<pre class="shell"><code>% oc --as test create -f pod-nginx.yaml
pod/nginx created</code></pre>
<p>After a few seconds, the pod is running:</p>
<pre class="shell"><code>% oc get -o json pod/nginx | jq .status.phase
&quot;Running&quot;</code></pre>
<p>Tail the pod’s log. Observe the final lines of systemd boot output and the login prompt:</p>
<pre class="shell"><code>% oc logs --tail 10 pod/nginx
[  OK  ] Started The nginx HTTP and reverse proxy server.
[  OK  ] Reached target Multi-User System.
[  OK  ] Reached target Graphical Interface.
         Starting Update UTMP about System Runlevel Changes...
[  OK  ] Finished Update UTMP about System Runlevel Changes.

Fedora 33 (Container Image)
Kernel 4.18.0-305.3.1.el8_4.x86_64 on an x86_64 (console)

nginx login: %</code></pre>
<div class="note">
<p>Without <code>tty: true</code> in the <code>Container</code> spec, the pod won’t produce any output and <code>oc logs</code> won’t have anything to show.</p>
</div>
<p>The log tail also shows that systemd started the <code>nginx</code> service. We already set up a <code>route</code> in the previous step. Use <code>curl</code> to issue an HTTP request and verify that the service is running properly:</p>
<pre class="shell"><code>% curl --head \
    nginx.apps.ci-ln-jqbnbfk-f76d1.origin-ci-int-gce.dev.openshift.com
HTTP/1.1 200 OK
Server: nginx/1.18.0
Date: Wed, 21 Jul 2021 06:55:38 GMT
Content-Type: text/html
Content-Length: 5564
Last-Modified: Mon, 27 Jul 2020 22:20:49 GMT
ETag: &quot;5f1f5341-15bc&quot;
Accept-Ranges: bytes
Set-Cookie: 6cf5f3bc2fa4d24f45018c591d3617c3=f114e839b2eef9cdbe00856f18a06336; path=/; HttpOnly
Cache-control: private</code></pre>
<h3 id="verify-sandbox">Verify sandbox <a href="#verify-sandbox" class="section">§</a></h3>
<p>Now let’s verify that the container is indeed running in a user namespace. Container UIDs must map to unprivileged UIDs on the host. Query the worker node on which the pod is running, and its CRI-O container ID:</p>
<pre class="shell"><code>% oc get -o json pod/nginx | jq \
    &#39;.spec.nodeName, .status.containerStatuses[0].containerID&#39;
&quot;ci-ln-jqbnbfk-f76d1-gnkkv-worker-c-db89w&quot;
&quot;cri-o://bf2b3d15cbd6944366e29927988ba30bc36d1efee00c28fb4c6d5b2036e462b0&quot;</code></pre>
<p>Start a debug shell on the node and query the PID of the container init process:</p>
<pre class="shell"><code>% oc debug node/ci-ln-jqbnbfk-f76d1-gnkkv-worker-c-db89w
Starting pod/ci-ln-jqbnbfk-f76d1-gnkkv-worker-c-db89w-debug ...
To use host binaries, run `chroot /host`
Pod IP: 10.0.32.4
If you don&#39;t see a command prompt, try pressing enter.
sh-4.2# chroot /host
sh-4.4# crictl inspect bf2b3d | jq .info.pid
7759</code></pre>
<p>Query the UID map and process tree of the container:</p>
<pre class="shell"><code>sh-4.4# cat /proc/7759/uid_map
         0     200000      65536
sh-4.4# pgrep --ns 7759 | xargs ps -o user,pid,cmd --sort pid
USER         PID CMD
200000      7759 /sbin/init
200000      7796 /usr/lib/systemd/systemd-journald
200193      7803 /usr/lib/systemd/systemd-resolved
200000      7806 /usr/lib/systemd/systemd-homed
200000      7807 /usr/lib/systemd/systemd-logind
200081      7809 /usr/bin/dbus-broker-launch --scope system --audit
200000      7812 /sbin/agetty -o -p -- \u --noclear --keep-baud console 115200,38400,9600 xterm
200081      7813 dbus-broker --log 4 --controller 9 --machine-id 2f2fcc4033c5428996568ca34219c72a --max-bytes 5
200000      7815 nginx: master process /usr/sbin/nginx
200999      7816 nginx: worker process
200999      7817 nginx: worker process
200999      7818 nginx: worker process
200999      7819 nginx: worker process</code></pre>
<p>This confirms that the container has a user namespace. The container’s UID range is <code>0</code>–<code>65535</code>, which maps to the host UID range <code>200000</code>–<code>265535</code>. The <code>ps</code> output shows various services running under systemd, running under unprivileged host UIDs in this range.</p>
<p>So, everything is running as expected. One last thing: let’s look at the cgroup ownership. Query the container’s <code>cgroupsPath</code>:</p>
<pre class="shell"><code>sh-4.4# crictl inspect bf2b3d | jq .info.runtimeSpec.linux.cgroupsPath
&quot;kubepods-besteffort-podc7f11ee7_e178_4dea_9d8c_c005ad648988.slice:crio:bf2b3d15cbd6944366e29927988ba30bc36d1efee00c28fb4c6d5b2036e462b0&quot;</code></pre>
<p>The value isn’t a filesystem path. <code>runc</code> interprets it relative to an implementation-defined location. We expect the cgroup directory and the three files mentioned earlier to be owned by the user that maps to UID <code>0</code> in the container’s user namespace. In my case, that’s <code>200000</code>. We also expect to see scopes and slices created by systemd <strong>in the container</strong> to be owned by the same user.</p>
<pre class="shell"><code>sh-4.4# ls -ali /sys/fs/cgroup\
/kubepods.slice/kubepods-besteffort.slice\
/kubepods-besteffort-podc7f11ee7_e178_4dea_9d8c_c005ad648988.slice\
/crio-bf2b3d15cbd6944366e29927988ba30bc36d1efee00c28fb4c6d5b2036e462b0.scope \
    | grep 200000
14755 drwxr-xr-x.  5 200000 root   0 Jul 21 06:00 .
14757 -rw-r--r--.  1 200000 root   0 Jul 21 06:00 cgroup.procs
14760 -rw-r--r--.  1 200000 root   0 Jul 21 06:00 cgroup.subtree_control
14758 -rw-r--r--.  1 200000 root   0 Jul 21 06:00 cgroup.threads
14806 drwxr-xr-x.  2 200000 200000 0 Jul 21 06:00 init.scope
14835 drwxr-xr-x. 11 200000 200000 0 Jul 21 06:15 system.slice
14922 drwxr-xr-x.  2 200000 200000 0 Jul 21 06:00 user.slice</code></pre>
<p>Note the <em>inode</em> of the container cgroup directory: <code>14755</code>. We can query the inode and ownership of <code>/sys/fs/cgroup</code> <em>within the pod</em>:</p>
<pre class="shell"><code>% oc exec pod/nginx -- ls -ldi /sys/fs/cgroup
14755 drwxr-xr-x. 5 root nobody 0 Jul 21 06:00 /sys/fs/cgroup</code></pre>
<p>The inode is the same; this is indeed the same cgroup. But within the container’s user namespace, the owner appears as <code>root</code>.</p>
<p>This concludes the verification steps. With my modified version of <code>runc</code>, systemd-based workloads are indeed working properly in user namespaces.</p>
<h2 id="next-steps">Next steps <a href="#next-steps" class="section">§</a></h2>
<p>I submitted a <a href="https://github.com/opencontainers/runc/pull/3057">pull request</a> with these changes. It remains to be seen if the general approach will be accepted, but initial feedback is positive. Some implementation changes are needed. I might have to hide the behaviour behind a feature gate (e.g. to be activated via an annotation). I also need to write tests and documentation.</p>
<p>I also need to raise a ticket for the SCC issue. The requirement for <code>RunAsAny</code> (which is granted by the <code>anyuid</code> SCC) should be relaxed when the sandbox has a user namespace. The SCC enforcement machinery needs to be enhanced to understand user namespaces, so that unprivileged OpenShift user accounts can run workloads in them.</p>
<p>It would be nice to find a way to avoid the sysctl override to allow the container user to use <code>ping</code>. This is a much lower priority.</p>
<p>Alongside these matters, I can begin testing the FreeIPA container in the test environment. Although systemd is now working, I need to see if the FreeIPA’s constituent services will run properly. I anticipate that I will need to tweak the Pod configuration somewhat. But are there more runtime capability gaps waiting to be discovered? I don’t have a particular suspicion about it, but I do need to know for certain, one way or the other. So expect another blog post soon!</p>]]></summary>
</entry>
<entry>
    <title>FreeIPA on OpenShift: July 2021 update</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2021-07-21-freeipa-on-openshift-update.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2021-07-21-freeipa-on-openshift-update.html</id>
    <published>2021-07-21T00:00:00Z</published>
    <updated>2021-07-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="freeipa-on-openshift-july-2021-update">FreeIPA on OpenShift: July 2021 update</h1>
<p>Over the last year I’ve done a lot of investigations into OpenShift, and container runtimes more generally. The driver of this work is the FreeIPA on OpenShift project (known within Red Hat as IDMOCP). I published the results of my investigations in numerous blog posts, but I have not yet written much about <em>why</em> we are doing this at all.</p>
<p>So it’s time to fix that. In this short post I discuss why we want FreeIPA on OpenShift, and the major decision that put us on our current implementation path.</p>
<p>FreeIPA is a centralised identity management system for the enterprise. You enrol users, hosts and services, and configure access policies and other security mechanisms. The system provides authentication and policy enforcement mechanisms. It is similar to Microsoft Active Directory (and indeed can integrate with AD). FreeIPA is a complex system with lots of components including:</p>
<ul>
<li>LDAP server (389 DS / RHDS)</li>
<li>Kerberos KDC (MIT Kerberos)</li>
<li>Certificate authority (Dogtag / RHCS)</li>
<li>HTTP API (Apache httpd and a lot of Python code)</li>
<li>Host client daemon (SSSD)</li>
<li>several smaller supporting services</li>
<li>installation and administration tools</li>
</ul>
<p>FreeIPA is available on Fedora and RHEL. You install the RPMs and the installation program configures the system. It is intended to be deployed on a dedicated machine (VM or bare metal).</p>
<p>We are motivated to support FreeIPA on OpenShift for several reasons, including:</p>
<ul>
<li><p>Easily providing identity services to applications running on OpenShift.</p></li>
<li><p>Leveraging OpenShift and Kubernetes orchestration, scalaing and management features to improve robustness and reduce management overhead of FreeIPA deployments.</p></li>
<li><p>Offering FreeIPA, hosted on OpenShift, as a managed service.</p></li>
</ul>
<p>Understandably, moving such an application to OpenShift is a non-trivial task. At the beginning of this effort, we had to decide the main implementation approach. There were three options:</p>
<ol type="1">
<li><p>Put the whole system in a single “monolithic container”, with systemd as the init process. At the time (and still today) OpenShift only supports running systemd workloads in privileged containers, which is not acceptable. The runtime needs to evolve to support this use case. Work on <em>some</em> of the missing features (such as user namespaces and cgroups v2) was already underway.</p></li>
<li><p>Deploy different parts of the FreeIPA system in different containers, running unprivileged. This is a fundamental shift from the current architecture and a huge up-front engineering effort. Also, the current architecture has to be maintained and supported for a long time (&gt;10 years). So this approach brings a substantial ongoing cost in maintaining two architectures of the same application. On a technical level, this approach is feasible today.</p></li>
<li><p>Use a VM-based workload (Kata / OpenShift Sandboxed Containers). This option probably has the lowest up-front and ongoing engineering costs. But it requires a bare metal cluster or nested virtualisation, which is not available from most cloud providers. By extension, <a href="https://www.openshift.com/products/dedicated/">OpenShift Dedicated (OSD)</a> also does not supported it. Red Hat managed services run on OSD. Offering a managed service is one of the motivators of our effort. So at this time, VM-based workloads are not an option for us.</p></li>
</ol>
<p>As a small team, and considering the business reality of the existing offering as part of RHEL, we decided to pursue the “monolithic container” approach. We are depending on the OpenShift runtime evolving to a point where it can support fully isolated systemd-based workloads. And that is why I have invested much of the last 12 months in understanding container runtimes and pushing their limits.</p>
<p>Our approach is not “cloud native” and indeed many people have expressed alarm or confusion when we tell them what we are doing. Certainly, if we were designing FreeIPA from the ground up in today’s world, it would look very different from the current architecture. But this is the reality: if you want customers to bring their mature, complex applications onto OpenShift, don’t expect them to spend big money and assume big risk to rearchitect the application to fit the new environment.</p>
<p>What customers actually need is to be able to bring the application across more or less as-is. Then they can realise the benefits (automation, monitoring, scaling, etc) <em>incrementally</em>, with lower up-front costs and less risk.</p>
<p>If my claims are correct, then proper systemd workload support in OpenShift will be a Very Big Deal. But even if I’m wrong, it is still critical for our FreeIPA on OpenShift effort. And it is achievable. In my next post I’ll demonstrate my working proof of concept for user-namespaced systemd workloads on OpenShift.</p>]]></summary>
</entry>
<entry>
    <title>Live-testing changes in OpenShift clusters</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2021-06-29-openshift-live-changes.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2021-06-29-openshift-live-changes.html</id>
    <published>2021-06-29T00:00:00Z</published>
    <updated>2021-06-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="live-testing-changes-in-openshift-clusters">Live-testing changes in OpenShift clusters</h1>
<p>I have been hacking on the <a href="https://github.com/opencontainers/runc"><code>runc</code></a> container runtime. So how do I test my changes in an OpenShift cluster?</p>
<p>One option is to compose a <code>machine-os-content</code> release via <a href="https://github.com/coreos/coreos-assembler"><em>coreos-assembler</em></a>. Then you can deploy or upgrade a cluster with that release. Indeed, this approach is <em>necessary</em> for testing installation and upgrades. It also seems useful for publishing modified versions for other people to test. But it is a heavyweight and time consuming option.</p>
<p>For development I want a more lightweight approach. In this post I’ll demonstrate how to use the <code>rpm-ostree usroverlay</code> and <code>rpm-ostree override replace</code> commands to test changes in a live OpenShift cluster.</p>
<h2 id="background">Background <a href="#background" class="section">§</a></h2>
<p>OpenShift runs on CoreOS. CoreOS uses <a href="https://en.wikipedia.org/wiki/OSTree"><em>OSTree</em></a> to manage the filesystem. Most of the filesystem is immutable. When upgrading, a new filesystem is prepared before rebooting the system. The old filesystem is preserved, so it is easy to roll back.</p>
<p>So I can’t just log onto an OpenShift node and replace <code>/usr/bin/runc</code> with my modified version. Nevertheless, I have seen <a href="https://github.com/openshift/machine-config-operator/blob/master/docs/HACKING.md#directly-applying-changes-live-to-a-node">references</a> to the <code>rpm-ostree usroverlay</code> command. It is supposed to provide a writable overlayfs on <code>/usr</code>, so that you can test modifications. Changes are lost upon reboot, but that’s fine for testing.</p>
<p>There’s also the <code>rpm-ostree override replace …</code> command. This command works on the level of RPM packages. It allows you to install new packages or replace or remove packages. Changes persist across reboots, but it is easy to roll back to the <em>pristine</em> state of the current CoreOS release.</p>
<p>The rest of this article explores how to use these two commands to apply changes to the cluster.</p>
<h2 id="usroverlay-via-debug-container-doesnt-work"><code>usroverlay</code> via debug container (doesn’t work) <a href="#usroverlay-via-debug-container-doesnt-work" class="section">§</a></h2>
<p>I first attempted to use <code>rpm-ostree usroverlay</code> in a node debug pod.</p>
<pre class="shell"><code>% oc debug node/worker-a
Starting pod/worker-a-debug ...
To use host binaries, run `chroot /host`
Pod IP: 10.0.128.2
If you don&#39;t see a command prompt, try pressing enter.
sh-4.2# chroot /host
sh-4.4# rpm-ostree usroverlay
Development mode enabled.  A writable overlayfs is now mounted on /usr.
All changes there will be discarded on reboot.
sh-4.4# touch /usr/bin/foo
touch: cannot touch &#39;/usr/bin/foo&#39;: Read-only file system</code></pre>
<p>The <code>rpm-ostree usroverlay</code> command succeeded. But <code>/usr</code> remained read-only. The debug container has its own mount namespace, which was unaffected. I guess that I need to log into the node directly to use the writable <code>/usr</code> overlay. Perhaps it is also necessary to execute <code>rpm-ostree usroverlay</code> as an unconfined user (in the SELinux sense). I <strong>restarted the node</strong> to begin afresh:</p>
<pre class="shell"><code>sh-4.4# reboot

Removing debug pod ...</code></pre>
<h2 id="usroverlay-via-ssh"><code>usroverlay</code> via SSH <a href="#usroverlay-via-ssh" class="section">§</a></h2>
<p>For the next attempt, I logged into the worker node over SSH. The first step was to add the SSH public key to the <code>core</code> user’s <code>authorized_keys</code> file. Roberto Carratalá’s <a href="https://rcarrata.com/openshift/update-workers-ssh/">helpful blog post</a> explains how to do this. I will recap the critical bits.</p>
<p>SSH keys can be added via <code>MachineConfig</code> objects, which must also specify the machine role (e.g. <code>worker</code>). The Machine Config Operator will only add keys to the <code>core</code> user. Multiple keys can be specified, across multiple <code>MachineConfig</code> objects—all the keys in matching objects will be included.</p>
<div class="note">
<p>I don’t have direct network access to the worker node. So how could I log in over SSH? I generated a key <strong><em>in the node debug shell</em></strong>, and will log in from there!</p>
<pre class="shell"><code>sh-4.4# ssh-keygen
Generating public/private rsa key pair.
Enter file in which to save the key (/root/.ssh/id_rsa):
Created directory &#39;/root/.ssh&#39;.
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /root/.ssh/id_rsa.
Your public key has been saved in /root/.ssh/id_rsa.pub.
The key fingerprint is:
SHA256:jAmv…NMnY root@worker-a
sh-4.4# cat ~/.ssh/id_rsa.pub
ssh-rsa AAAA…4OU= root@worker-a</code></pre>
</div>
<p>The following <code>MachineConfig</code> adds the SSH key for user <code>core</code>:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> machineconfiguration.openshift.io/v1</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> MachineConfig</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> ssh-authorized-keys-worker</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">labels</span><span class="kw">:</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">machineconfiguration.openshift.io/role</span><span class="kw">:</span><span class="at"> worker</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">config</span><span class="kw">:</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">ignition</span><span class="kw">:</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">version</span><span class="kw">:</span><span class="at"> </span><span class="fl">3.2.0</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">passwd</span><span class="kw">:</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">users</span><span class="kw">:</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> core</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">sshAuthorizedKeys</span><span class="kw">:</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> ssh-rsa AAAA…40U= root@worker-a</span></span></code></pre></div>
<p>I created the <code>MachineConfig</code>:</p>
<pre class="shell"><code>% oc create -f machineconfig-ssh-worker.yaml
machineconfig.machineconfiguration.openshift.io/ssh-authorized-keys created</code></pre>
<p>In the node debug shell, I observed that Machine Config Operator applied the change after a few seconds. It did not restart the worker node. My key was added alongside a key defined in some other <code>MachineConfig</code>.</p>
<pre class="shell"><code>sh-4.4# cat /var/home/core/.ssh/authorized_keys
ssh-rsa AAAA…jjNV devenv

ssh-rsa AAAA…4OU= root@worker-a</code></pre>
<p>Now I could log in over SSH:</p>
<pre class="shell"><code>sh-4.4# ssh core@$(hostname)
The authenticity of host &#39;worker-a (10.0.128.2)&#39; can&#39;t be established.
ECDSA key fingerprint is SHA256:LUaZOleqVFunmLCp4/E1naIQ+E5BpmVp0gcsXHGacPE.
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added &#39;worker-a,10.0.128.2&#39; (ECDSA) to the list of known hosts.
Red Hat Enterprise Linux CoreOS 48.84.202106231817-0
  Part of OpenShift 4.8, RHCOS is a Kubernetes native operating system
  managed by the Machine Config Operator (`clusteroperator/machine-config`).

WARNING: Direct SSH access to machines is not recommended; instead,
make configuration changes via `machineconfig` objects:
  https://docs.openshift.com/container-platform/4.8/architecture/architecture-rhcos.html

---
[core@worker-a ~]$</code></pre>
<p>The user is unconfined and I can see the normal, read-only (<code>ro</code>) <code>/usr</code> mount (but no overlay):</p>
<pre class="shell"><code>[core@worker-a ~]$ id -Z
unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023
[core@worker-a ~]$ mount |grep &quot;on /usr&quot;
/dev/sda4 on /usr type xfs (ro,relatime,seclabel,attr2,inode64,logbufs=8,logbsize=32k,prjquota)
overlay on /usr type overlay (rw,relatime,seclabel,lowerdir=usr,upperdir=/var/tmp/ostree-unlock-ovl.KZ4V50/upper,workdir=/var/tmp/ostree-unlock-ovl.KZ4V50/work)</code></pre>
<p>I executed <code>rpm-ostree usroverlay</code> via <code>sudo</code>. After that, a read-write (<code>rw</code>) overlay filesystem is visible:</p>
<pre class="shell"><code>[core@worker-a ~]$ sudo rpm-ostree usroverlay
Development mode enabled.  A writable overlayfs is now mounted on /usr.
All changes there will be discarded on reboot.
[core@worker-a ~]$ mount |grep &quot;on /usr&quot;
/dev/sda4 on /usr type xfs (ro,relatime,seclabel,attr2,inode64,logbufs=8,logbsize=32k,prjquota)
overlay on /usr type overlay (rw,relatime,seclabel,lowerdir=usr,upperdir=/var/tmp/ostree-unlock-ovl.TCPM50/upper,workdir=/var/tmp/ostree-unlock-ovl.TCPM50/work)</code></pre>
<p>And it is indeed writable. I made a copy of the original <code>runc</code> binary, then installed my modified version:</p>
<pre class="shell"><code>[core@worker-a ~]$ sudo cp /usr/bin/runc /usr/bin/runc.orig
[core@worker-a ~]$ sudo curl -Ss -o /usr/bin/runc \
    https://ftweedal.fedorapeople.org/runc</code></pre>
<h2 id="digression-use-a-buildroot">Digression: use a buildroot <a href="#digression-use-a-buildroot" class="section">§</a></h2>
<p>The <code>runc</code> executable I installed on the previous step didn’t work. I had built it on my workstation, against a too-new version of <em>glibc</em>. The OpenShift node (which was running RHCOS 4.8, based on RHEL 8.4) was unable to link <code>runc</code>. Therefore it could not run <em>any</em> container workloads. I was able to SSH in from another node and reboot, discarding the transient change in the <code>usroverlay</code> and restoring the node to a functional state.</p>
<p>All of this is obvious in hindsight. You have to build the program for the environment in which it will be executed. In my case, it was easiest to do this via Brew or Koji. I cloned the dist-git repository (via the <code>fedpkg</code> or <code>rhpkg</code> tool), created patches and updated the <code>runc.spec</code> file. Then I built the SRPM (<code>.src.rpm</code>) and started a scratch build in Brew. After the build completed I made the resulting <code>.rpm</code> publicly available, so that it can be fetched from the OpenShift cluster.</p>
<h2 id="override-replace-via-node-debug-container"><code>override replace</code> via node debug container <a href="#override-replace-via-node-debug-container" class="section">§</a></h2>
<p>I now have my modified <code>runc</code> in an RPM package. So I can use <code>rpm-ostree override replace</code> to install it. In a debug node on the host:</p>
<pre class="shell"><code>sh-4.4# rpm-ostree override replace \
  https://ftweedal.fedorapeople.org/runc-1.0.0-98.rhaos4.8.gitcd80260.el8.x86_64.rpm
Downloading &#39;https://ftweedal.fedorapeople.org/runc-1.0.0-98.rhaos4.8.gitcd80260.el8.x86_64.rpm&#39;... done!
Checking out tree eb6dd3b... done
No enabled rpm-md repositories.
Importing rpm-md... done
Resolving dependencies... done
Applying 1 override
Processing packages... done
Running pre scripts... done
Running post scripts... done
Running posttrans scripts... done
Writing rpmdb... done
Writing OSTree commit... done
Staging deployment... done
Upgraded:
  runc 1.0.0-97.rhaos4.8.gitcd80260.el8 -&gt; 1.0.0-98.rhaos4.8.gitcd80260.el8
Run &quot;systemctl reboot&quot; to start a reboot</code></pre>
<p><code>rpm-ostree</code> downloaded the package and prepared the updated OS. Per the advice, the update is not active yet; I need to reboot:</p>
<pre class="shell"><code>sh-4.4# rpm -q runc
runc-1.0.0-97.rhaos4.8.gitcd80260.el8.x86_64
sh-4.4# systemctl reboot
sh-4.4# exit
sh-4.2# 
Removing debug pod ...</code></pre>
<p>After reboot I started a node debug container and verified that the modified version of <code>runc</code> is visible:</p>
<pre class="shell"><code>% oc debug node/worker-a
Starting pod/worker-a-debug ...
To use host binaries, run `chroot /host`
Pod IP: 10.0.128.2
If you don&#39;t see a command prompt, try pressing enter.
sh-4.2# chroot /host
sh-4.4# rpm -q runc
runc-1.0.0-98.rhaos4.8.gitcd80260.el8.x86_64</code></pre>
<p>And the fact that the debug container is working proves that the modified version of runc isn’t <em>completely</em> broken! Testing the new functionality is a topic for a different post, so I’ll leave it at that.</p>
<h3 id="listing-and-resetting-overrides">Listing and resetting overrides <a href="#listing-and-resetting-overrides" class="section">§</a></h3>
<p><code>rpm-ostree status --booted</code> lists the current base image and any overrides that have been applied:</p>
<pre class="shell"><code>sh-4.4# rpm-ostree status --booted
State: idle
BootedDeployment:
* pivot://quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9a23adde268dc8937ae293594f58fc4039b574210f320ebdac85a50ef40220dd
              CustomOrigin: Managed by machine-config-operator
                   Version: 48.84.202106231817-0 (2021-06-23T18:21:06Z)
      ReplacedBasePackages: runc 1.0.0-97.rhaos4.8.gitcd80260.el8 -&gt; 1.0.0-98.rhaos4.8.gitcd80260.el8</code></pre>
<p>To reset an override for a specific package, run <code>rpm-ostree override reset $PKG</code>:</p>
<pre class="shell"><code>sh-4.4# rpm-ostree override reset runc
Staging deployment... done
Freed: 1.1 GB (pkgcache branches: 0)
Downgraded:
  runc 1.0.0-98.rhaos4.8.gitcd80260.el8 -&gt; 1.0.0-97.rhaos4.8.gitcd80260.el8
Run &quot;systemctl reboot&quot; to start a reboot</code></pre>
<p>To reset <em>all</em> overrides, execute <code>rpm-ostree reset</code>:</p>
<pre class="shell"><code>sh-4.4# rpm-ostree reset
Staging deployment... done
Freed: 54.8 MB (pkgcache branches: 0)
Downgraded:
  runc 1.0.0-98.rhaos4.8.gitcd80260.el8 -&gt; 1.0.0-97.rhaos4.8.gitcd80260.el8
Run &quot;systemctl reboot&quot; to start a reboot</code></pre>
<h2 id="discussion">Discussion <a href="#discussion" class="section">§</a></h2>
<p>I achieved my goal of installed a modified <code>runc</code> executable on an OpenShift node. There were two approaches:</p>
<ol type="1">
<li><p><code>rpm-ostree usroverlay</code> creates a writable overlay on <code>/usr</code>. The overlay disappears at reboot, which is fine for my testing needs. This technique doesn’t work from a node debug container; you have to log in over SSH, which requires additional steps to add SSH keys.</p></li>
<li><p><code>rpm-ostree override replace</code> overrides a particular package RPM. The change takes effect after reboot and is persistent. It is easy to rollback or reset the override. This technique does not require SSH login; it works fine in a node debug container.</p></li>
</ol>
<p>Because I needed to build my package in a RHEL 8.4 / RHCOS 4.8 buildroot, I used Brew. The build artifacts are RPMs. Therefore <code>rpm-ostree override replace</code> is the most convenient option for me.</p>
<p>Both options apply changes <em>per-node</em>. After confirming with CoreOS developers, there is currently no way to roll out a package override cluster-wide or to a defined group of nodes (e.g. to <code>MachineConfigPool/worker</code> via a <code>MachineConfig</code>). So for now, you either have to apply changes/overrides on specific nodes, or build the whole <code>machine-os-content</code> image and upgrade the cluster. As a container runtime developer, my sweet spot is in a gulf between the existing options. I can tolerate this mild annoyance on the assumption that it discourages messing around in production environments.</p>
<p>In the meantime, now that I have worked out how to install my modified <code>runc</code> onto worker nodes, I will get on with testing it!</p>]]></summary>
</entry>
<entry>
    <title>systemd, cgroups and subuid ranges</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2021-06-09-systemd-cgroups-subuid.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2021-06-09-systemd-cgroups-subuid.html</id>
    <published>2021-06-09T00:00:00Z</published>
    <updated>2021-06-09T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="systemd-cgroups-and-subuid-ranges">systemd, cgroups and subuid ranges</h1>
<p>In my <a href="2021-05-27-oci-runtime-spec-runc.html">previous post</a> I experimented with <code>runc</code> as a way of understanding the behaviour of OCI runtimes. I ended up focusing on cgroup creation and the interaction between <code>runc</code> and <em>systemd</em>. The experiment revealed a critical deficiency: when using user namespaces the container’s cgroup is not owned by the user executing the container process. As a result, <em>systemd</em>-based workloads cannot run.</p>
<p><code>runc</code> creates cgroups via systemd’s <em>transient unit API</em>. Could a container runtime use this API to control the cgroup ownership? Let’s find out.</p>
<h2 id="how-runc-talks-to-systemd">How <code>runc</code> talks to <em>systemd</em> <a href="#how-runc-talks-to-systemd" class="section">§</a></h2>
<p>The <em>Open Container Initiative (OCI)</em> <a href="https://github.com/opencontainers/runtime-spec">runtime spec</a> defines a low-level container runtime interface. OCI runtimes must create the Linux namespaces specified by an OCI config, including the cgroup namespace.</p>
<p><code>runc</code> uses the systemd D-Bus API to ask systemd to create a cgroup scope for the container. Then it creates a cgroup namespace with the new cgroup scope as the root. We can see that <code>runc</code> invokes the <code>StartTransientUnit</code> API method with a name for the new unit, and a list of properties (<a href="https://github.com/opencontainers/runc/blob/v1.0.0-rc95/vendor/github.com/coreos/go-systemd/v22/dbus/methods.go#L198-L200">source code</a>):</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode go"><code class="sourceCode go"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">// .../go-systemd/v22/dbus/methods.go</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="kw">func</span> <span class="op">(</span>c <span class="op">*</span>Conn<span class="op">)</span> StartTransientUnitContext<span class="op">(</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  ctx context<span class="op">.</span>Context<span class="op">,</span> name <span class="dt">string</span><span class="op">,</span> mode <span class="dt">string</span><span class="op">,</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  properties <span class="op">[]</span>Property<span class="op">,</span> ch <span class="kw">chan</span><span class="op">&lt;-</span> <span class="dt">string</span><span class="op">)</span> <span class="op">(</span><span class="dt">int</span><span class="op">,</span> <span class="dt">error</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  <span class="kw">return</span> c<span class="op">.</span>startJob<span class="op">(</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    ctx<span class="op">,</span> ch<span class="op">,</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;org.freedesktop.systemd1.Manager.StartTransientUnit&quot;</span><span class="op">,</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    name<span class="op">,</span> mode<span class="op">,</span> properties<span class="op">,</span> <span class="bu">make</span><span class="op">([]</span>PropertyCollection<span class="op">,</span> <span class="dv">0</span><span class="op">))</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>Most of the unit configuration is passed as properties.</p>
<h2 id="the-user-property">The <code>User=</code> property <a href="#the-user-property" class="section">§</a></h2>
<p><a href="https://www.freedesktop.org/software/systemd/man/systemd.exec.html#User="><code>systemd.exec(5)</code></a> describes the properties that configure a systemd unit (including transient units). Among the properties are <code>User=</code> and <code>Group=</code>:</p>
<blockquote>
<p>Set the UNIX user or group that the processes are executed as, respectively. Takes a single user or group name, or a numeric ID as argument.</p>
</blockquote>
<p>This sounds promising. Further searching turned up a systemd documentation page entitled <a href="https://systemd.io/CGROUP_DELEGATION/">Control Group APIs and Delegation</a>. That document states:</p>
<blockquote>
<p>By turning on the <code>Delegate=</code> property for a scope or service you get a few guarantees: … If your service makes use of the <code>User=</code> functionality, then the sub-tree will be <code>chown()</code>ed to the indicated user so that it can correctly create cgroups below it.</p>
</blockquote>
<p><code>runc</code> already supplies <code>Delegate=true</code>. The <code>User=</code> property seems to be exactly what we need.</p>
<h2 id="determining-the-uid">Determining the UID <a href="#determining-the-uid" class="section">§</a></h2>
<p>The OCI configuration specifies the <a href="https://github.com/opencontainers/runtime-spec/blob/master/config.md#posix-platform-user"><code>user</code></a> that will execute the container process (in the <strong>container’s user namespace</strong>). It also specifies <a href=""><code>uidMappings</code></a> between the host and container user namespaces. For example:</p>
<pre class="shell"><code>% jq -c &#39;.process.user, .linux.uidMappings&#39; &lt; config.json
{&quot;uid&quot;:0,&quot;gid&quot;:0}
[{&quot;containerID&quot;:0,&quot;hostID&quot;:100000,&quot;size&quot;:65536}]</code></pre>
<p><code>runc</code> has all the data it needs to compute the appropriate value for the <code>User=</code> property. The algorithm, expressed as Python is:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>uid <span class="op">=</span> config[<span class="st">&quot;process&quot;</span>][<span class="st">&quot;user&quot;</span>][<span class="st">&quot;uid&quot;</span>]</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="bu">map</span> <span class="kw">in</span> config[<span class="st">&quot;linux&quot;</span>][<span class="st">&quot;uidMappings&quot;</span>]:</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    uid_min <span class="op">=</span> <span class="bu">map</span>[<span class="st">&quot;containerID&quot;</span>]</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    uid_max <span class="op">=</span> map_min <span class="op">+</span> <span class="bu">map</span>[<span class="st">&quot;size&quot;</span>] <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> uid_min <span class="op">&lt;=</span> uid <span class="op">&lt;=</span> uid_max:</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        offset <span class="op">=</span> uid <span class="op">-</span> uid_min</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">map</span>[<span class="st">&quot;hostID&quot;</span>] <span class="op">+</span> offset</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">raise</span> <span class="pp">RuntimeError</span>(<span class="st">&quot;user.uid is not mapped&quot;</span>)</span></code></pre></div>
<h2 id="testing-with-systemd-run">Testing with <code>systemd-run</code> <a href="#testing-with-systemd-run" class="section">§</a></h2>
<p><code>systemd-run(1)</code> uses the transient unit API to run programs via transient scope or service units. You can use the <code>--property</code>/<code>-p</code> option to pass additional properties. I used <code>systemd-run</code> to observe how systemd handles the <code>Delegate=true</code> and <code>User=</code> properties.</p>
<h3 id="create-and-inspect-transient-unit">Create and inspect transient unit <a href="#create-and-inspect-transient-unit" class="section">§</a></h3>
<p>First I will do a basic test, talking to my user account’s service manager:</p>
<pre class="shell"><code>% id -u
1000

% systemd-run --user sleep 300
Running as unit: run-r8e3c22d2bb64491a85882d8303202dca.service

% systemctl --user status run-r8e3c22d2bb64491a85882d8303202dca.service
● run-r8e3c22d2bb64491a85882d8303202dca.service - /bin/sleep 300
     Loaded: loaded (/run/user/1000/systemd/transient/run-r8e3c22d2bb64491a85882d8303202dca.service; transient)
  Transient: yes
     Active: active (running) since Wed 2021-06-09 11:31:14 AEST; 9s ago
   Main PID: 11412 (sleep)
      Tasks: 1 (limit: 2325)
     Memory: 184.0K
        CPU: 3ms
     CGroup: /user.slice/user-1000.slice/user@1000.service/app.slice/run-r8e3c22d2bb64491a85882d8303202dca.service
             └─11412 /bin/sleep 300

Jun 09 11:31:14 f33-1.ipa.local systemd[863]: Started /bin/sleep 300.

% ls -nld /sys/fs/cgroup/user.slice/user-1000.slice/user@1000.service/app.slice/run-r8e3c22d2bb64491a85882d8303202dca.service
drwxr-xr-x. 2 1000 1000 0 Jun  9 11:31 /sys/fs/cgroup/user.slice/user-1000.slice/user@1000.service/app.slice/run-r8e3c22d2bb64491a85882d8303202dca.service</code></pre>
<p>We can see that:</p>
<ul>
<li>systemd-run creates the transient unit</li>
<li>the unit was started successfully, and is running</li>
<li>the unit has is own <code>CGroup</code></li>
<li>the cgroup is owned by user <code>1000</code></li>
</ul>
<p>As I try different ways of invoking <code>systemd-run</code>, I will repeat this pattern of unit creation, inspection and cgroup ownership checks.</p>
<h3 id="specify-user-user-service-manager">Specify <code>User=</code> (user service manager) <a href="#specify-user-user-service-manager" class="section">§</a></h3>
<p>Next I explicity specify <code>User=1000</code>:</p>
<pre class="shell"><code>% systemd-run --user -p User=1000 sleep 300
Running as unit: run-r651ff7d0d1214037b70def6d5694dcd6.service

% systemctl --no-pager --full --user status run-r651ff7d0d1214037b70def6d5694dcd6.service
× run-r651ff7d0d1214037b70def6d5694dcd6.service - /bin/sleep 300
     Loaded: loaded (/run/user/1000/systemd/transient/run-r651ff7d0d1214037b70def6d5694dcd6.service; transient)
  Transient: yes
     Active: failed (Result: exit-code) since Wed 2021-06-09 11:38:50 AEST; 1min 17s ago
    Process: 11432 ExecStart=/bin/sleep 300 (code=exited, status=216/GROUP)
   Main PID: 11432 (code=exited, status=216/GROUP)
        CPU: 4ms

Jun 09 11:38:50 f33-1.ipa.local systemd[863]: Started /bin/sleep 300.
Jun 09 11:38:50 f33-1.ipa.local systemd[11432]: run-r651ff7d0d1214037b70def6d5694dcd6.service: Failed to determine supplementary groups: Operation not permitted
Jun 09 11:38:50 f33-1.ipa.local systemd[11432]: run-r651ff7d0d1214037b70def6d5694dcd6.service: Failed at step GROUP spawning /bin/sleep: Operation not permitted
Jun 09 11:38:50 f33-1.ipa.local systemd[863]: run-r651ff7d0d1214037b70def6d5694dcd6.service: Main process exited, code=exited, status=216/GROUP
Jun 09 11:38:50 f33-1.ipa.local systemd[863]: run-r651ff7d0d1214037b70def6d5694dcd6.service: Failed with result &#39;exit-code&#39;.</code></pre>
<p>This unit failed to execute, because the user service manager does not have permission to determine supplementary groups. Without going into too much detail, this is because the user systemd instance lacks the <code>CAP_SETGID</code> capability required by the <code>setgroups(2)</code> system call used by <code>initgroups(3)</code>.</p>
<p>There doesn’t seem to be a way around this. For the rest of my testing I’ll talk to the system service manager. That’s okay, because <code>runc</code> on OpenShift also talks to the system service manager.</p>
<h3 id="specify-user-system-service-manager">Specify <code>User=</code> (system service manager) <a href="#specify-user-system-service-manager" class="section">§</a></h3>
<pre class="shell"><code>% sudo systemd-run -p User=1000 sleep 300
Running as unit: run-r94725453119e4003af336d7294984085.service

% systemctl status run-r94725453119e4003af336d7294984085.service
● run-r94725453119e4003af336d7294984085.service - /usr/bin/sleep 300
     Loaded: loaded (/run/systemd/transient/run-r94725453119e4003af336d7294984085.service; transient)
  Transient: yes
     Active: active (running) since Wed 2021-06-09 11:50:10 AEST; 11s ago
   Main PID: 11517 (sleep)
      Tasks: 1 (limit: 2325)
     Memory: 184.0K
        CPU: 4ms
     CGroup: /system.slice/run-r94725453119e4003af336d7294984085.service
             └─11517 /usr/bin/sleep 300

Jun 09 11:50:10 f33-1.ipa.local systemd[1]: Started /usr/bin/sleep 300.

% ls -nld /sys/fs/cgroup/system.slice/run-r94725453119e4003af336d7294984085.service
drwxr-xr-x. 2 0 0 0 Jun  9 11:50 /sys/fs/cgroup/system.slice/run-r94725453119e4003af336d7294984085.service

% ps -o uid,pid,cmd --pid 11517
  UID     PID CMD
 1000   11517 /usr/bin/sleep 300</code></pre>
<p>The process is running as user <code>1000</code>, but the cgroup is owned by <code>root</code>.</p>
<h3 id="specify-delegatetrue">Specify <code>Delegate=true</code> <a href="#specify-delegatetrue" class="section">§</a></h3>
<p>We need to specify <code>Delegate=true</code> to tell systemd to delegate the cgroup to the specified <code>User</code>:</p>
<pre class="shell"><code>% sudo systemd-run -p Delegate=true -p User=1000 sleep 300
Running as unit: run-r518dbc963502423c9c67b1c72d3d4c12.service

% systemctl status run-r518dbc963502423c9c67b1c72d3d4c12.service
● run-r518dbc963502423c9c67b1c72d3d4c12.service - /usr/bin/sleep 300
     Loaded: loaded (/run/systemd/transient/run-r518dbc963502423c9c67b1c72d3d4c12.service; transient)
  Transient: yes
     Active: active (running) since Wed 2021-06-09 11:59:34 AEST; 1min 21s ago
   Main PID: 11579 (sleep)
      Tasks: 1 (limit: 2325)
     Memory: 184.0K
        CPU: 3ms
     CGroup: /system.slice/run-r518dbc963502423c9c67b1c72d3d4c12.service
             └─11579 /usr/bin/sleep 300

Jun 09 11:59:34 f33-1.ipa.local systemd[1]: Started /usr/bin/sleep 300.

% ls -nld /sys/fs/cgroup/system.slice/run-r518dbc963502423c9c67b1c72d3d4c12.service
drwxr-xr-x. 2 1000 1000 0 Jun  9 11:59 /sys/fs/cgroup/system.slice/run-r518dbc963502423c9c67b1c72d3d4c12.service</code></pre>
<p>systemd <code>chown()</code>ed the cgroup to the specified <code>User</code>. Note that very few of the cgroup controls in the cgroup directory are writable by user <code>1000</code>:</p>
<pre class="shell"><code>% ls -nl /sys/fs/cgroup/system.slice/run-r518dbc963502423c9c67b1c72d3d4c12.service \
    |grep 1000 
-rw-r--r--. 1 1000 1000 0 Jun  9 11:59 cgroup.procs
-rw-r--r--. 1 1000 1000 0 Jun  9 11:59 cgroup.subtree_control
-rw-r--r--. 1 1000 1000 0 Jun  9 11:59 cgroup.threads</code></pre>
<p>So the process cannot adjust its root cgroup’s <code>memory.max</code>, <code>pids.max</code>, <code>cpu.weight</code> and so on. It <em>can</em> create cgroup subtrees, manage resources within them, and move processes and threads among those subtrees and its root cgroup.</p>
<h3 id="arbitrary-uids">Arbitrary UIDs <a href="#arbitrary-uids" class="section">§</a></h3>
<p>So far I have specified <code>User=1000</code>. User <code>1000</code> is a “known user”. That is, the Name Service Switch (see <code>nss(5)</code>) returns information about the user (name, home directory, shell, etc):</p>
<pre class="shell"><code>% getent passwd $(id -u)
ftweedal:x:1000:1000:ftweedal:/home/ftweedal:/bin/zsh</code></pre>
<p>However, when executing containers with user namespaces, we usually map the namespace UIDs to unprivileged host UIDs from a <em>subordinate ID</em> range. Subordinate UIDs and GID ranges are currently defined in <code>/etc/subuid</code> and <code>/etc/subgid</code> respectively. The subuid range for user <code>1000</code> is:</p>
<pre class="shell"><code>% grep $(id -un) /etc/subuid
ftweedal:100000:65536</code></pre>
<p>User <code>1000</code> has been allocated the range <code>100000</code>–<code>165535</code>. So let’s try <code>systemd-run</code> with <code>User=100000</code>:</p>
<pre class="shell"><code>% sudo systemd-run -p Delegate=true -p User=100000 sleep 300
Running as unit: run-r1498304af7df406c9698da5c683ea79e.service

% systemctl --no-pager --full status run-r1498304af7df406c9698da5c683ea79e.service
× run-r1498304af7df406c9698da5c683ea79e.service - /usr/bin/sleep 300
     Loaded: loaded (/run/systemd/transient/run-r1498304af7df406c9698da5c683ea79e.service; transient)
  Transient: yes
     Active: failed (Result: exit-code) since Wed 2021-06-09 12:32:43 AEST; 14s ago
    Process: 11766 ExecStart=/usr/bin/sleep 300 (code=exited, status=217/USER)
   Main PID: 11766 (code=exited, status=217/USER)
        CPU: 2ms

Jun 09 12:32:43 f33-1.ipa.local systemd[1]: Started /usr/bin/sleep 300.
Jun 09 12:32:43 f33-1.ipa.local systemd[11766]: run-r1498304af7df406c9698da5c683ea79e.service: Failed to determine user credentials: No such process
Jun 09 12:32:43 f33-1.ipa.local systemd[11766]: run-r1498304af7df406c9698da5c683ea79e.service: Failed at step USER spawning /usr/bin/sleep: No such process
Jun 09 12:32:43 f33-1.ipa.local systemd[1]: run-r1498304af7df406c9698da5c683ea79e.service: Main process exited, code=exited, status=217/USER
Jun 09 12:32:43 f33-1.ipa.local systemd[1]: run-r1498304af7df406c9698da5c683ea79e.service: Failed with result &#39;exit-code&#39;.</code></pre>
<p>It failed. Cutting the noise, the cause is:</p>
<pre><code>Failed to determine user credentials: No such process</code></pre>
<p>The string <code>No such process</code> is a bit misleading. It is the string associated with the <code>ESRCH</code> error value (see <code>errno(3)</code>). Here it indicates that <code>getpwuid(3)</code> did not find a user record for uid <code>100000</code>. systemd unconditionally fails in this scenario. And this is a problem for us because without intervention, subordinate UIDs do not have associated user records.</p>
<h3 id="arbitrary-uids-with-passwd-entry">Arbitrary UIDs (with <code>passwd</code> entry) <a href="#arbitrary-uids-with-passwd-entry" class="section">§</a></h3>
<p>So let’s make NSS return something for user <code>100000</code>. There are several ways we could do this, including adding it to <code>/etc/passwd</code>, or creating an NSS module that generates passwd records for ranges declared in <code>/etc/subuid</code>.</p>
<p>Another way is to use <a href="https://www.freedesktop.org/software/systemd/man/nss-systemd.html">systemd’s NSS module</a>, which returns passwd records for containers created by <a href="https://www.freedesktop.org/software/systemd/man/systemd-machined.html"><code>systemd-machined</code></a>. And that’s what I did. Given the root filesystem for a container in <code>./rootfs</code>, <a href="https://www.freedesktop.org/software/systemd/man/systemd-nspawn.html"><code>systemd-nspawn</code></a> creates the container. The <code>--private-users=100000</code> option tells it to create a user namespace mapping to the host UID <code>100000</code> with default size 65536:</p>
<pre class="shell"><code>% sudo systemd-nspawn --directory rootfs --private-users=100000 /bin/sh
Spawning container rootfs on /home/ftweedal/go/src/github.com/opencontainers/runc/rootfs.
Press ^] three times within 1s to kill container.
Selected user namespace base 100000 and range 65536.
sh-5.0#</code></pre>
<p>On the host we can see the “machine” via <a href="https://www.freedesktop.org/software/systemd/man/machinectl.html"><code>machinectl(1)</code></a>. We also observe that NSS now returns results for UIDs in the mapped host range.</p>
<pre class="shell"><code>% getent passwd 100000 165535  
vu-rootfs-0:x:100000:65534:UID 0 of Container rootfs:/:/usr/sbin/nologin

% getent passwd 100000 165534
vu-rootfs-0:x:100000:65534:UID 0 of Container rootfs:/:/usr/sbin/nologin
vu-rootfs-65534:x:165534:65534:UID 65534 of Container rootfs:/:/usr/sbin/nologin</code></pre>
<p>The <code>passwd</code> records are constructed on demand by <a href="https://www.freedesktop.org/software/systemd/man/nss-systemd.html"><code>nss-systemd(8)</code></a> using data registered by <code>systemd-machined</code>.</p>
<p>Now let’s try <code>systemd-run</code> again:</p>
<pre class="shell"><code>% sudo systemd-run -p Delegate=true -p User=100000 sleep 300
Running as unit: run-r076a82c36fcd4934b13bba47fcc8462e.service

% systemctl status run-r076a82c36fcd4934b13bba47fcc8462e.service
● run-r076a82c36fcd4934b13bba47fcc8462e.service - /usr/bin/sleep 300
     Loaded: loaded (/run/systemd/transient/run-r076a82c36fcd4934b13bba47fcc8462e.service; transient)
  Transient: yes
     Active: active (running) since Wed 2021-06-09 14:14:34 AEST; 11s ago
   Main PID: 12045 (sleep)
      Tasks: 1 (limit: 2325)
     Memory: 180.0K
        CPU: 4ms
     CGroup: /system.slice/run-r076a82c36fcd4934b13bba47fcc8462e.service
             └─12045 /usr/bin/sleep 300

Jun 09 14:14:34 f33-1.ipa.local systemd[1]: Started /usr/bin/sleep 300.

% ls -nld /sys/fs/cgroup/system.slice/run-r076a82c36fcd4934b13bba47fcc8462e.service 
drwxr-xr-x. 2 100000 65534 0 Jun  9 14:14 /sys/fs/cgroup/system.slice/run-r076a82c36fcd4934b13bba47fcc8462e.service

% ps -o uid,gid,pid,cmd --pid 12045
  UID   GID     PID CMD
  100000 65534  12045 /usr/bin/sleep 300

% id -un 65534
nobody</code></pre>
<p>Now the cgroup is owned by <code>100000</code>. But the group ID (<code>gid</code>) under which the process runs, and the group owner of the cgroup, is <code>65534</code>. This is the host’s <code>nobody</code> account.</p>
<h3 id="specify-group">Specify <code>Group=</code> <a href="#specify-group" class="section">§</a></h3>
<p>In a user-namespaced container, ordinarily you would want both the user <em>and</em> the group of the container process to be mapped into the user namespace. Likewise, you would expect the cgroup to be owned by a known (in the namespace) user. Setting the <code>Group=</code> property should achieve this.</p>
<pre class="shell"><code>% sudo systemd-run -p Delegate=true -p User=100000 -p Group=100000 sleep 300      
Running as unit: run-re610d14cc0584a37a3d4099268df75d8.service

% systemctl status run-re610d14cc0584a37a3d4099268df75d8.service
● run-re610d14cc0584a37a3d4099268df75d8.service - /usr/bin/sleep 300
     Loaded: loaded (/run/systemd/transient/run-re610d14cc0584a37a3d4099268df75d8.service; transient)
  Transient: yes
     Active: active (running) since Wed 2021-06-09 14:24:58 AEST; 7s ago
   Main PID: 12131 (sleep)
      Tasks: 1 (limit: 2325)
     Memory: 184.0K
        CPU: 5ms
     CGroup: /system.slice/run-re610d14cc0584a37a3d4099268df75d8.service
             └─12131 /usr/bin/sleep 300

Jun 09 14:24:58 f33-1.ipa.local systemd[1]: Started /usr/bin/sleep 300.

% ls -nld /sys/fs/cgroup/system.slice/run-re610d14cc0584a37a3d4099268df75d8.service
drwxr-xr-x. 2 100000 100000 0 Jun  9 14:24 /sys/fs/cgroup/system.slice/run-re610d14cc0584a37a3d4099268df75d8.service

% ps -o uid,gid,pid,cmd --pid 12131
  UID   GID     PID CMD
100000 100000 12131 /usr/bin/sleep 300</code></pre>
<p>Finally, systemd is exhibiting the behaviour we desire.</p>
<h2 id="discussion-and-next-steps">Discussion and next steps <a href="#discussion-and-next-steps" class="section">§</a></h2>
<p>In summary, the findings from this investigation are:</p>
<ul>
<li><p>systemd changes the cgroup ownership of transient units according to the <code>User=</code> and <code>Group=</code> properties, if and only if <code>Delegate=true</code>.</p></li>
<li><p>systemd currently requires <code>User=</code> and <code>Group=</code> to refer to known (via NSS) users and groups.</p></li>
<li><p>Unprivileged user systemd service manager instances lack the privileges to set supplementary groups for the container process. This is not a problem for the OpenShift use case, because it uses the system service manager.</p></li>
</ul>
<p>As to the second point, I am curious why systemd behaves this way. It does makes sense to query NSS to find out the shell, home directory, and login name for setting up the execution environment. But if there is no <code>passwd</code> record, why not synthesise one with conservative defaults? Running processes as anonymous UIDs has a valid use case—increasingly so, as adoption of user namespaces increases. I <a href="https://github.com/systemd/systemd/issues/19781">filed an RFE (systemd#19781)</a> against systemd to suggest relaxing this restriction, and inquire whether this is a Bad Idea for some reason I don’t yet understand.</p>
<p>There are some alternative approaches that don’t require changing systemd:</p>
<ul>
<li><p>Use <code>systemd-machined</code> to register a machine. It provides the <code>org.freedesktop.machine1.Manager.RegisterMachine</code> D-Bus method for this purpose. But <code>systemd-machined</code> is not used (or even present) on OpenShift cluster nodes.</p></li>
<li><p>Implement, ship and configure an NSS module that synthesises <code>passwd</code> records for user subordinate ID ranges. The <em>shadow</em> project has <a href="https://github.com/shadow-maint/shadow/pull/321">defined an NSS interface</a> for subid ranges. <em>libsubid</em>, part of <em>shadow</em>, will provide abstract subid range lookups (forward and reverse). So a <em>libsubid</em>-based solution to this should be possible. Unfortunately, <em>libsubid</em> is not yet widely available as a shared library.</p>
<p>As an example, synthetic user records could have a username like <code>subuid-{username}-{uid}</code>. The home directory and shell would be <code>/</code> and <code>/sbin/nologin</code>, like the records synthesised by <code>nss-systemd</code>.</p></li>
<li><p>Update the container runtime (<code>runc</code>) to <code>chown</code> the cgroup <em>after systemd creates it</em>. In fact, this is what <code>systemd-nspawn</code> does. This approach is nice because the only component to change is <code>runc</code>—which had to change anyway, to add the logic to determine the cgroup owner UID. To the best of my knowledge, on OpenShift <code>runc</code> gets executed as <code>root</code> (on the node), so it should have the permissions required to do this. Unless SELinux prevents it.</p></li>
</ul>
<p>Of these three options, modifying <code>runc</code> to <code>chown</code> the cgroup directory seems the most promising. While I wait for feedback on <a href="https://github.com/systemd/systemd/issues/19781">systemd#19781</a>, I will start hacking on <code>runc</code> and testing my modifications.</p>]]></summary>
</entry>
<entry>
    <title>Using runc to explore the OCI Runtime Specification</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2021-05-27-oci-runtime-spec-runc.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2021-05-27-oci-runtime-spec-runc.html</id>
    <published>2021-05-27T00:00:00Z</published>
    <updated>2021-05-27T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="using-runc-to-explore-the-oci-runtime-specification">Using <code>runc</code> to explore the OCI Runtime Specification</h1>
<p>In recent posts I explored how to use user namespaces and cgroups v2 on OpenShift. My main objective is to run <em>systemd</em>-based workloads in user namespaces that map to unprivileged users on the host. This is a prerequisite to running <a href="https://www.freeipa.org/page/Main_Page">FreeIPA</a> <em>securely</em> in OpenShift, and supporting multitenancy.</p>
<p>Independently, user namespaces and cgroups v2 already work well in OpenShift. But for <em>systemd</em> support there is a critical gap: the pod’s cgroup directory (mounted as <code>/sys/fs/cgroup/</code> in the container) is owned by <code>root</code>—the <em>host’s</em> UID 0, which is unmapped in the pod’s user namespace. As a consequence, the container’s main process (<code>/sbin/init</code>, which is <em>systemd</em>) cannot manage cgroups, and terminates.</p>
<p>To understand how to close this gap, I needed to become familiar with the low-level container runtime behaviour. This post discusses the relationship between various container runtime components and demonstrates how to use <code>runc</code> directly to create and run containers. I also outline some possible approaches to solving the cgroup ownership issue.</p>
<h2 id="podman-kubernetes-cri-cri-o-runc-oh-my">Podman, Kubernetes, CRI, CRI-O, runc, oh my! <a href="#podman-kubernetes-cri-cri-o-runc-oh-my" class="section">§</a></h2>
<p>What actually happens when you “run a container”. Abstractly, a container runtime sets up a <em>sandbox</em> and runs a process in it. The sandbox consists of a set of namespaces (PID, UTS, mount, cgroup, user, network, etc), and a restricted view of a filesystem (via <code>chroot(2)</code> or similar mechanism).</p>
<p>There are several different container runtimes in widespread use. In fact, there are several different <em>layers</em> of container runtime with different purposes:</p>
<ul>
<li><p>End-user focused container runtimes include <a href="https://podman.io/"><em>Podman</em></a> and <em>Docker</em>.</p></li>
<li><p>Kubernetes defines the <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md">Container Runtime Interface (CRI)</a>, which it uses to run containers. Compliant implementations include <em>containerd</em> and <a href="https://github.com/cri-o/cri-o"><em>CRI-O</em></a>.</p></li>
<li><p>The <em>Open Container Initiative (OCI)</em> <a href="https://github.com/opencontainers/runtime-spec">runtime spec</a> defines a low-level container runtime interface. Implementations include <a href="https://github.com/opencontainers/runc"><code>runc</code></a> and <a href="https://github.com/containers/crun"><em>crun</em></a>. OCI runtimes are designed to be used by higher-level container runtimes. They are not friendly for humans to use directly.</p></li>
</ul>
<p>Running a container usually involves a higher-level runtime <em>and</em> a low-level runtime. For example, Podman uses an OCI runtime; crun by default on Fedora but <code>runc</code> works fine too. OpenShift (which is built on Kubernetes) uses CRI-O, which in turn uses <code>runc</code> (CRI-O itself can use any OCI runtime).</p>
<h3 id="division-of-responsibilities">Division of responsibilities <a href="#division-of-responsibilities" class="section">§</a></h3>
<p>So, what are responsibilities of the higher-level runtime compared to the OCI (or other low-level) runtime? In general the high-level runtime is responsible for:</p>
<ul>
<li><p>Image management (pulling layers, preparing overlay filesystem)</p></li>
<li><p>Determining the mounts, environment, namespaces, resource limits and security policies for the container</p></li>
<li><p>Network setup for the container</p></li>
<li><p>Metrics, accounting, etc.</p></li>
</ul>
<p>The steps performed by the low-level runtime include:</p>
<ul>
<li><p>Create and and enter required namespaces</p></li>
<li><p><code>chroot(2)</code> or <code>pivot_root(2)</code> to the specified root filesystem path</p></li>
<li><p>Create requested mounts</p></li>
<li><p>Create cgroups and apply resource limits</p></li>
<li><p>Adjust capabilities and apply seccomp policy</p></li>
<li><p>Execute the container’s main process</p></li>
</ul>
<div class="note">
<p>I mentioned several features specific to Linux in the list above. The OCI Runtime Specification also specifies Windows, Solaris and VM-based workloads. This post assumes a Linux workload, so many details are Linux-specific.</p>
</div>
<p>The above list is just a rough guide and not absolute. Depending on use case the high-level runtime might perform some of the low-level steps. For example, if container networking is required, Podman might create the network namespace, setting up devices and routing. Then, instead of asking the OCI runtime to create a network namespace, it tells the runtime to enter the existing namespace.</p>
<h2 id="running-containers-via-runc">Running containers via <code>runc</code> <a href="#running-containers-via-runc" class="section">§</a></h2>
<p>Because our effort is targeting OpenShift, the rest of this post mainly deals with <code>runc</code>.</p>
<div class="note">
<p>The functions demonstrated in this post were performed using <code>runc</code> version 1.0.0-rc95+dev, which I built from source (commit <code>19d75e1c</code>). The Fedora 33 and 34 repositories offer <code>runc</code> version 1.0.0-rc93, which <strong>does not work</strong>.</p>
</div>
<h3 id="clone-and-build">Clone and build <a href="#clone-and-build" class="section">§</a></h3>
<p>Install the Go compiler and <em>libseccomp</em> development headers:</p>
<pre class="shell"><code>% sudo dnf -y --quiet install libseccomp-devel

Installed:
  golang-1.16.3-1.fc34.x86_64
  golang-bin-1.16.3-1.fc34.x86_64
  golang-src-1.16.3-1.fc34.noarch
  libseccomp-devel-2.5.0-4.fc34.x86_64</code></pre>
<p>Clone the <code>runc</code> source code and build the program:</p>
<pre class="shell"><code>% mkdir -p ~/go/src/github.com/opencontainers
% cd ~/go/src/github.com/opencontainers
% git clone --quiet https://github.com/opencontainers/runc
% cd runc
% make --quiet
% ./runc --version
runc version 1.0.0-rc95+dev
commit: v1.0.0-rc95-31-g19d75e1c
spec: 1.0.2-dev
go: go1.16.3
libseccomp: 2.5.0</code></pre>
<h3 id="prepare-root-filesystem">Prepare root filesystem <a href="#prepare-root-filesystem" class="section">§</a></h3>
<p>I want to create a filesystem from my <em>systemd</em> based <a href="https://quay.io/repository/ftweedal/test-nginx"><code>test-nginx</code></a> container image. To avoid configuring overlay filesystems myself, I used Podman to create a container, then exported the whole container filesystem, via <code>tar(1)</code>, to a local directory:</p>
<pre class="shell"><code>% podman create --quiet quay.io/ftweedal/test-nginx
e97930b3…
% mkdir rootfs
% podman export e97930b3 | tar -xC rootfs
% ls rootfs
bin  dev home lib64      media opt  root sbin sys usr
boot etc lib  lost+found mnt   proc run  srv  tmp var</code></pre>
<h3 id="create-config.json">Create <code>config.json</code> <a href="#create-config.json" class="section">§</a></h3>
<p>OCI runtimes read the container configuration from <code>config.json</code> in the <em>bundle</em> directory. (<code>runc</code> uses the current directory as the default bundle directory). The <code>runc spec</code> command generates a sample <code>config.json</code> which can serve as a starting point:</p>
<pre class="shell"><code>% ./runc spec --rootless
% file config.json
config.json: JSON data
% jq -c .process.args &lt; config.json
[&quot;sh&quot;]</code></pre>
<p>We can see that <code>runc</code> created the sample config. The command to execute is <code>sh(1)</code>. Let’s change that to <code>/sbin/init</code>:</p>
<pre class="shell"><code>% mv config.json config.json.orig
% jq &#39;.process.args=[&quot;/sbin/init&quot;]&#39; config.json.orig \
    &gt; config.json</code></pre>
<div class="notes">
<p><code>jq(1)</code> cannot operate on JSON files in situ, so you first have to copy or move the input file. The <a href="https://linux.die.net/man/1/sponge"><code>sponge(1)</code></a> command, provided by the <em>moreutils</em> package, offers an alternative approach.</p>
</div>
<h3 id="run-container">Run container <a href="#run-container" class="section">§</a></h3>
<p>Now we can try and run the container:</p>
<pre class="shell"><code>% ./runc --systemd-cgroup run test
Mount failed for selinuxfs on /sys/fs/selinux:  No such file or directory
Another IMA custom policy has already been loaded, ignoring: Permission denied
Failed to mount tmpfs at /run: Operation not permitted
[!!!!!!] Failed to mount API filesystems.
Freezing execution.</code></pre>
<p>That didn’t work. systemd failed to mount a <code>tmpfs</code> (temporary, memory-based filesystem) at <code>/tmp</code>, and halted. The container itself was still running (but frozen). I was able to kill it from another terminal:</p>
<pre class="shell"><code>% ./runc list --quiet
test
% ./runc kill test KILL
% ./runc list --quiet</code></pre>
<p>It turned out that in addition to the process to run, the config requires several changes to successfully run a <em>systemd</em>-based container. I will not repeat the whole process here, but I achieved a working config through a combination of trial-and-error, and comparison against OCI configurations produced by Podman. The following <a href="https://stedolan.github.io/jq/manual/"><code>jq(1)</code></a> program performs the required modifications:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode numberSource json numberLines"><code class="sourceCode json"><span id="cb8-1"><a href="#cb8-1"></a><span class="er">.process.args</span> <span class="er">=</span> <span class="ot">[</span><span class="st">&quot;/sbin/init&quot;</span><span class="ot">]</span></span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="er">|</span> <span class="er">.process.env</span> <span class="er">|=</span> <span class="er">.</span> <span class="er">+</span> <span class="ot">[</span><span class="st">&quot;container=oci&quot;</span><span class="ot">]</span></span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="er">|</span> <span class="ot">[</span><span class="fu">{</span><span class="dt">&quot;containerID&quot;</span><span class="fu">:</span><span class="dv">1</span><span class="fu">,</span><span class="dt">&quot;hostID&quot;</span><span class="fu">:</span><span class="dv">100000</span><span class="fu">,</span><span class="dt">&quot;size&quot;</span><span class="fu">:</span><span class="dv">65536</span><span class="fu">}</span><span class="ot">]</span> <span class="er">as</span> <span class="er">$idmap</span></span>
<span id="cb8-4"><a href="#cb8-4"></a><span class="er">|</span> <span class="er">.linux.uidMappings</span> <span class="er">|=</span> <span class="er">.</span> <span class="er">+</span> <span class="er">$idmap</span></span>
<span id="cb8-5"><a href="#cb8-5"></a><span class="er">|</span> <span class="er">.linux.gidMappings</span> <span class="er">|=</span> <span class="er">.</span> <span class="er">+</span> <span class="er">$idmap</span></span>
<span id="cb8-6"><a href="#cb8-6"></a><span class="er">|</span> <span class="er">.linux.cgroupsPath</span> <span class="er">=</span> <span class="er">&quot;user.slice:runc:test&quot;</span></span>
<span id="cb8-7"><a href="#cb8-7"></a><span class="er">|</span> <span class="er">.linux.namespaces</span> <span class="er">|=</span> <span class="er">.</span> <span class="er">+</span> <span class="ot">[</span><span class="fu">{</span><span class="dt">&quot;type&quot;</span><span class="fu">:</span><span class="st">&quot;network&quot;</span><span class="fu">}</span><span class="ot">]</span></span>
<span id="cb8-8"><a href="#cb8-8"></a><span class="er">|</span> <span class="er">.process.capabilities</span><span class="ot">[]</span> <span class="er">=</span></span>
<span id="cb8-9"><a href="#cb8-9"></a>  <span class="ot">[</span> <span class="st">&quot;CAP_CHOWN&quot;</span><span class="ot">,</span> <span class="st">&quot;CAP_FOWNER&quot;</span><span class="ot">,</span> <span class="st">&quot;CAP_SETUID&quot;</span><span class="ot">,</span> <span class="st">&quot;CAP_SETGID&quot;</span><span class="ot">,</span></span>
<span id="cb8-10"><a href="#cb8-10"></a>    <span class="st">&quot;CAP_SETPCAP&quot;</span><span class="ot">,</span> <span class="st">&quot;CAP_NET_BIND_SERVICE&quot;</span> <span class="ot">]</span></span>
<span id="cb8-11"><a href="#cb8-11"></a><span class="er">|</span> <span class="fu">{</span><span class="dt">&quot;type&quot;</span><span class="fu">:</span> <span class="st">&quot;tmpfs&quot;</span><span class="fu">,</span></span>
<span id="cb8-12"><a href="#cb8-12"></a>   <span class="dt">&quot;source&quot;</span><span class="fu">:</span> <span class="st">&quot;tmpfs&quot;</span><span class="fu">,</span></span>
<span id="cb8-13"><a href="#cb8-13"></a>   <span class="dt">&quot;options&quot;</span><span class="fu">:</span> <span class="ot">[</span><span class="st">&quot;rw&quot;</span><span class="ot">,</span><span class="st">&quot;rprivate&quot;</span><span class="ot">,</span><span class="st">&quot;nosuid&quot;</span><span class="ot">,</span><span class="st">&quot;nodev&quot;</span><span class="ot">,</span><span class="st">&quot;tmpcopyup&quot;</span><span class="ot">]</span></span>
<span id="cb8-14"><a href="#cb8-14"></a>  <span class="fu">}</span> <span class="er">as</span> <span class="er">$tmpfs</span></span>
<span id="cb8-15"><a href="#cb8-15"></a><span class="er">|</span> <span class="er">.mounts</span> <span class="er">|=</span> <span class="ot">[</span><span class="fu">{</span><span class="dt">&quot;destination&quot;</span><span class="fu">:</span><span class="st">&quot;/var/log&quot;</span><span class="fu">}</span> <span class="er">+</span> <span class="er">$tmpfs</span><span class="ot">]</span> <span class="er">+</span> <span class="er">.</span></span>
<span id="cb8-16"><a href="#cb8-16"></a><span class="er">|</span> <span class="er">.mounts</span> <span class="er">|=</span> <span class="ot">[</span><span class="fu">{</span><span class="dt">&quot;destination&quot;</span><span class="fu">:</span><span class="st">&quot;/tmp&quot;</span><span class="fu">}</span> <span class="er">+</span> <span class="er">$tmpfs</span><span class="ot">]</span> <span class="er">+</span> <span class="er">.</span></span>
<span id="cb8-17"><a href="#cb8-17"></a><span class="er">|</span> <span class="er">.mounts</span> <span class="er">|=</span> <span class="ot">[</span><span class="fu">{</span><span class="dt">&quot;destination&quot;</span><span class="fu">:</span><span class="st">&quot;/run&quot;</span><span class="fu">}</span> <span class="er">+</span> <span class="er">$tmpfs</span><span class="ot">]</span> <span class="er">+</span> <span class="er">.</span></span></code></pre></div>
<p>This program performs the following actions:</p>
<ul>
<li><p>Set the container process to <code>/sbin/init</code> (which is <em>systemd</em>).</p></li>
<li><p>Set the <code>$container</code> environment variable, as <a href="https://systemd.io/CONTAINER_INTERFACE/#environment-variables">required by systemd</a>.</p></li>
<li><p>Add UID and GID mappings for IDs <code>1</code>–<code>65536</code> in the container’s user namespace. The host range (started at <code>100000</code>) is taken from my user account’s assigned ranges in <code>/etc/subuid</code> and <code>/etc/subgid</code>. <strong>You may need a different number.</strong> The mapping for the container’s UID <code>0</code> to my user account already exists in the config.</p></li>
<li><p>Set the container’s cgroup path. A non-absolute path is interpreted relative to a runtime-determined location.</p></li>
<li><p>Tell the runtime to create a network namespace. Without this, the container will have no network stack and <em>nginx</em> won’t run.</p></li>
<li><p>Set the <a href="https://linux.die.net/man/7/capabilities">capabilities</a> required by the container. <em>systemd</em> requires all of these capabilities, although <code>CAP_NET_BIND_SERVICE</code> is only required for network name resolution (<em>systemd-resolved</em>). And <em>nginx</em>.</p></li>
<li><p>Tell the runtime to mount <code>tmpfs</code> filesystems at <code>/run</code>, <code>/tmp</code> and <code>/var/log</code>.</p></li>
</ul>
<p>I ran the program to modify the config, then started the container:</p>
<pre class="shell"><code>% jq --from-file filter.jq config.json.orig &gt; config.json
% ./runc --systemd-cgroup run test
systemd v246.10-1.fc33 running in system mode. (+PAM …
Detected virtualization container-other.
Detected architecture x86-64.

Welcome to Fedora 33 (Container Image)!

…

[  OK  ] Started The nginx HTTP and reverse proxy server.
[  OK  ] Reached target Multi-User System.
[  OK  ] Reached target Graphical Interface.
         Starting Update UTMP about System Runlevel Changes.
[  OK  ] Finished Update UTMP about System Runlevel Changes.

Fedora 33 (Container Image)
Kernel 5.11.17-300.fc34.x86_64 on an x86_64 (console)

runc login:</code></pre>
<p>OK! <em>systemd</em> initialised the system properly and started <em>nginx</em>. We can confirm <em>nginx</em> is running properly by running <code>curl</code> in the container:</p>
<pre class="shell"><code>% ./runc exec test curl --silent --head localhost:80
HTTP/1.1 200 OK
Server: nginx/1.18.0
Date: Thu, 27 May 2021 02:29:58 GMT
Content-Type: text/html
Content-Length: 5564
Last-Modified: Mon, 27 Jul 2020 22:20:49 GMT
Connection: keep-alive
ETag: &quot;5f1f5341-15bc&quot;
Accept-Ranges: bytes</code></pre>
<p>At this point we cannot access <em>nginx</em> from outside the container. That’s fine; I don’t need to work out how to do that. Not today, anyhow.</p>
<h2 id="how-runc-creates-cgroups">How <code>runc</code> creates cgroups <a href="#how-runc-creates-cgroups" class="section">§</a></h2>
<p><code>runc</code> manages container cgroups via the host’s <em>systemd</em> service. Specifically, it communicates with <em>systemd</em> over DBus to create a <a href="https://www.freedesktop.org/wiki/Software/systemd/ControlGroupInterface/">transient scope</a> for the container. Then it binds the container cgroup namespace to this new scope.</p>
<p>Observe that the inode of <code>/sys/fs/cgroup/</code> in the container is the same as the scope created for the container by <em>systemd</em> on the host:</p>
<pre class="shell"><code>% ./runc exec test ls -aldi /sys/fs/cgroup
64977 drwxr-xr-x. 5 root root 0 May 27 02:26 /sys/fs/cgroup

% ls -aldi /sys/fs/cgroup/user.slice/user-1000.slice/user@1000.service/user.slice/runc-test.scope 
64977 drwxr-xr-x. 5 ftweedal ftweedal 0 May 27 12:26 /sys/fs/cgroup/user.slice/user-1000.slice/user@1000.service/user.slice/runc-test.scope</code></pre>
<p>The mapping of <code>root</code> in the container’s user namespace to <code>ftweedal</code> is confirmed by the UID map of the container process:</p>
<pre class="shell"><code>% id --user ftweedal
1000
% ./runc list -f json | jq &#39;.[]|select(.id=&quot;test&quot;).pid&#39;
186718
% cat /proc/186718/uid_map
         0       1000          1
         1     100000      65536</code></pre>
<h2 id="next-steps">Next steps <a href="#next-steps" class="section">§</a></h2>
<p><em>systemd</em> is running properly in the container, but <code>root</code> in the container is mapped to my main user account. The container is not as isolated as I would like it to be. A partial sandbox escape could lead to the containerised process(es) messing with local files, or other processes owned by my user (including other containers).</p>
<p>User-namespaced containers in OpenShift (via CRI-O annotations) are allocated non-overlapping host ID ranges. All the host IDs are essentially anonymous. I confirmed this in <a href="2021-03-10-openshift-user-namespace-multi-user.html">a previous blog post</a>. That is good! But the container’s cgroup is owned by the <em>host’s</em> UID 0, which is unmapped in the container. <em>systemd</em>-based workloads cannot run because the container cannot write to its cgroupfs.</p>
<p>Therefore, the next steps in my investigation are:</p>
<ol type="1">
<li><p>Alter the ID mappings to use a single mapping of only “anonymous” users. This is a simple change to the OCI config. The host IDs still have to come from the user’s allocated sub-ID range.</p></li>
<li><p>Find (or implement) a way to change the ownership of the container’s cgroup scope to the <strong>container’s</strong> UID 0.</p></li>
</ol>
<p>When using the <em>systemd</em> cgroup manager, <code>runc</code> uses the <a href="https://www.freedesktop.org/wiki/Software/systemd/ControlGroupInterface/"><em>transient unit API</em></a> to ask <em>systemd</em> to create a new scope for the container. I am still learning about this API. Perhaps there is a way to specify a different ownership for the new scope or service. If so, we should be able to avoid changes to higher-level container runtimes like CRI-O. That would be the best outcome.</p>
<p>Otherwise, I will investigate whether we could use the OCI <code>createRuntime</code> hook to <code>chown(2)</code> the container’s cgroup scope. Unfortunately, the semantics of <code>createRuntime</code> is currently underspecified. The specification is ambiguous about whether the containers cgroup scope exists when this hook is executed. If this approach is valid, we will have to update CRI-O to add the relevant hook command to the OCI config.</p>
<p>Another possible approach is for the high-level runtime to perform the ownership change itself. This would be done after it invokes the OCI runtime’s <code>create</code> command, but before it invokes <code>start</code>. (See also the OCI <a href="https://github.com/opencontainers/runtime-spec/blob/master/runtime.md#lifecycle">container lifecycle description</a>). However, on OpenShift CRI-O runs as user <code>containers</code> and the container’s cgroup scope is owned by <code>root</code>. So I have doubts about the viability of this approach, as well as the OCI hook approach.</p>
<p>Whatever the outcome, there will certainly be more blog posts as I continue this long-running investigation. I still have much to learn as I struggle towards the goal of systemd-based workloads running securely on OpenShift.</p>]]></summary>
</entry>
<entry>
    <title>systemd containers on OpenShift with cgroups v2</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2021-03-30-openshift-cgroupv2-systemd.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2021-03-30-openshift-cgroupv2-systemd.html</id>
    <published>2021-03-30T00:00:00Z</published>
    <updated>2021-03-30T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="systemd-containers-on-openshift-with-cgroups-v2">systemd containers on OpenShift with cgroups v2</h1>
<p><em>systemd</em> in a container is a practical reality of migrating nontrivial applications to container infrastructure. It is not the “cloud native” way, but many applications written in The Before Times cannot be broken up and rearchitected without a huge cost. And so, there is a demand to run containers that run systemd, which in turn manages application services.</p>
<p>FreeIPA is one example. Its traditional environment is a dedicated Linux server (ignoring replicas). There are <em>many</em> services which both interact among themselves, and process requests from external clients and other FreeIPA servers. The engineering effort to redesign FreeIPA as a suite of several containerised services is expected to be very high. Therefore our small team focused on bringing FreeIPA to OpenShift therefore decided to pursue the “monolithic container” approach.</p>
<p>Support for systemd containers in OpenShift, <em>without hacks</em>, is a prerequisite for this approach to viable. In this post I experiment with systemd containers in OpenShift and share my results.</p>
<h2 id="test-application-http-server">Test application: HTTP server <a href="#test-application-http-server" class="section">§</a></h2>
<p>To test systemd containers on OpenShift, I created a Fedora-based container running the <em>nginx</em> HTTP server. I enable the <code>nginx</code> systemd and set the default command to <code>/sbin/init</code>, which is systemd. The server doesn’t host any interesting content, but if it responds to requests we know that systemd is working.</p>
<p>The <code>Containerfile</code> definition is:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode dockerfile"><code class="sourceCode dockerfile"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">FROM</span> fedora:33-x86_64</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="kw">RUN</span> dnf -y install nginx &amp;&amp; dnf clean all &amp;&amp; systemctl enable nginx</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">EXPOSE</span> 80</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="kw">CMD</span> [ <span class="st">&quot;/sbin/init&quot;</span> ]</span></code></pre></div>
<p>I built the container on my workstation and tagged it <code>test-nginx</code>. To check that the container works, I ran it locally and performed an HTTP request via <code>curl</code>:</p>
<pre class="shell"><code>% podman run --detach --publish 8080:80 test-nginx
2d8059e555c821d9ffcccd84bee88996207794957696c54e8d29787e8c33fab3

% curl --head localhost:8080
HTTP/1.1 200 OK
Server: nginx/1.18.0
Date: Thu, 25 Mar 2021 00:22:23 GMT
Content-Type: text/html
Content-Length: 5564
Last-Modified: Mon, 27 Jul 2020 22:20:49 GMT
Connection: keep-alive
ETag: &quot;5f1f5341-15bc&quot;
Accept-Ranges: bytes

% podman kill 2d8059e5
2d8059e555c821d9ffcccd84bee88996207794957696c54e8d29787e8c33fab3</code></pre>
<p>The container works properly in <code>podman</code>. I proceed to testing it on OpenShift.</p>
<h2 id="running-privileged-user">Running (<strong>privileged</strong> user) <a href="#running-privileged-user" class="section">§</a></h2>
<p>I performed my testing on an OpenShift 4.8 nightly cluster. The exact build is <code>4.8.0-0.nightly-2021-03-26-010831</code>. As far as I’m aware, with respect to systemd and cgroups there are no major differences between OpenShift 4.7 (which is Generally Available) and the build I’m using. So results should be similar on OpenShift 4.7.</p>
<p>The Pod definition for my test service is:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> nginx</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">labels</span><span class="kw">:</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">app</span><span class="kw">:</span><span class="at"> nginx</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> nginx</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">image</span><span class="kw">:</span><span class="at"> quay.io/ftweedal/test-nginx:latest</span></span></code></pre></div>
<p>I create the Pod, operating with the cluster <code>admin</code> credential. After a few seconds, the pod is running:</p>
<pre class="shell"><code>% oc create -f pod-nginx.yaml 
pod/nginx created

% oc get -o json pod/nginx | jq .status.phase
&quot;Running&quot;</code></pre>
<h3 id="verifying-that-the-service-is-working">Verifying that the service is working <a href="#verifying-that-the-service-is-working" class="section">§</a></h3>
<p><code>pod/nginx</code> is running, but it is not exposed to other pods in the cluster, or to the outside world. To test that the server is working, I will expose it on the hostname <code>nginx.apps.ft-48dev-5.idmocp.lab.eng.rdu2.redhat.com</code>. First, observe that performing an HTTP request from my workstation fails because the service is not available:</p>
<pre class="shell"><code>% curl --head nginx.apps.ft-48dev-5.idmocp.lab.eng.rdu2.redhat.com
HTTP/1.0 503 Service Unavailable
pragma: no-cache
cache-control: private, max-age=0, no-cache, no-store
content-type: text/html</code></pre>
<p>Now I create Service and Route objects to expose the nginx server. The Service definition is:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Service</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> nginx</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">selector</span><span class="kw">:</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">app</span><span class="kw">:</span><span class="at"> nginx</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">ports</span><span class="kw">:</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> </span><span class="fu">protocol</span><span class="kw">:</span><span class="at"> TCP</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">port</span><span class="kw">:</span><span class="at"> </span><span class="dv">80</span></span></code></pre></div>
<p>And the Route definition is:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Route</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> nginx</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">host</span><span class="kw">:</span><span class="at"> nginx.apps.ft-48dev-5.idmocp.lab.eng.rdu2.redhat.com</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">to</span><span class="kw">:</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">kind</span><span class="kw">:</span><span class="at"> Service</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">name</span><span class="kw">:</span><span class="at"> nginx</span></span></code></pre></div>
<p>I create the objects:</p>
<pre class="shell"><code>% oc create -f service-nginx.yaml 
service/nginx created

% oc create -f route-nginx.yaml
route.route.openshift.io/nginx created</code></pre>
<p>After a few seconds I performed the HTTP request again, and it succeeded:</p>
<pre class="shell"><code>% curl --head nginx.apps.ft-48dev-5.idmocp.lab.eng.rdu2.redhat.com
HTTP/1.1 200 OK
server: nginx/1.18.0
date: Tue, 30 Mar 2021 08:16:23 GMT
content-type: text/html
content-length: 5564
last-modified: Mon, 27 Jul 2020 22:20:49 GMT
etag: &quot;5f1f5341-15bc&quot;
accept-ranges: bytes
set-cookie: 6cf5f3bc2fa4d24f45018c591d3617c3=6f2f093d36d535f1dde195e08a311bda; path=/; HttpOnly
cache-control: private</code></pre>
<p>This confirms that the systemd container is running properly on OpenShift 4.8.</p>
<h3 id="low-level-details">Low-level details <a href="#low-level-details" class="section">§</a></h3>
<p>Now I will inspect some low-level details of the container. I’ll do that in a debug shell on the worker node. So first, I query the pod’s worker node name and container ID:</p>
<pre class="shell"><code>% oc get -o json pod/nginx \
    | jq &#39;.spec.nodeName,
          .status.containerStatuses[0].containerID&#39;
&quot;ft-48dev-5-f24l6-worker-0-q7lff&quot;
&quot;cri-o://d9d106cb65e4c965737ef66f15bd5b9e0988c386675e3404e24fd36e58284638&quot;</code></pre>
<p>Now I enter a debug shell on the worker node:</p>
<pre class="shell"><code>% oc debug node/ft-48dev-5-f24l6-worker-0-q7lff
Starting pod/ft-48dev-5-f24l6-worker-0-q7lff-debug ...
To use host binaries, run `chroot /host`
Pod IP: 10.8.1.64
If you don&#39;t see a command prompt, try pressing enter.
sh-4.2# chroot /host
sh-4.4# </code></pre>
<p>I use <code>crictl</code> to query the namespaces of the container:</p>
<pre class="shell"><code>sh-4.4# crictl inspect d9d106 \
        | jq .info.runtimeSpec.linux.namespaces[].type
&quot;pid&quot;
&quot;network&quot;
&quot;ipc&quot;
&quot;uts&quot;
&quot;mount&quot;</code></pre>
<p>Observe that there are <code>pid</code> and <code>mount</code> namespaces (among others), but no <code>cgroup</code> namespace. The worker node and container are using cgroups v1.</p>
<p>The <code>container_manage_cgroup</code> SELinux boolean is <code>off</code>:</p>
<pre class="shell"><code>sh-4.4# getsebool container_manage_cgroup
container_manage_cgroup --&gt; off</code></pre>
<p>Now let’s see what processes are running in the container. We can query the PID of the initial container process via <code>crictl inspect</code>. Then I use <code>pgrep(1)</code> with the <code>--ns</code> option, which lists processes executing in the same namespace(s) as the specified PID:</p>
<pre class="shell"><code>sh-4.4# crictl inspect d9d106 | jq .info.pid
14591

sh-4.4# pgrep --ns 14591 | xargs ps -o user,pid,cmd --sort pid
USER         PID CMD
root       14591 /sbin/init
root       14625 /usr/lib/systemd/systemd-journald
systemd+   14636 /usr/lib/systemd/systemd-resolved
root       14642 /usr/lib/systemd/systemd-homed
root       14643 /usr/lib/systemd/systemd-logind
root       14646 /sbin/agetty -o -p -- \u --noclear --keep-baud console 115200,38400,9600 xterm
dbus       14647 /usr/bin/dbus-broker-launch --scope system --audit
dbus       14651 dbus-broker --log 4 --controller 9 --machine-id 2f2fcc4033c5428996568ca34219c72a --max-bytes 536870912 --max-fds 4096 --max-matches 16384 --audit
root       14654 nginx: master process /usr/sbin/nginx
polkitd    14655 nginx: worker process
polkitd    14656 nginx: worker process
polkitd    14657 nginx: worker process
polkitd    14658 nginx: worker process
polkitd    14659 nginx: worker process
polkitd    14660 nginx: worker process
polkitd    14661 nginx: worker process
polkitd    14662 nginx: worker process</code></pre>
<p>The <code>PID</code> column shows the PIDs from the point of view of the host’s PID namespace. The first process (PID 1 <em>inside</em> the container) is systemd (<code>/sbin/init</code>). systemd has started other system services, and also nginx.</p>
<p>systemd is running as <code>root</code> <strong>on the host</strong>. The other processes run under various system accounts. The container does not have its own user namespace. This pod was created by a privileged account, which allows it to run as <code>root</code> on the host.</p>
<h2 id="running-unprivileged-user">Running (<strong>unprivileged</strong> user) <a href="#running-unprivileged-user" class="section">§</a></h2>
<p>I created an unprivileged user called <code>test</code>, and granted it admin privileges (so it can create pods).</p>
<pre class="shell"><code>% oc create user test
user.user.openshift.io/test created 

% oc adm policy add-role-to-user admin test
clusterrole.rbac.authorization.k8s.io/admin added: &quot;test&quot;</code></pre>
<p>I did not grant to the <code>test</code> account any <em>Security Context Constraints (SCCs)</em> that would allow it to run privileged containers or use host user accounts (including <code>root</code>).</p>
<p>Now I create the same <code>nginx</code> pod, as this user <code>test</code>. The pod fails to execute:</p>
<pre class="shell"><code>% oc --as test create -f pod-nginx.yaml
pod/nginx created

% oc get pod/nginx
NAME    READY   STATUS             RESTARTS   AGE
nginx   0/1     CrashLoopBackOff   1          23s</code></pre>
<p>Let’s inspect the logs to see what went wrong:</p>
<pre class="shell"><code>% oc logs pod/nginx
%</code></pre>
<p>There is no output. This baffled me, at first. Eventually I learned that Kubernetes, by default, does not allocate pseudo-terminal devices to containers. You can overcome this on a per-container basis by including <code>tty: true</code> in the Container object definition:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> nginx</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">labels</span><span class="kw">:</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">app</span><span class="kw">:</span><span class="at"> nginx</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> nginx</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">image</span><span class="kw">:</span><span class="at"> quay.io/ftweedal/test-nginx:latest</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">tty</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span></code></pre></div>
<p>With the pseudo-terminal enabled, <code>oc logs</code> now shows the error output:</p>
<pre class="shell"><code>% oc logs pod/nginx
systemd v246.10-1.fc33 running in system mode. (+PAM +AUDIT +SELINUX +IMA -APPARMOR +SMACK +SYSVINIT +UTMP +LIBCRYPTSETUP +GCRYPT +GNUTLS +ACL +XZ +LZ4 +ZSTD +SECCOMP +BLKID +ELFUTILS +KMOD +IDN2 -IDN +PCRE2 default-hierarchy=unified)
Detected virtualization container-other.
Detected architecture x86-64.

Welcome to Fedora 33 (Container Image)!

Set hostname to &lt;nginx&gt;.
Failed to write /run/systemd/container, ignoring: Permission denied
Failed to create /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod3bbed45f_634a_4f60_bb07_5f080c483f0f.slice/crio-90dead4cf549b844c4fb704765edfbba9e9e188b30299f484906f15d22b29fbd.scope/init.scope control group: Permission denied
Failed to allocate manager object: Permission denied
[!!!!!!] Failed to allocate manager object.
Exiting PID 1...</code></pre>
<p>The user executing systemd does not have permissions to write the cgroup filesystem. Although cgroups are heirarchical, cgroups v1 does not support delegating management of part of the heirarchy to unprivileged users. But cgroups v2 does support this.</p>
<div class="note">
<p>Set the <a href="https://www.freedesktop.org/software/systemd/man/systemd.html#%24SYSTEMD_LOG_LEVEL"><code>SYSTEMD_LOG_LEVEL</code></a> environment variable to <code>info</code> or <code>debug</code> to get more detail in the systemd log output.</p>
</div>
<h2 id="enabling-cgroups-v2">Enabling cgroups v2 <a href="#enabling-cgroups-v2" class="section">§</a></h2>
<p>We can enable cgroups v2 (only) on worker nodes via the following MachineConfig object:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> machineconfiguration.openshift.io/v1</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> MachineConfig</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> enable-cgroupv2-workers</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">labels</span><span class="kw">:</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">machineconfiguration.openshift.io/role</span><span class="kw">:</span><span class="at"> worker</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">kernelArguments</span><span class="kw">:</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> systemd.unified_cgroup_hierarchy=1</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> cgroup_no_v1=&quot;all&quot;</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> psi=1</span></span></code></pre></div>
<p>After creating the MachineConfig, the <em>Machine Config Operator</em> applies the configuration change and restarts each worker node, one by one. This occurs over several minutes.</p>
<h2 id="running-unprivileged-cgroups-v2">Running (<strong>unprivileged</strong>; <strong>cgroups v2</strong>) <a href="#running-unprivileged-cgroups-v2" class="section">§</a></h2>
<p>After some time, all worker nodes have the updated kernel configuration to enable cgroups v2 and disable cgroups v1. I again created the pod as the unprivileged <code>test</code> user. And again, pod execution failed. But this time the error is different:</p>
<pre class="shell"><code>% oc --as test create -f pod-nginx.yaml
pod/nginx created

% oc get pod
NAME    READY   STATUS   RESTARTS   AGE
nginx   0/1     Error    1          12s

% oc logs pod/nginx
systemd v246.10-1.fc33 running in system mode. (+PAM +AUDIT +SELINUX +IMA -APPARMOR +SMACK +SYSVINIT +UTMP +LIBCRYPTSETUP +GCRYPT +GNUTLS +ACL +XZ +LZ4 +ZSTD +SECCOMP +BLKID +ELFUTILS +KMOD +IDN2 -IDN +PCRE2 default-hierarchy=unified)
Detected virtualization container-other.
Detected architecture x86-64.

Welcome to Fedora 33 (Container Image)!

Set hostname to &lt;nginx&gt;.
Failed to write /run/systemd/container, ignoring: Permission denied
Failed to create /init.scope control group: Permission denied
Failed to allocate manager object: Permission denied
[!!!!!!] Failed to allocate manager object.
Exiting PID 1...</code></pre>
<p>The error suggests that the container now has its own cgroup namespace. I can confirm it by creating a <em>pod</em> debug container…</p>
<pre class="shell"><code>% oc debug pod/nginx
Starting pod/nginx-debug ...
Pod IP: 10.130.2.10
If you don&#39;t see a command prompt, try pressing enter.
sh-5.0$</code></pre>
<p>…finding out the node and container ID…</p>
<pre class="shell"><code>% oc get -o json pod/nginx-debug \
    | jq &#39;.spec.nodeName,
          .status.containerStatuses[0].containerID&#39;
&quot;ft-48dev-5-f24l6-worker-0-qv7kq&quot;
&quot;cri-o://e870d022d1c53adf94e36877312fcfef5ef0431ad9cf1fbe9c9d2ace02bee858&quot;</code></pre>
<p>…and analysing the container sandbox in a <em>node</em> debug shell:</p>
<pre><code>sh-4.4# crictl inspect e870d02 \
        | jq .info.runtimeSpec.linux.namespaces[].type
&quot;pid&quot;
&quot;network&quot;
&quot;ipc&quot;
&quot;uts&quot;
&quot;mount&quot;
&quot;cgroup&quot;</code></pre>
<p>The output confirms that the pod has a cgroup namespace. Despite this, the unprivileged user running systemd in the container does not have permission to manage the namespace. The <code>oc logs</code> output demonstrates this.</p>
<h3 id="container_manage_cgroups-selinux-boolean"><code>container_manage_cgroups</code> SELinux boolean <a href="#container_manage_cgroups-selinux-boolean" class="section">§</a></h3>
<p>I have one more thing to try. The <code>container_manage_cgroups</code> SELinux boolean was disabled on the worker nodes (per default configuration). Perhaps it is still needed, even when using cgroups v2. I enabled it on the worker node (directly from the debug shell, for now):</p>
<pre class="shell"><code>sh-4.4# setsebool container_manage_cgroup on</code></pre>
<p>I again created the nginx pod as the <code>test</code> user. It failed with the same error as the previous attempt, when <code>container_manage_cgroup</code> was <em>off</em>. So that was not the issue, or at least not the immediate issue.</p>
<h2 id="next-steps">Next steps <a href="#next-steps" class="section">§</a></h2>
<p>At this point, I have successfully enabled cgroups v2 on worker nodes. Container sandboxes have their own cgroup namespace. But inside the container, systemd fails with permission errors when it attempts some cgroup management.</p>
<p>The next step is to test the systemd container in OpenShift with cgroups v2 enabled <em>and</em> <a href="2021-03-03-openshift-4.7-user-namespaces.html">user namespaces enabled</a>. Both of these features are necessary for securely running a complex, systemd-based application in OpenShift. My hope is that enabling them <em>together</em> is the last step to getting systemd-based containers working properly in OpenShift. I will investigate and report the results in an upcoming post.</p>]]></summary>
</entry>
<entry>
    <title>Multiple users in user namespaces on OpenShift</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2021-03-10-openshift-user-namespace-multi-user.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2021-03-10-openshift-user-namespace-multi-user.html</id>
    <published>2021-03-10T00:00:00Z</published>
    <updated>2021-03-10T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="multiple-users-in-user-namespaces-on-openshift">Multiple users in user namespaces on OpenShift</h1>
<p>In the <a href="2021-03-03-openshift-4.7-user-namespaces.html">previous post</a> I confirmed that user namespaced pods are working in OpenShift 4.7. There are some rough edges, and the feature must be explicitly enabled in the cluster. But it fundamentally works.</p>
<p>One area I identified for a follow-up investigation is the behaviour of containers that execute multiple processes as different users. The correct and “expected” behaviour is important for <em>systemd</em>-based containers (among other scenarios). I did not anticipate any problems, but this is something we need to verify as part of the effort to bring FreeIPA to OpenShift. This post records my steps to verify that multi-user containers work as needed in user namespaces on OpenShift.</p>
<h2 id="setup">Setup <a href="#setup" class="section">§</a></h2>
<h3 id="cluster-configuration">Cluster configuration <a href="#cluster-configuration" class="section">§</a></h3>
<p>I configured the cluster as recorded in my earlier post, <a href="2020-12-01-openshift-crio-userns.html"><em>User namespaces in OpenShift via CRI-O annotations</em></a>.</p>
<h3 id="test-program">Test program <a href="#test-program" class="section">§</a></h3>
<p>I wrote a small Python program to serve as the container entrypoint. This program will run as <code>root</code> (in the namespace). For each of several hardcoded system accounts, it invokes <code>fork(2)</code> to duplicate the process. The child process executes <code>setuid(2)</code> to switch user account, then <code>execlp(3)</code> to replace itself with the <code>sleep(1)</code> program. The duration to sleep depends on the UID of the system account that executes it.</p>
<p>Outside the container, we will be able to observe whether the program (and its child processes) are running, and which user accounts they are running under.</p>
<p>The source of the test program:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os, pwd, time</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>users <span class="op">=</span> [<span class="st">&#39;root&#39;</span>, <span class="st">&#39;daemon&#39;</span>, <span class="st">&#39;operator&#39;</span>, <span class="st">&#39;nobody&#39;</span>, <span class="st">&#39;mail&#39;</span>]</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> user <span class="kw">in</span> users:</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    ent <span class="op">=</span> pwd.getpwnam(user)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    uid <span class="op">=</span> ent.pw_uid</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> os.fork() <span class="op">!=</span> <span class="dv">0</span>:</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        os.setuid(uid)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        os.execlp(<span class="st">&#39;sleep&#39;</span>, <span class="st">&#39;sleep&#39;</span>, <span class="bu">str</span>(<span class="dv">3000</span> <span class="op">+</span> uid))</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>time.sleep(<span class="dv">3600</span>)</span></code></pre></div>
<h3 id="container">Container <a href="#container" class="section">§</a></h3>
<p>The <code>Containerfile</code> is simple. Based on <code>fedora:33-x86_64</code>, it copies the Python program into the container and defines the entry point:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode dockerfile"><code class="sourceCode dockerfile"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">FROM</span> fedora:33-x86_64</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="kw">COPY</span> test_multiuser.py .</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="kw">ENTRYPOINT</span> [<span class="st">&quot;python3&quot;</span>, <span class="st">&quot;test_multiuser.py&quot;</span>]</span></code></pre></div>
<p>I built the container and pushed it to <a href="https://quay.io/repository/ftweedal/test-multiuser"><code>quay.io/ftweedal/test-multiuser:latest</code></a>.</p>
<h3 id="pod-specification">Pod specification <a href="#pod-specification" class="section">§</a></h3>
<p>The pod YAML is:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> multiuser-test</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">annotations</span><span class="kw">:</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">openshift.io/scc</span><span class="kw">:</span><span class="at"> restricted</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">io.kubernetes.cri-o.userns-mode</span><span class="kw">:</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="st">&quot;auto:size=65536;map-to-root=true&quot;</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> multiuser-test</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">image</span><span class="kw">:</span><span class="at"> quay.io/ftweedal/test-multiuser:latest</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">securityContext</span><span class="kw">:</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">runAsUser</span><span class="kw">:</span><span class="at"> </span><span class="dv">0</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">runAsGroup</span><span class="kw">:</span><span class="at"> </span><span class="dv">0</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">securityContext</span><span class="kw">:</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">sysctls</span><span class="kw">:</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;net.ipv4.ping_group_range&quot;</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">value</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;0 65535&quot;</span></span></code></pre></div>
<p>The <code>io.kubernetes.cri-o.userns-mode</code> annotation tells CRI-O to run the pod in a user namespace. The <code>runAsUser</code> and <code>runAsGroup</code> fields tell CRI-O to execute the entry point process as <code>root</code> (inside the namespace).</p>
<h2 id="verification">Verification <a href="#verification" class="section">§</a></h2>
<p>I created the pod:</p>
<pre class="shell"><code>% oc --as test create -f multiuser-test.yaml
pod/multiuser-test created</code></pre>
<p>After a short time, I queried the status, node and container ID of the pod:</p>
<pre class="shell"><code>% oc get -o json pod multiuser-test \
    | jq &#39;.status.phase,
          .spec.nodeName,
          .status.containerStatuses[0].containerID&#39;
&quot;Running&quot;
&quot;ft-47dev-1-4kplg-worker-0-qjfcj&quot;
&quot;cri-o://ee693645f41aa5b54b890862778f173ebaf465f741231426c9e80237aa60660b&quot;</code></pre>
<p>Next I opened a debug shell on the worker node and queried the container PID (process ID):</p>
<pre class="shell"><code>% oc debug node/ft-47dev-1-4kplg-worker-0-qjfcj
Starting pod/ft-47dev-1-4kplg-worker-0-qjfcj-debug ...
To use host binaries, run `chroot /host`
Pod IP: 10.8.0.165
If you don&#39;t see a command prompt, try pressing enter.
sh-4.2# chroot /host
sh-4.4# crictl inspect ee69364 | jq .info.pid
2445729</code></pre>
<p>I viewed the user map of the process:</p>
<pre class="shell"><code>sh-4.4# cat /proc/2445729/uid_map
         0     265536      65536</code></pre>
<p>This confirms that the container is in a user namespace. The UID range <code>0</code>–<code>65535</code> in the container is mapped to <code>265536</code>–<code>331071</code> on the host. That is in line with what I expect.</p>
<p>Now let’s see what else is running in that namespace. We can use <code>pgrep(1)</code> with the <code>--ns PID</code> option, which selects all processes in the same namespace(s) as <code>PID</code>. Then <code>ps(1)</code> can tell us which users are executing those processes.</p>
<pre class="shell"><code>sh-4.4# pgrep --ns 2445729 \
        | xargs ps -o user,pid,cmd --sort pid
USER         PID CMD
265536   2445729 sleep 3000
265538   2445766 sleep 3002
265547   2445767 sleep 3011
331070   2445768 sleep 68534
265544   2445769 sleep 3008
265536   2445770 python3 stuff.py</code></pre>
<p>The entry point spawned the expected 5 child processes. Each is running as a different user. This is the <em>host</em> view of the processes. Subtracting the base of the <code>uid_map</code> from each UID, we observe that the UIDs <em>in the namespace</em> are: <code>0</code>, <code>2</code>, <code>11</code>, <code>65534</code> and <code>8</code>. These are the UIDs of the five accounts declared in the test program.</p>
<h2 id="conclusion">Conclusion <a href="#conclusion" class="section">§</a></h2>
<p>Containers that use multiple users work as expected when using user namespaces in OpenShift.</p>
<p>The so far unstated assumption is that the mapped UID range includes all the UIDs actually used by the containerised application. Different applications use different UIDs, and different operating systems define different UIDs. So take care that the UID map hinted by the CRI-O annotation suits the container and application.</p>
<p>Note that mapped UID ranges in Linux need not be contiguous (either outside or inside the container). That is, a process may have multiple lines in its <code>/proc/&lt;PID&gt;/uid_map</code>, mapping multiple, non-overlapping and not-necessarily-adjacent ranges. But I am talking about the Linux user namespace feature here. I have not yet checked whether CRI-O + OpenShift admits this more complex scenario. But it is fundamentally possible.</p>
<p>The <code>nobody</code> user in Fedora has UID <code>65534</code>. Therefore a “simple mapping” must have a size not less than <em>65535</em> to use the <code>nobody</code> account in a user namespaced pod. OK, let’s round that up to <em>65536 = 2<sup>16</sup></em>. With a total UID space of <em>2<sup>16+16</sup></em>, you are limited to less than <em>65536</em> separate mappings. It sounds like a lot, but this limit could be a problem in large, complex environments. But most applications will use only a handful of UIDs. Non-contiguous UID mapping could dramatically increase the number of ranges available, by not mapping UIDs that applications do not use. But there is substantial complexity in defining and managing non-contiguous UID mappings.</p>]]></summary>
</entry>
<entry>
    <title>User namespace support in OpenShift 4.7</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2021-03-03-openshift-4.7-user-namespaces.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2021-03-03-openshift-4.7-user-namespaces.html</id>
    <published>2021-03-03T00:00:00Z</published>
    <updated>2021-03-03T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="user-namespace-support-in-openshift-4.7">User namespace support in OpenShift 4.7</h1>
<p>In a <a href="2020-12-01-openshift-crio-userns.html">previous post</a> I investigated how to use the annotation-based <a href="https://github.com/cri-o/cri-o/pull/3944">user namespace support</a> in CRI-O 1.20. At the end of that post, I was stuck. Now that <a href="https://www.openshift.com/blog/red-hat-openshift-4.7-is-now-available">OpenShift 4.7 has been released</a>, where do things stand?</p>
<h2 id="user-namespaces-are-working">User namespaces are working <a href="#user-namespaces-are-working" class="section">§</a></h2>
<p>Using the same setup, and a similar pod specification, I am able to run the pod in a user namespace. The process executes as <code>root</code> inside the namespace, and an unprivileged account outside the namespace.</p>
<p>I won’t repeat all the setup here, but one important difference is that I granted to <code>anyuid</code> SCC to the account that creates the pod (named <code>test</code>):</p>
<pre class="shell"><code>% oc adm policy add-scc-to-user anyuid test
securitycontextconstraints.security.openshift.io/anyuid added to: [&quot;test&quot;]</code></pre>
<p>The pod definition is:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="pp">% cat userns-test.yaml</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> userns-test</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">annotations</span><span class="kw">:</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">openshift.io/scc</span><span class="kw">:</span><span class="at"> restricted</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">io.kubernetes.cri-o.userns-mode</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;auto:size=65536;map-to-root=true&quot;</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> userns-test</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">image</span><span class="kw">:</span><span class="at"> freeipa/freeipa-server:fedora-31</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">command</span><span class="kw">:</span><span class="at"> </span><span class="kw">[</span><span class="st">&quot;sleep&quot;</span><span class="kw">,</span><span class="at"> </span><span class="st">&quot;3601&quot;</span><span class="kw">]</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">securityContext</span><span class="kw">:</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">runAsUser</span><span class="kw">:</span><span class="at"> </span><span class="dv">0</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">runAsGroup</span><span class="kw">:</span><span class="at"> </span><span class="dv">0</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">securityContext</span><span class="kw">:</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">sysctls</span><span class="kw">:</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;net.ipv4.ping_group_range&quot;</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">value</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;0 65535&quot;</span></span></code></pre></div>
<p>Note the <code>io.kubernetes.cri-o.userns-mode</code> annotation. That activates the user namespace feature. The <code>runAsUser</code> and <code>runAsGroup</code> fields in <code>securityContext</code> are also important.</p>
<p>I create the pod. After a few moments I observe that it is running, and query the node and container ID:</p>
<pre class="shell"><code>$ oc --as test create -f userns-test.yaml
pod/userns-test created

% oc get -o json pod userns-test \
    | jq .status.phase
&quot;Running&quot;

% oc get -o json pod userns-test \
    | jq .spec.nodeName
&quot;ft-47dev-1-4kplg-worker-0-qjfcj&quot;

% oc get -o json pod userns-test \
    | jq &quot;.status.containerStatuses[0].containerID&quot;
&quot;cri-o://92bf6c3b61337f18f4c963450b5db76cbcd4aa73e2659759ba2725f4d0f8aac7&quot;</code></pre>
<p>In a debug shell on the worker node, I use <code>crictl</code> to find out the pid of the pod’s (first) process:</p>
<pre class="shell"><code>% oc debug node/ft-47dev-1-4kplg-worker-0-qjfcj
Starting pod/ft-47dev-1-4kplg-worker-0-qjfcj-debug ...
To use host binaries, run `chroot /host`
Pod IP: 10.8.0.165
If you don&#39;t see a command prompt, try pressing enter.
sh-4.2# chroot /host
sh-4.4# crictl inspect 92bf6c3b | jq .info.pid
937107</code></pre>
<div class="note">
<p>Earlier versions of <code>crictl</code> have the PID in the top-level object (<code>jq</code> selector <code>.pid</code>). The selector is now <code>.info.pid</code>.</p>
</div>
<p>Now we can query the UID map of the container process:</p>
<pre class="shell"><code>sh-4.4# cat /proc/937107/uid_map
         0     200000      65536</code></pre>
<p>This shows that the process is running as uid 0 (<code>root</code>) in the namespace, and uid 200000 outside the namespace. The mapped range is contiguous and has size 65536, which agrees with the annotation:</p>
<pre><code>io.kubernetes.cri-o.userns-mode: &quot;auto:size=65536;map-to-root=true&quot;</code></pre>
<p>This is great!</p>
<h2 id="they-still-require-a-privileged-service-account">They still require a privileged service account <a href="#they-still-require-a-privileged-service-account" class="section">§</a></h2>
<p>In my earlier investigation I found that that users require the <code>anyuid</code> SCC (or equivalent) to create user-namespaced pods running as specific UIDs (e.g. <code>root</code>) inside the pod. This is still the case. Rescinding <code>anyuid</code> from user <code>test</code> and (re)creating the pod results in an error:</p>
<pre class="shell"><code>% oc adm policy remove-scc-from-user anyuid test
securitycontextconstraints.security.openshift.io/anyuid removed from: [&quot;test&quot;]

% oc --as test create -f userns-test.yaml
Error from server (Forbidden): error when creating
&quot;userns-test.yaml&quot;: pods &quot;userns-test&quot; is forbidden: unable to
validate against any security context constraint:
[spec.containers[0].securityContext.runAsUser: Invalid value: 0:
must be in the ranges: [1000630000, 1000639999]]</code></pre>
<p>At the end of my previous post, I wrote:</p>
<blockquote>
<p>The security context constraint (SCC) is prohibiting the use of uid <code>0</code> for the container process. Switching to a permissive SCC might allow me to proceed, but it would also mean using a more privileged OpenShift user account. Then that privileged account could then create containers running as <code>root</code> <em>in the system user namespace</em>. We want user namespaces in OpenShift so that we can <em>avoid</em> this exact scenario. So resorting to a permissive SCC (e.g. <code>anyuid</code>) feels like the wrong way to go.</p>
</blockquote>
<p>After giving this more thought, my opinion has shifted. This is still an important gap in overall security, and it should be addressed. But even though it currently requires a privileged account to create user-namespaced pods, that fact that you even <em>can</em> is a huge win.</p>
<p>In other words, the user namespace support in its current form is still a giant leap forward. Previously, many kinds of applications cannot run securely in OpenShift. The service account privileges caveat may be unacceptable to some, but I hope that would be addressed in time.</p>
<h2 id="inconsistent-treatment-of-securitycontext">Inconsistent treatment of <code>securityContext</code> <a href="#inconsistent-treatment-of-securitycontext" class="section">§</a></h2>
<p>The PodSpec I used above (with success) is:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> userns-test</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">image</span><span class="kw">:</span><span class="at"> freeipa/freeipa-server:fedora-31</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">command</span><span class="kw">:</span><span class="at"> </span><span class="kw">[</span><span class="st">&quot;sleep&quot;</span><span class="kw">,</span><span class="at"> </span><span class="st">&quot;3601&quot;</span><span class="kw">]</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">securityContext</span><span class="kw">:</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">runAsUser</span><span class="kw">:</span><span class="at"> </span><span class="dv">0</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">runAsGroup</span><span class="kw">:</span><span class="at"> </span><span class="dv">0</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="fu">securityContext</span><span class="kw">:</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">sysctls</span><span class="kw">:</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;net.ipv4.ping_group_range&quot;</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">value</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;0 65535&quot;</span></span></code></pre></div>
<p>Note there are two <code>securityContext</code> fields. The first, in the Container spec, is a <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#securitycontext-v1-core">SecurityContext</a> object. The second, in the PodSpec, is a <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#podsecuritycontext-v1-core">PodSecurityContext</a> object.</p>
<p>The <code>runAsUser</code> and <code>runAsGroup</code> fields can be specified in either of these objects (or both, with SecurityContext taking precedence). I can move these fields to the PodSecurityContext, as below.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> userns-test</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">image</span><span class="kw">:</span><span class="at"> freeipa/freeipa-server:fedora-31</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">command</span><span class="kw">:</span><span class="at"> </span><span class="kw">[</span><span class="st">&quot;sleep&quot;</span><span class="kw">,</span><span class="at"> </span><span class="st">&quot;3601&quot;</span><span class="kw">]</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="fu">securityContext</span><span class="kw">:</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">runAsUser</span><span class="kw">:</span><span class="at"> </span><span class="dv">0</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">runAsGroup</span><span class="kw">:</span><span class="at"> </span><span class="dv">0</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">sysctls</span><span class="kw">:</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;net.ipv4.ping_group_range&quot;</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">value</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;0 65535&quot;</span></span></code></pre></div>
<p>According to the documentation, this object should have the same meaning as the previous one. But there is a critical behavioural difference! I create and examine the pod as before:</p>
<pre class="shell"><code>$ oc --as test create -f userns-test.yaml
pod/userns-test created

% oc get -o json pod userns-test \
    | jq .status.phase
&quot;Running&quot;

% oc get -o json pod userns-test \
    | jq .spec.nodeName
&quot;ft-47dev-1-4kplg-worker-0-qjfcj&quot;

% oc get -o json pod userns-test \
    | jq &quot;.status.containerStatuses[0].containerID&quot;
&quot;cri-o://c90760e88ee8493bfdb9af661c18afef139b79541160850ceac125b0c62e1de3&quot;</code></pre>
<p>And in the node debug shell, I query the <code>uid_map</code> for the container:</p>
<pre class="shell"><code>sh-4.4# crictl inspect c90760e | jq .info.pid
1022187
sh-4.4# cat /proc/1022187/uid_map
         1     200001      65535
         0          0          1
</code></pre>
<p>This subtle change to the object definition caused OpenShift to run the process as <code>root</code> in the container <strong>and on the host!</strong> Given that the Kubernetes documentation implies that the two configurations are equivalent, this is a dangerous situation. I will file a ticket to bring this to the attention of the developers.</p>
<h2 id="continuing-investigation">Continuing investigation <a href="#continuing-investigation" class="section">§</a></h2>
<p>There are two particular lines of investigation I need to pursue from here. The first is to confirm that <code>setuid(2)</code> and related functionality work properly in the namespaced container. This is important for containers that run multiple processes as different users. I do not anticipate any particular issues here. But I still need to verify it.</p>
<div class="note">
<p>This is not the <em>cloud native</em> way. But this is the approach we are taking, for now. “Monolithic container” is a reasonable way to bring complex, traditional software systems into the cloud. As long as it can be done securely.</p>
</div>
<p>The other line of investigation is to find out how user-namespaced containers interact with volume mounts. If multiple containers, perhaps running on different nodes, read and write the same volume, what are the UIDs on that volume? Do we need stable, cluster-wide subuid/subgid mappings? If so, how can that be achieved? I expect I will much more to say about this in upcoming posts.</p>
<h2 id="conclusion">Conclusion <a href="#conclusion" class="section">§</a></h2>
<p>CRI-O annotation-based user namespaces work in OpenShift 4.7. But there are some caveats, and at least one scary “gotcha”. Nevertheless, for simple workloads the feature does work well. It is big leap forward for running more kinds of workloads without compromising the security of your cluster.</p>
<p>In time, I hope the account privilege (SCC) caveat and <code>securityContext</code> issues can be resolved. I will file tickets and continue to discuss these topics with the OpenShift developers. And my investigations about more complex workloads and multi-node considerations shall continue.</p>]]></summary>
</entry>
<entry>
    <title>Simple Java to C bindings via JNA</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2020-12-16-java-jna.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2020-12-16-java-jna.html</id>
    <published>2020-12-16T00:00:00Z</published>
    <updated>2020-12-16T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="simple-java-to-c-bindings-via-jna">Simple Java to C bindings via JNA</h1>
<p>This post is an introduction to <em>JNI</em>, an FFI system for Java.</p>
<p>Most languages offer a way to bind to (use) shared libraries, which are often written in C (Rust is becoming popular too). The general name for such a facility is <a href="https://en.wikipedia.org/wiki/Foreign_function_interface"><em>foreign function interface</em></a> (<em>FFI</em>). FFIs facilitate code reuse, and use of operating system-level functions that would not otherwise be possible.</p>
<p>There are two significant FFI systems for Java. The older is <a href="https://docs.oracle.com/en/java/javase/14/docs/specs/jni/index.html"><em>Java Native Interface</em></a> (<em>JNI</em>)—an official Java standard. The <a href="https://github.com/dogtagpki/jss"><em>JSS</em></a> Java binding to the <a href="https://developer.mozilla.org/en-US/docs/Mozilla/Projects/NSS"><em>NSS</em></a> cryptography and security library makes heavy use of JNI. The main drawbacks of JNI are that it involves writing C, and is boilerplate-heavy.</p>
<p><a href="https://github.com/java-native-access/jna"><em>Java Native Access</em></a> (<em>JNA</em>) offers a more lightweight approach. You import JNA as a library and define your binding as a native Java object. There is only a small amount of boilerplate to import the JNA packages, open the shared library, and declare Java method signatures for the functions you want to use. JNA performs <a href="https://github.com/java-native-access/jna/blob/5.6.0/www/Mappings.md">automatic conversion</a> between native Java and C types.</p>
<p>If you are familiar with Python, you might recognise that the JNA approach is similar to <a href="https://cffi.readthedocs.io/en/latest/"><em>cffi</em></a>. In fact, JNA and cffi use the same underlying FFI library, <a href="https://sourceware.org/libffi/"><em>libffi</em></a>.</p>
<h2 id="using-jna-in-dogtag">Using JNA in Dogtag <a href="#using-jna-in-dogtag" class="section">§</a></h2>
<p>To simplify and speed up <a href="https://www.freeipa.org/">FreeIPA</a> startup, I needed to implement systemd notification support in <a href="https://www.dogtagpki.org/">Dogtag PKI</a>. Dogtag (when so configured) should call <a href="https://www.freedesktop.org/software/systemd/man/sd_notify.html"><code>sd_notify(3)</code></a> to notify the system service manager when it has started up and is ready to service requests.</p>
<p>Dogtag already uses JNI in a few places (as does some of its dependencies, including JSS). But I was not keen to use JNI, with all its complexity, for this small use case. A colleague pointed me to JNA, and I decided to give it a go.</p>
<p>The resulting code is so small I’ll just include it all here, with commentary. (I made some changes for clarity; you can review the actual patch in the <a href="https://github.com/dogtagpki/pki/pull/569/files">pull request</a>).</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode java"><code class="sourceCode java"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">package</span><span class="im"> com</span><span class="op">.</span><span class="im">netscape</span><span class="op">.</span><span class="im">cmscore</span><span class="op">.</span><span class="im">systemd</span><span class="op">;</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">import</span> <span class="im">com</span><span class="op">.</span><span class="im">sun</span><span class="op">.</span><span class="im">jna</span><span class="op">.</span><span class="im">Library</span><span class="op">;</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="kw">import</span> <span class="im">com</span><span class="op">.</span><span class="im">sun</span><span class="op">.</span><span class="im">jna</span><span class="op">.</span><span class="im">Native</span><span class="op">;</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="kw">public</span> <span class="kw">class</span> SystemdStartupNotifier <span class="op">{</span></span></code></pre></div>
<p>Import JNA and begin the class definition.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode java"><code class="sourceCode java"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">interface</span> Systemd <span class="kw">extends</span> Library <span class="op">{</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">public</span> <span class="dt">int</span> <span class="fu">sd_booted</span><span class="op">();</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">public</span> <span class="dt">int</span> <span class="fu">sd_notify</span><span class="op">(</span><span class="dt">int</span> unset_env<span class="op">,</span> <span class="bu">String</span> state<span class="op">);</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p>Declare an interface to the shared library by extending <code>sun.jna.Native</code>. Method signatures must match the native function signatures, according to the <a href="https://github.com/java-native-access/jna/blob/5.6.0/www/Mappings.md">type mappings</a>.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode java"><code class="sourceCode java"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>Systemd systemd <span class="op">=</span> <span class="kw">null</span><span class="op">;</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="kw">public</span> <span class="dt">void</span> <span class="fu">init</span><span class="op">()</span> <span class="op">{</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    systemd <span class="op">=</span> Native<span class="op">.</span><span class="fu">load</span><span class="op">(</span><span class="st">&quot;systemd&quot;</span><span class="op">,</span> Systemd<span class="op">.</span><span class="fu">class</span><span class="op">);</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p><code>init()</code> gets called by initialisation code. <code>Native.load()</code> loads <code>libsystemd.so</code> and initialises the foreign library proxy with respect to the <code>Systemd</code> interface. The proxy object is assigned to the instance variable <code>systemd</code>. An alternative approach is to assign the proxy object to a static variable in the interface definition (<a href="https://github.com/java-native-access/jna/blob/5.6.0/www/GettingStarted.md#getting-started-with-jna">example</a>).</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode java"><code class="sourceCode java"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="dt">boolean</span> <span class="fu">notify</span><span class="op">(</span><span class="bu">String</span> status<span class="op">)</span> <span class="op">{</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(!</span>systemd<span class="op">.</span><span class="fu">sd_booted</span><span class="op">())</span> <span class="op">{</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="kw">true</span><span class="op">;</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span> <span class="cf">else</span> <span class="op">{</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        <span class="dt">int</span> r <span class="op">=</span> systemd<span class="op">.</span><span class="fu">sd_notify</span><span class="op">(</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>            <span class="dv">0</span> <span class="co">/* don&#39;t unset environment */</span><span class="op">,</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>            status<span class="op">);</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="op">(</span>r <span class="op">&lt;</span> <span class="dv">1</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>            <span class="bu">System</span><span class="op">.</span><span class="fu">err</span><span class="op">.</span><span class="fu">println</span><span class="op">(</span><span class="st">&quot;sd_notify failed&quot;</span><span class="op">);</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="kw">false</span><span class="op">;</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span> <span class="cf">else</span> <span class="op">{</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="kw">true</span><span class="op">;</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
<p><code>notify()</code> makes two foreign calls. First it calls <a href="https://www.freedesktop.org/software/systemd/man/sd_booted.html"><code>sd_booted(3)</code></a> to see if the system was booted using systemd. If not, we return (successfully). If the program <em>is</em> running under systemd it calls <a href="https://www.freedesktop.org/software/systemd/man/sd_notify.html"><code>sd_notify(3)</code></a>, logging an error on failure.</p>
<p>That’s pretty much all there is to it. This is much, <em>much</em> nicer than JNI.</p>
<h2 id="discussion">Discussion <a href="#discussion" class="section">§</a></h2>
<p>The adoption of JNA in Dogtag—which already (and still) uses JNI—was not without debate. But JNA is mature, widely available and supported in Dogtag’s target platforms (Fedora and RHEL). In the end, it was agreed that JNA is a nice approach. If JNA becomes problematic for any reason we can reimplement the binding to use JNI instead. The patch was accepted.</p>
<p>As the Dogtag experience demonstrates, where multiple FFI systems are available it is not necessarily an either/or choice. JNI and JNA now happily coexist in the Dogtag database. It would be nice to gradually migrate Dogtag away from JNI and use JNA exclusively, but this is not a priority.</p>
<p>There are more advanced topics that were not covered in this post. These include callbacks, custom type mapping and dealing with C <code>struct</code> and <code>union</code> types. The <a href="https://github.com/java-native-access/jna/tree/5.6.0#using-the-library">in-tree documentation</a> provides guidance on these and other advanced topics.</p>]]></summary>
</entry>

</feed>
