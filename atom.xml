<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Fraser's IdM Blog</title>
    <link href="https://frasertweedale.github.io/blog-redhat/atom.xml" rel="self" />
    <link href="https://frasertweedale.github.io/blog-redhat" />
    <id>https://frasertweedale.github.io/blog-redhat/atom.xml</id>
    <author>
        <name>Fraser Tweedale</name>
        <email>frase@frase.id.au</email>
    </author>
    <updated>2020-12-01T00:00:00Z</updated>
    <entry>
    <title>User namespaces in OpenShift via CRI-O annotations</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2020-12-01-openshift-crio-userns.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2020-12-01-openshift-crio-userns.html</id>
    <published>2020-12-01T00:00:00Z</published>
    <updated>2020-12-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="user-namespaces-in-openshift-via-cri-o-annotations">User namespaces in OpenShift via CRI-O annotations</h1>
<p>In a recent post I covered the lack of user namespace support in OpenShift, and discussed the <a href="https://github.com/cri-o/cri-o/pull/3944">upcoming CRI-O feature</a> for user namespacing of containers, controlled by annotations.</p>
<p>I now have an OpenShift nightly cluster deployed. It uses a prerelease version of CRI-O v1.20, which includes this new feature. So it’s time to experiment! This post records my investigation of this feature.</p>
<h2 id="preliminaries">Preliminaries</h2>
<p>I’ll skip the details of deploying the nightly (4.7) cluster (because they are not important). What <em>is</em> important is that I created a <code>MachineConfig</code> to enable the CRI-O user namespace annotation feature, <a href="2020-11-30-openshift-machine-config-operator.html">as described in my previous post</a>.</p>
<p>As in the initial investigation, I created a new user account and project namespace for the experiments:</p>
<pre><code>% oc new-project test
Now using project &quot;test&quot; on server &quot;https://api.permanent.idmocp.lab.eng.rdu2.redhat.com:6443&quot;.

% oc create user test
user.user.openshift.io/test created

% oc adm policy add-role-to-user admin test
clusterrole.rbac.authorization.k8s.io/admin added: &quot;test&quot;</code></pre>
<h2 id="creating-a-user-namespaced-pod---attempt-1">Creating a user namespaced pod - Attempt 1</h2>
<p>I defined a pod that just runs <code>sleep</code>, but uses the new annotation to run it in a user namespace. The <code>map-to-root=true</code> directive says that the “beginning” of the host uid range assigned to the container should maps to uid 0 (i.e. <code>root</code>) in the container.</p>
<pre><code>$ cat userns-test.yaml
apiVersion: v1
kind: Pod
metadata:
  name: userns-test
  annotations:
    io.kubernetes.cri-o.userns-mode: &quot;auto:map-to-root=true&quot;
spec:
  containers:
  - name: userns-test
    image: freeipa/freeipa-server:fedora-31
    command: [&quot;sleep&quot;, &quot;3601&quot;]</code></pre>
<p>Create the pod:</p>
<pre><code>$ oc --as test create -f userns-test.yaml
pod/userns-test created</code></pre>
<p>After a few seconds, does everything look OK?</p>
<pre><code>$ oc get pod userns-test
NAME          READY   STATUS              RESTARTS   AGE
userns-test   0/1     ContainerCreating   0          14s</code></pre>
<p>Hm, 14 seconds seems a long time to be stuck at <code>ContainerCreating</code>. What does <code>oc describe</code> reveal?</p>
<pre><code>$ oc describe pod/userns-test
Name:         userns-test
Namespace:    test
Priority:     0
Node:         ft-47dev-2-27h8r-worker-0-j4jjn/10.8.1.106
Start Time:   Mon, 30 Nov 2020 12:41:34 +0000
Labels:       &lt;none&gt;
Annotations:  io.kubernetes.cri-o.userns-mode: auto:map-to-root=true
              openshift.io/scc: restricted
Status:       Pending

...

Events:
  Type     Reason                  Age                       From                                      Message
  ----     ------                  ----                      ----                                      -------
  Normal   Scheduled               &lt;unknown&gt;                                                           Successfully assigned test/userns-test to ft-47dev-2-27h8r-worker-0-j4jjn
  Warning  FailedCreatePodSandBox  &lt;invalid&gt; (x96 over 20m)  kubelet, ft-47dev-2-27h8r-worker-0-j4jjn  Failed to create pod sandbox: rpc error: code = Unknown desc = error creating pod sandbox with name &quot;k8s_userns-test_test_e4f69d50-e061-46ca-b933-000bcea3363a_0&quot;: could not find enough available IDs</code></pre>
<p>The node failed to create the pod sandbox. To spare you scrolling to read the unwrapped error message, I’ll reproduce it:</p>
<pre><code>Failed to create pod sandbox: rpc error: code = Unknown
desc = error creating pod sandbox with name
&quot;k8s_userns-test_test_e4f69d50-e061-46ca-b933-000bcea3363a_0&quot;:
could not find enough available IDs</code></pre>
<p>My initial reaction to this error is: <strong>this is good!</strong> It <em>seems</em> that CRI-O is attempting to create a user namespace for the container, but cannot. Another problem to solve, but we seem to be on the right track.</p>
<h2 id="etcsubuid"><code>/etc/subuid</code></h2>
<p>I had not yet done any host configuration related to user namespace mappings. But I had a feeling that the <code>/etc/subuid</code> and <code>/etc/subgid</code> files would come into play. According to <code>subuid(5)</code>:</p>
<blockquote>
<p>Each line in /etc/subuid contains a user name and a range of subordinate user ids that user is allowed to use.</p>
</blockquote>
<p>The description in <code>subgid(5)</code> is similar.</p>
<p>If the user that is attempting to create the containers doesn’t have an sufficient range of unused host uids and gids to use, it follows that it will not be able to create the user namespace for the pod.</p>
<p>I used a debug shell to observe the current contents of <code>/etc/subuid</code> and <code>/etc/subgid</code> on worker nodes:</p>
<pre><code>sh-4.4# cat /etc/subuid
core:100000:65536
sh-4.4# cat /etc/subgid
core:100000:65536</code></pre>
<p>The user <code>core</code> owns a uid and gid range of size 65536, starting at uid/gid 100000. There are no other ranges defined.</p>
<p>At this point, I have a strong feeling we need to define uid and gid ranges for the appropriate user, and then things will hopefully start working. The next question is: <em>who is the appropriate user</em>? That is, in OpenShift which user is responsible for creating the containers and, in this case, the user namespaces? Again on the worker node debug shell, I queried which user is running <code>crio</code>:</p>
<pre><code>sh-4.4# ps -o user,pid,cmd -p $(pgrep crio)
USER         PID CMD
root        1791 /usr/bin/crio --enable-metrics=true --metrics-port=9537</code></pre>
<p><code>crio</code> is running as the <code>root</code> user, which is not surprising. So we will need to add mappings for the <code>root</code> user to the mapping files.</p>
<h3 id="machineconfig-for-modifying-etcsubugid"><code>MachineConfig</code> for modifying <code>/etc/sub[ug]id</code></h3>
<p>I will create a <code>MachineConfig</code> to append the mappings <code>/etc/subuid</code> and <code>/etc/subgid</code>. First we need the base64 encoding of the line we want to add:</p>
<pre><code>$ echo &quot;root:200000:268435456&quot; | base64
cm9vdDoyMDAwMDA6MjY4NDM1NDU2Cg==</code></pre>
<p>The <code>MachineConfig</code> definition (note that it is scoped to the <code>worker</code> role):</p>
<pre><code>$ cat machineconfig-subuid-subgid.yaml 
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: subuid-subgid
spec:
  config:
    ignition:
      version: 3.1.0
    storage:
      files:
      - path: /etc/subuid
        append:
          - source: data:text/plain;charset=utf-8;base64,cm9vdDoyMDAwMDA6MjY4NDM1NDU2Cg==
      - path: /etc/subgid
        append:
          - source: data:text/plain;charset=utf-8;base64,cm9vdDoyMDAwMDA6MjY4NDM1NDU2Cg==</code></pre>
<p>Creating the <code>MachineConfig</code> object:</p>
<pre><code>$ oc create -f machineconfig-subuid-subgid.yaml
machineconfig.machineconfiguration.openshift.io/subuid-subgid created</code></pre>
<p>After a few moments, checking the <code>machineconfigpool/worker</code> object revealed that cluster is in a degraded state:</p>
<pre><code>$ oc get -o json mcp/worker |jq &#39;.status.conditions[-2:]&#39;
[
  {
    &quot;lastTransitionTime&quot;: &quot;2020-12-01T02:55:52Z&quot;,
    &quot;message&quot;: &quot;Node ft-47dev-2-27h8r-worker-0-f8bnl is reporting: \&quot;can&#39;t reconcile config rendered-worker-a37679c5cfcefb5b0af61bb3674dccc4 with rendered-worker-3cbd4cabeedd441500c83363dbf505fd: ignition file /etc/subuid includes append: unreconcilable\&quot;&quot;,
    &quot;reason&quot;: &quot;1 nodes are reporting degraded status on sync&quot;,
    &quot;status&quot;: &quot;True&quot;,
    &quot;type&quot;: &quot;NodeDegraded&quot;
  },
  {
    &quot;lastTransitionTime&quot;: &quot;2020-12-01T02:55:52Z&quot;,
    &quot;message&quot;: &quot;&quot;,
    &quot;reason&quot;: &quot;&quot;,
    &quot;status&quot;: &quot;True&quot;,
    &quot;type&quot;: &quot;Degraded&quot;
  }
]</code></pre>
<p>The error message is:</p>
<pre><code>Node ft-47dev-2-27h8r-worker-0-f8bnl is reporting: \&quot;can&#39;t
reconcile config rendered-worker-a37679c5cfcefb5b0af61bb3674dccc4
with rendered-worker-3cbd4cabeedd441500c83363dbf505fd: ignition
file /etc/subuid includes append: unreconcilable\&quot;&quot;,</code></pre>
<p>Upon further investigation, I learned that the Machine Config Operator does not support <code>append</code> operations. This is because are not idempotent. So I will try again with a new machine config that completely replaces the <code>/etc/subuid</code> and <code>/etc/subgid</code> files.</p>
<p>The new content shall be:</p>
<pre><code>core:100000:65536
root:200000:268435456</code></pre>
<p>The updated <code>MachineConfig</code> definition is:</p>
<pre><code>$ cat machineconfig-subuid-subgid.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: subuid-subgid
spec:
  config:
    ignition:
      version: 3.1.0
    storage:
      files:
      - path: /etc/subuid
        overwrite: true
        contents:
          source: data:text/plain;charset=utf-8;base64,Y29yZToxMDAwMDA6NjU1MzYKcm9vdDoyMDAwMDA6MjY4NDM1NDU2Cg==
      - path: /etc/subgid
        overwrite: true
        contents:
          source: data:text/plain;charset=utf-8;base64,Y29yZToxMDAwMDA6NjU1MzYKcm9vdDoyMDAwMDA6MjY4NDM1NDU2Cg==</code></pre>
<p>I replaced the <code>MachineConfig</code> object:</p>
<pre><code>$ oc replace -f machineconfig-subuid-subgid.yaml
machineconfig.machineconfiguration.openshift.io/subuid-subgid replaced</code></pre>
<p>After a few moments, the cluster is no longer degraded and the worker nodes will be updated over the next several minutes:</p>
<pre><code>$ oc get mcp/worker
NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
worker   rendered-worker-a37679c5cfcefb5b0af61bb3674dccc4   False     True       False      4              0                   0                     0                      3d20h</code></pre>
<p>After <code>READYMACHINECOUNT</code> reached <code>4</code> (all machines in the <code>worker</code> pool), I used a debug shell on one of the worker nodes to confirm that the changes had been applied:</p>
<pre><code>$ oc debug node/ft-47dev-2-27h8r-worker-0-j4jjn
Starting pod/ft-47dev-2-27h8r-worker-0-j4jjn-debug ...
To use host binaries, run `chroot /host`
Pod IP: 10.8.1.106
If you don&#39;t see a command prompt, try pressing enter.
sh-4.2# chroot /host
sh-4.4# cat /etc/subuid
core:100000:65536
root:200000:268435456
sh-4.4# cat /etc/subgid
core:100000:65536
root:200000:268435456</code></pre>
<p>Looks good!</p>
<h2 id="creating-a-user-namespaced-pod---attempt-2">Creating a user namespaced pod - Attempt 2</h2>
<p>It’s time to create the user namespaced pod again, and see if it succeeds this time.</p>
<pre><code>$ oc --as test create -f userns-test.yaml
pod/userns-test created</code></pre>
<p>Unfortunately, the same <code>FailedCreatePodSandBox</code> error occurred. My <code>subuid</code> remedy was either incorrect, or insufficient. I decided to use a debug shell on the worker node to examine the system journal. I searched for the error string <code>could not find enough available IDs</code>, and found the error in the output of the <code>hyperkube</code> unit. A few lines above that, there are some <code>crio</code> log messages, including:</p>
<pre><code>Cannot find mappings for user \&quot;containers\&quot;: No subuid
ranges found for user \&quot;containers\&quot; in /etc/subuid&quot;</code></pre>
<p>So, my mistake was defining ID map ranges for the <code>root</code> user. I should have used the <code>containers</code> user. I fixed the <code>MachineConfig</code> definition to use the file content:</p>
<pre><code>core:100000:65536
containers:200000:268435456</code></pre>
<p>Then I replaced the <code>subuid-subgid</code> object and again waited for Machine Config Operator to update the worker nodes.</p>
<h2 id="creating-a-user-namespaced-pod---attempt-3">Creating a user namespaced pod - Attempt 3</h2>
<p>Once again, the container remained at <code>ContainerCreating</code>. But the error was different (lines wrapped for readability):</p>
<pre><code>Failed to create pod sandbox: rpc error:
code = Unknown
desc = container create failed:
  time=&quot;2020-12-01T06:40:49Z&quot;
  level=warning
  msg=&quot;unable to terminate initProcess&quot;
  error=&quot;exit status 1&quot;

time=&quot;2020-12-01T06:40:49Z&quot;
level=error
msg=&quot;container_linux.go:366: starting container process caused:
  process_linux.go:472: container init caused:
    write sysctl key net.ipv4.ping_group_range:
      write /proc/sys/net/ipv4/ping_group_range: invalid argument&quot;</code></pre>
<p>After a bit of research, here is my understanding of the situation: CRI-O successfully created the pod sandbox (which includes the user namespace) and is now initialising it. One of the initialisation steps is to set the <code>net.ipv4.ping_group_range</code> sysctl (the subroutine is part of <code>runc</code>), and this is failing. This step is performed for all pods, but it is only failing when the pod is using a user namespace.</p>
<h2 id="net.ipv4.ping_group_range-and-user-namespaces"><code>net.ipv4.ping_group_range</code> and user namespaces</h2>
<p>The <code>net.ipv4.ping_group_range</code> sysctl defines the range of group IDs that are allowed to send ICMP Echo packets. Setting it to the full gid range allows <code>ping</code> to be used in rootless containers, without setuid or the <code>CAP_NET_ADMIN</code> and <code>CAP_NET_RAW</code> capabilities.</p>
<p>The CRI-O config key <code>crio.runtime.default_sysctls</code> declares the default sysctls that will be set in all containers. The default OpenShift CRI-O configuration sets it to the full gid range:</p>
<pre><code>sh-4.4# cat /etc/crio/crio.conf.d/00-default \
    | grep -A2 default_sysctls
default_sysctls = [
    &quot;net.ipv4.ping_group_range=0 2147483647&quot;,
]</code></pre>
<p>My working hypothesis is that setting the sysctl in the user-namespaced container fails because the gid range in the sandbox is not <code>0–2147483647</code> but much smaller. This could explain the <code>invalid argument</code> part of the error message.</p>
<p>How to overcome this? I first thought to update the pod spec to specify a different value for the sysctl that reflects the actual gid range in the sandbox. And to do that, I have to calculate what that gid range is.</p>
<h3 id="computing-the-gid-range">Computing the gid range</h3>
<p>I will work on the assumption that I must refer to the range as it appears <em>in the namespace</em>. That assumption could be wrong, but that’s where I’m starting.</p>
<p>Because I am using <code>map-to-root=true</code>, the start value of the range should be <code>0</code>. The second number in the <code>ping_group_range</code> sysctl value is not the range size but the end gid (inclusive). CRI-O currently hard-codes a default user namespace size of <code>65536</code>.</p>
<p>Because the size of the uid range is a critical parameter, I shall from now on explicitly declare the desired size in the <code>userns-mode</code> annotation. This will protect the solution from change to the default range size. I probably won’t need 65536 uids/gids but I’ll stick with the default for now.</p>
<pre><code>io.kubernetes.cri-o.userns-mode: &quot;auto:size=65536;map-to-root=true&quot;</code></pre>
<p>With a range of <code>65536</code> starting at <code>0</code>, the desired sysctl setting is <code>net.ipv4.ping_group_range=0 65535</code>.</p>
<h3 id="configuring-the-sysctl">Configuring the sysctl</h3>
<p>We need <code>ping</code> to continue working in containers that are not namespaced. Therefore, overriding or clearing the CRI-O <code>default_sysctls</code> config is not an option. Instead I need a way to optionally set the <code>net.ipv4.ping_group_range</code> sysctl to a specified value on a per-pod basis.</p>
<p>You can specify sysctls to be set in a pod via the <code>spec.securityContext.sysctls</code> array (see Kubernetes <a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#podsecuritycontext-v1-core">PodSecurityContext documentation</a>). I updated the pod definition to include the sysctl:</p>
<pre><code>$ cat userns-test.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: userns-test
  annotations:
    openshift.io/scc: restricted
    io.kubernetes.cri-o.userns-mode: &quot;auto:size=65536;map-to-root=true&quot;
spec:
  containers:
  - name: userns-test
    image: freeipa/freeipa-server:fedora-31
    command: [&quot;sleep&quot;, &quot;3601&quot;]
  securityContext:
    sysctls:
    - name: &quot;net.ipv4.ping_group_range&quot;
      value: &quot;0 65535&quot;</code></pre>
<p>As I write this, I don’t know yet how CRI-O behaves when both <code>default_sysctls</code> and the pod spec define the same sysctl. It might just set the value from the pod spec, which is the behaviour I need. Or it might first attempt to set the value from <code>default_sysctls</code>, and afterwards set it again to the value from the pod spec (this will fail as before).</p>
<p>Time to find out!</p>
<h2 id="creating-a-user-namespaced-pod---attempt-4">Creating a user namespaced pod - Attempt 4</h2>
<pre><code>$ oc --as test create -f userns-test.yaml
pod/userns-test created

# ... wait ...

$ oc get pod userns-test
NAME          READY   STATUS                 RESTARTS   AGE
userns-test   0/1     CreateContainerError   0          118s</code></pre>
<p>OK, progress was made! It did not get stuck at <code>ContainerCreating</code>; this time we got a <code>CreateContainerError</code>. This means that the CRI-O sysctl behaviour is what we were hoping for. As for the new error, <code>oc describe</code> gave the detail:</p>
<pre><code>Error: container create failed:
time=&quot;2020-12-01T12:38:45Z&quot;
level=error
msg=&quot;container_linux.go:366: starting container process caused:
  setup user: cannot set uid to unmapped user in user namespace&quot;</code></pre>
<p>My guess is that CRI-O is ignoring the fact that the pod is in a user namespace and is attempting to execute the process using the same uid as it would if the pod were not in a user namespace. The uid is outside the mapped range (<code>0</code>–<code>65535</code>). For my next attempt I will add <code>runAsUser</code> and <code>runAsGroup</code> to the <code>securityContext</code>.</p>
<p>But first some other quick notes and observations. First of all, a user namespace was indeed created for this pod!</p>
<pre><code>sh-4.4# lsns -t user
        NS TYPE  NPROCS    PID USER   COMMAND
4026531837 user     277      1 root   /usr/lib/systemd/systemd --switched-root --system --deserialize 16
4026532599 user       1 684279 200000 /usr/bin/pod</code></pre>
<p>We can examine the uid and gid maps for the namespace:</p>
<pre><code>sh-4.4# cat /proc/684279/uid_map
         0     200000      65536

sh-4.4# cat /proc/684279/gid_map
         1     200001      65535
         0 1000610000          1</code></pre>
<p>It surprised me that gid <code>0</code> is mapped to system user <code>1000610000</code>. I don’t know what consequences this might have; for now I am just noting it.</p>
<p>Because the pod sandbox does exist, I also decided to see if I could get a debug shell:</p>
<pre><code>$ oc debug pod/userns-test
Starting pod/userns-test-debug, command was: sleep 3601
Pod IP: 10.129.3.170
If you don&#39;t see a command prompt, try pressing enter.
sh-5.0$ id
uid=1000610000(1000610000) gid=0(root) groups=0(root),1000610000</code></pre>
<p>It worked! But the debug shell cannot be running in the user namespace; the uid (<code>1000610000</code>) is too high. Running <code>lsns</code> in my worker node debug shell confirms it; the namespace still has only one process running in it:</p>
<pre><code>sh-4.4# lsns -t user
        NS TYPE  NPROCS    PID USER   COMMAND
4026531837 user     282      1 root   /usr/lib/systemd/systemd --switched-root --system --deserialize 16
4026532599 user       1 684279 200000 /usr/bin/pod</code></pre>
<h2 id="creating-a-user-namespaced-pod---attempt-5">Creating a user namespaced pod - Attempt 5</h2>
<p>I once again deleted the <code>userns-test</code> pod. As proposed above, I modified the pod security context to specify that the entry point should be run as uid <code>0</code> and gid <code>0</code>:</p>
<pre><code>$ cat userns-test.yaml
apiVersion: v1
kind: Pod
metadata:
  name: userns-test
  annotations:
    openshift.io/scc: restricted
    io.kubernetes.cri-o.userns-mode: &quot;auto:size=65536;map-to-root=true&quot;
spec:
  containers:
  - name: userns-test
    image: freeipa/freeipa-server:fedora-31
    command: [&quot;sleep&quot;, &quot;3601&quot;]
  securityContext:
    runAsUser: 0
    runAsGroup: 0
    sysctls:
    - name: &quot;net.ipv4.ping_group_range&quot;
      value: &quot;0 65535&quot;</code></pre>
<p>Here we go:</p>
<pre><code>$ oc --as test create -f userns-test.yaml
Error from server (Forbidden): error when creating
&quot;userns-test.yaml&quot;: pods &quot;userns-test&quot; is forbidden: unable to
validate against any security context constraint:
[spec.containers[0].securityContext.runAsUser: Invalid value: 0:
must be in the ranges: [1000610000, 1000619999]]</code></pre>
<p><em>sad trombone</em></p>
<p>I don’t have a clear idea how I could proceed. The security context constraint (SCC) is prohibiting the use of uid <code>0</code> for the container process. Switching to a permissive SCC might allow me to proceed, but it would also mean using a more privileged OpenShift user account. Then that privileged account could then create containers running as <code>root</code> <em>in the system user namespace</em>. We want user namespaces in OpenShift so that we can <em>avoid</em> this exact scenario. So resorting to a permissive SCC (e.g. <code>anyuid</code>) feels like the wrong way to go.</p>
<p>It could be that it’s the only way to go for now, and that more nuanced security policy mechanisms must be implemented before user namespaces can be used in OpenShift to achieve the security objective. In any case, I’ll be reaching out to other engineers and OpenShift experts for their suggestions.</p>
<p>For now, I’m calling it a day! See you soon for the next episode.</p>]]></summary>
</entry>
<entry>
    <title>Using the OpenShift Machine Config Operator</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2020-11-30-openshift-machine-config-operator.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2020-11-30-openshift-machine-config-operator.html</id>
    <published>2020-11-30T00:00:00Z</published>
    <updated>2020-11-30T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="using-the-openshift-machine-config-operator">Using the OpenShift Machine Config Operator</h1>
<p>In a <a href="2020-11-05-openshift-user-namespace.html">recent post</a> I discussed how OpenShift and Kubernetes do not have user namespace isolation. An <a href="https://github.com/cri-o/cri-o/pull/3944">upcoming CRI-O enhancement</a> should allow pods to be run in separate user namespaces. This feature is controlled via <em>annotations</em>; no explicit Kubernetes support is required.</p>
<p>To experiment with this feature I deployed an OpenShift nightly (4.7) cluster, which uses a CRI-O v1.20 prerelease build. But having CRI-O v1.20 is not enough. The feature must be explicitly enabled in the CRI-O configuration. This leads to the question, <em>what is the proper way to manage machine configuration in an OpenShift cluster?</em> The answer is the <em>Machine Config Operator (MCO)</em>.</p>
<p>The <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.6/html/post-installation_configuration/post-install-machine-configuration-tasks">official OpenShift documentation</a> does a good job of introducing and explaining the MCO, so there’s no need to regurgitate it all here. Instead I’ll review the configuration, object definitions and procedure from my CRI-O use case.</p>
<h2 id="configuring-cri-o-via-the-machine-config-operator">Configuring CRI-O via the Machine Config Operator</h2>
<p>CRI-O is configured via <code>/etc/crio/crio.conf</code> and additional files in the <code>/etc/crio/crio.conf.d/</code> directory. Directives from <code>crio.conf.d</code> files have higher precedence and files are processed in lexicographic order.</p>
<p>The follow configuration enables the user namespaces feature:</p>
<pre><code>[crio.runtime.runtimes.runc]
allowed_annotations=[&quot;io.kubernetes.cri-o.userns-mode&quot;]</code></pre>
<p>I used MCO to drop that configuration snippet into the file <code>/etc/crio/crio.conf.d/99-crio-userns.conf</code>. First I needed the base64 encoding of the configuration content:</p>
<pre><code>$ base64 --wrap=0 &lt;&lt;EOF
[crio.runtime.runtimes.runc]
allowed_annotations=[&quot;io.kubernetes.cri-o.userns-mode&quot;]
EOF
W2NyaW8ucnVudGltZS5ydW50aW1lcy5ydW5jXQphbGxvd2VkX2Fubm90YXRpb25zPVsiaW8ua3ViZXJuZXRlcy5jcmktby51c2VybnMtbW9kZSJdCg==</code></pre>
<p>Next I created <code>machineconfig-crio-userns.yaml</code>. This defines a <code>MachineConfig</code>, the primary resource type handled by the MCO. The base64 output from above is used in this file.</p>
<pre><code>apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: crio-userns
spec:
  config:
    ignition:
      version: 3.1.0
    storage:
      files:
      - path: /etc/crio/crio.conf.d/99-crio-userns.conf
        overwrite: true
        contents:
          source: data:text/plain;charset=utf-8;base64,W2NyaW8ucnVudGltZS5ydW50aW1lcy5ydW5jXQphbGxvd2VkX2Fubm90YXRpb25zPVsiaW8ua3ViZXJuZXRlcy5jcmktby51c2VybnMtbW9kZSJdCg==</code></pre>
<p>Note that the examples in the official documentation contain a lot of extraneous fields that can be omitted. <code>MachineConfig</code> objects use the <em>Ignition</em> configuration format. Read the <a href="https://github.com/coreos/ignition/blob/master/docs/configuration-v3_1.md">Ignition Configuration Specification</a> to see what fields are available or required (or not) for your use case.</p>
<p>There are just a few things about this <code>MachineConfig</code> that I’d like to highlight.</p>
<ul>
<li>For creating files, the <code>mode</code> field allows specifying the file access permissions. The default is <code>420</code> (<em>decimal!</em>, equivalent to <code>0644</code>); this was suitable for my use case so I omitted it. But there may be many cases where the default is not suitable and it will be necessary to specify the <code>mode</code>.</li>
<li>This config only needs to be applied on worker nodes. The <code>machineconfiguration.openshift.io/role: worker</code> label accomplishes this. The value <code>master</code> can be used for master-only configurations.</li>
<li>The file content is specified via a <a href="https://tools.ietf.org/html/rfc2397">"data" URI</a>. Other supported schemes include <code>https</code>, <code>s3</code> and <code>tftp</code>.</li>
</ul>
<p>Next I created the <code>MachineConfig</code> object:</p>
<pre><code>$ oc create -f machineconfig-crio-userns.yaml
machineconfig.machineconfiguration.openshift.io/crio-userns created</code></pre>
<p>Over the next several minutes, the Machine Config Operator applied the configuration change to all the worker nodes and restarted them.</p>
<h2 id="closing-thoughts">Closing thoughts</h2>
<p>Everything went smoothly and my impressions of MCO, from this first “hands on” experience, are very positive. It was a simple use case, I admit. But I am still very pleased that it was so easy and everything Just Worked. Hopefully other people have as good an experience with MCO as I did, even for more complex configuration changes.</p>]]></summary>
</entry>
<entry>
    <title>ACME Service Discovery</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2020-11-13-acme-service-discovery.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2020-11-13-acme-service-discovery.html</id>
    <published>2020-11-13T00:00:00Z</published>
    <updated>2020-11-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="acme-service-discovery">ACME Service Discovery</h1>
<p>Automated Certificate Management Environment (ACME) is a protocol for automated identifer validation certificate issuance. Over the past five years it gained widespread adoption thanks to <a href="https://letsencrypt.org/">Let's Encrypt</a>, the first publicly trusted CA that implemented it. ACME is supported by a plethora of server programs and service providers, Let’s Encrypt has now issued over <a href="https://letsencrypt.org/2020/02/27/one-billion-certs.html">1 billion certificates</a> and together with the ACME protocol itself is largely responsible for pushing the adoption of TLS from around 50% of page loads five years ago to well over 80% today. This is an amazing result!</p>
<p>So it’s no surprise that the ACME ecosystem is growing. Some other publicly trusted CAs now support the ACME protocol. Enterprise CAs are learning how to speak ACME. This includes <a href="https://www.dogtagpki.org/wiki/ACME">Dogtag</a>, and by extension FreeIPA. The upcoming FreeIPA 4.9 release will support ACME (I <a href="2020-05-06-ipa-acme-intro.html">blogged about this</a> a few months ago).</p>
<p>Having proved itself good for DNS certificates, <a href="https://tools.ietf.org/html/rfc8738">RFC 8738</a> introduced supported for IP addresses. Work to support email addresses (for S/MIME), <code>.onion</code> addresses (Tor services), and other identifer types is underway in the IETF <a href="https://datatracker.ietf.org/wg/acme/documents/">acme Working Group</a>. (ACME itself is defined in <a href="https://tools.ietf.org/html/rfc8555">RFC 8555</a>).</p>
<p>The outcome of all of this is that already today, and increasingly into the future, network environments will often have access to multiple ACME servers. These servers may differ in the kinds of certificates they issue and the validation methods (also called “challenge types”) they support. Also, it is desirable that a client (e.g. a printer or an IoT “thing”) would be able to opportunistically and automatically locate a suitable ACME server to acquire certificates without any operator (human or otherwise) intervention (and Let’s Encrypt or other public ACME servers may not be accessible in some environments).</p>
<p>So, what’s an ACME client to do?</p>
<h2 id="internet-draft">Internet-Draft</h2>
<p>I have <a href="https://datatracker.ietf.org/doc/draft-tweedale-acme-discovery/">published an Internet-Draft</a> defining a service discovery protocol for ACME. <em>Internet-Draft</em> is <a href="https://www.ietf.org/">IETF</a> jargon for a work-in-progress document that might one day become an <a href="https://www.ietf.org/standards/rfcs/">RFC</a>. An outline of how ACME Service Discovery works follows.</p>
<p>ACME Service Discovery is a profile of <em>DNS-based Service Discovery (DNS-SD)</em> (<a href="https://tools.ietf.org/html/rfc6763">RFC 6763</a>). Given a <em>parent domain</em>, <em>Service Instance Names</em> are listed by the PTR records of <code>_acme-server._tcp.$PARENT</code>. For example, the <code>corp.example.</code> parent domain advertises two service instances called <code>CorpCA</code> and <code>C4A</code>:</p>
<pre><code>$ORIGIN corp.example.

_acme-server._tcp PTR CorpCA._acme-server._tcp
_acme-server._tcp PTR C4A._acme-server._tcp</code></pre>
<p>Each Service Instance Name owns an SRV and TXT record that together describe the location, priority and capabilities of the server, as well as the path to the ACME directory object. Continuing with the example, <code>CorpCA</code> has the higher priority and supports the <code>ip</code> and <code>dns</code> identifer types, whereas <code>C4A</code> has a lower priority and only supports <code>dns</code> identifiers:</p>
<pre><code>$ORIGIN corp.example.

CorpCA._acme-server._tcp SRV 10 0 443 ca
CorpCA._acme-server._tcp TXT &quot;path=/acme&quot; &quot;i=ip,dns&quot;

C4A._acme-server._tcp    SRV 20 0 443 certs4all.example.
C4A._acme-server._tcp    TXT &quot;path=/acme/v2&quot; &quot;i=dns&quot;</code></pre>
<p>ACME clients are assumed to know (or deduce) one or more candidate parent domains. Possible sources for the candidate parent domain(s) are the DNS search domains, host FQDN or Kerberos realm. The client performs ACME Service Discovery on each parent domain, selecting and probing eligible service instances, until they find one that works. The probe step involves constructing a URL from the SRV target and port and TXT <code>path</code> attribute, performing an HTTP GET request for that resource, and checking that the response is a valid ACME directory object. In the example above, the directory URL for <code>CorpCA</code> is <code>https://ca.corp.example/acme</code>.</p>
<p>And that’s the main idea! There’s a fair bit more detail in the Internet-Draft but I won’t belabour it all here.</p>
<h2 id="enabling-acme-service-discovery-in-freeipa">Enabling ACME Service Discovery in FreeIPA</h2>
<p>To enable ACME Service Discovery in a FreeIPA environment using the integrated DNS service, add the PTR, SRV and TXT records for each service instance. This requires a <a href="https://github.com/freeipa/freeipa/pull/5239">recently merged patch</a> to allow PTR records to be created in arbitrary zones (PTR records were previously limited to <code>.arpa</code> reverse zones). The fix should be included in FreeIPA 4.9 and will also be backported to the 4.8.x branch.</p>
<p>The following DNS records advertise the FreeIPA CA itself:</p>
<pre><code>% ipa dnsrecord-add ipa.local ipa._acme-server._tcp \
    --srv-priority 10 --srv-weight 0 \
    --srv-port 443 --srv-target ipa-ca \
    --txt-rec &#39;&quot;path=/acme/directory&quot; &quot;i=dns&quot;&#39;
  Record name: ipa._acme-server._tcp
  SRV record: 10 0 443 ipa-ca
  TXT record: &quot;path=/acme/directory&quot; &quot;i=dns&quot;

% ipa dnsrecord-add ipa.local _acme-server._tcp \
    --ptr-rec &quot;ipa._acme-server._tcp.ipa.local.&quot;
  Record name: _acme-server._tcp
  PTR record: ipa._acme-server._tcp.ipa.local.</code></pre>
<p>The procedure to advertise additional ACME servers is similar.</p>
<p>If the ACME Service Discovery proposal gets traction we would ideally create these records to advertise the FreeIPA CA automatically (when it is enabled).</p>
<h2 id="certbot-plugin">Certbot plugin</h2>
<p>I wrote a <a href="https://certbot.eff.org/">Certbot</a> plugin to experiment with service discovery. It lives in a private branch at <a href="https://github.com/frasertweedale/certbot/tree/feature/discovery">https://github.com/frasertweedale/certbot/tree/feature/discovery</a>. I will probably submit a pull request soon, to invite feedback about the implementation and the service disovery proposal itself.</p>
<p>To install Certbot and the plugin under <code>~/.local/</code> (command output omitted):</p>
<pre><code># git clone https://github.com/certbot/certbot -b feature/discovery
# cd certbot/certbot
# pip install --user .
# cd ../certbot-discovery
# pip install --user .</code></pre>
<p>Run <code>certbot plugins</code> to verify that the plugin is installed:</p>
<pre><code># certbot plugins
Saving debug log to /var/log/letsencrypt/letsencrypt.log

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
* discovery
Description: ACME Service Discovery
Interfaces: IPlugin
Entry point: discovery = certbot_discovery:ACMEServiceDiscovery

* standalone
Description: Spin up a temporary webserver
Interfaces: IAuthenticator, IPlugin
Entry point: standalone = certbot._internal.plugins.standalone:Authenticator

* webroot
Description: Place files in webroot directory
Interfaces: IAuthenticator, IPlugin
Entry point: webroot = certbot._internal.plugins.webroot:Authenticator
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -</code></pre>
<p>Now register an account with the ACME server. Note the <code>--discovery</code> option:</p>
<pre><code># certbot --discovery register \
  --email ftweedal@redhat.com \
  --agree-tos --no-eff-email
Saving debug log to /var/log/letsencrypt/letsencrypt.log
Account registered.</code></pre>
<p>If service discovery fails, it will fail silently and use Let’s Encrypt (Certbot’s default). <code>--discovery=force</code> suppresses this fallback behaviour; if service discovery fails Certbot will abort.</p>
<p>Next request the certificate:</p>
<pre><code># certbot --discovery certonly \
    --domain $(hostname) --standalone
Saving debug log to /var/log/letsencrypt/letsencrypt.log
Plugins selected: Authenticator standalone, Installer None
Obtaining a new certificate
Performing the following challenges:
http-01 challenge for f33-0.ipa.local
Waiting for verification...
Cleaning up challenges

IMPORTANT NOTES:
 - Congratulations! Your certificate and chain have been saved at:
   /etc/letsencrypt/live/f33-0.ipa.local/fullchain.pem
   Your key file has been saved at:
   /etc/letsencrypt/live/f33-0.ipa.local/privkey.pem
   Your cert will expire on 2021-02-10. To obtain a new or tweaked
   version of this certificate in the future, simply run certbot
   again. To non-interactively renew *all* of your certificates, run
   &quot;certbot renew&quot;
 - If you like Certbot, please consider supporting our work by:

   Donating to ISRG / Let&#39;s Encrypt:   https://letsencrypt.org/donate
   Donating to EFF:                    https://eff.org/donate-le</code></pre>
<p>We can check that the certificate was issued by the FreeIPA CA, not Let’s Encrypt:</p>
<pre><code># openssl x509 -issuer -noout  \
    &lt; /etc/letsencrypt/live/f33-0.ipa.local/fullchain.pem
issuer=O = IPA.LOCAL 202011061623, CN = Certificate Authority</code></pre>
<p>You do have to supply the <code>--discovery</code> option to both the <code>register</code> and <code>certonly</code> commands (otherwise <code>certonly</code> will try to use Let’s Encrypt). Fortunately, for <em>renewal</em> (the <code>renew</code> command) Certbot does remember which server issued the certificate, and uses the same server for renewal.</p>
<p>What happens when service discovery fails? I’ll disable the ACME service on the FreeIPA server:</p>
<pre><code>% sudo ipa-acme-manage disable
The ipa-acme-manage command was successful</code></pre>
<p>Then, running <code>certbot register</code> again, this time with <code>--discovery=force</code> to prevent fallback to Let’s Encrypt:</p>
<pre><code># certbot --discovery=force register \
  --email ftweedal@redhat.com \
  --agree-tos --no-eff-email
usage:
  certbot [SUBCOMMAND] [options] [-d DOMAIN] [-d DOMAIN] ...

Certbot can obtain and install HTTPS/TLS/SSL certificates.  By default,
it will attempt to use a webserver both for obtaining and installing the
certificate.
certbot: error: service discovery failed (see /tmp/tmp6qq8pnks for info)</code></pre>
<p>The log file contains a transcript of the service discovery plugin’s activity:</p>
<pre><code># cat /tmp/tmp6qq8pnks
[INFO] processing parent domain ipa.local.
[INFO] enumerating service instances for _acme-server._tcp.ipa.local.
[INFO]   found service instances: [&lt;DNS name ipa._acme-server._tcp.ipa.local.&gt;]
[INFO] resolving service instance ipa._acme-server._tcp.ipa.local.
[INFO]   (&lt;DNS IN SRV rdata: 10 0 443 ipa-ca.ipa.local.&gt;, (b&#39;path=/acme/directory&#39;, b&#39;i=dns&#39;))
[INFO] eligible service instances:
[INFO]   (&lt;DNS IN SRV rdata: 10 0 443 ipa-ca.ipa.local.&gt;, (b&#39;path=/acme/directory&#39;, b&#39;i=dns&#39;))
[INFO] GET https://ipa-ca.ipa.local/acme/directory
[WARNING] failed to reach server: &lt;Response [503]&gt;</code></pre>
<p>We can see that the plugin found the service instance and requested the directory resource, but got a 503 response (as expected). So, when service discovery fails the plugin gives you some useful log output to debug the issue.</p>
<p>The log file is only persisted when service discovery fails, otherwise it is deleted. In the current implementation we cannot write to the “normal” Certbot log file because we don’t know where that is. The discovery plugin is actually doing all its work <em>inside the argument parsing</em>. It feels like a brutal hack but it’s the only way I found (in the limited time I had) to override the <code>--server</code> option whilst keeping the implementation as a plugin, fully separate from Certbot core. A nicer implementation is possible if service discovery were to be implemented in Certbot core (this would introduce a dependency on <em>dnspython</em>).</p>
<h2 id="next-steps">Next steps</h2>
<p>I will present and demo this proposal during the <code>acme</code> Working Group meeting at IETF 109 (November 2020). From there I hope that it will be adopted, developed, and shepherded through to become an RFC. I will also seek feedback from Certbot developers about the proposal and my experimental implementation.</p>
<p>I also intend to submit another Internet-Draft proposing a mechanism for servers to advertise their capabilities in the ACME directory object. This could be useful to help clients choose from multiple servers (regardless of how they find out about the servers). And I think it’s good practice. When a protocol has many possible features that a server may or may not implement, servers should declare their capabilities for the benefit of clients.</p>
<p>Beyond that, I am starting to think about SRVName support in ACME. This would be useful in enterprise environments and on the open internet for protocols where SRV records are used to locate servers. Such protocols include Kerberos, LDAP, SIP and XMPP.</p>]]></summary>
</entry>
<entry>
    <title>OpenShift and user namespaces</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2020-11-05-openshift-user-namespace.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2020-11-05-openshift-user-namespace.html</id>
    <published>2020-11-05T00:00:00Z</published>
    <updated>2020-11-05T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="openshift-and-user-namespaces">OpenShift and user namespaces</h1>
<p>FreeIPA in its current form is very much not a “cloud native” application. Likewise the current FreeIPA container, which runs all the required services under systemd. My current team is working on operationalising FreeIPA for the OpenShift container platform. Our initial efforts are focused around this “monolithic” container, trying to get it to run in OpenShift, securely. Although we recognise we may eventually need to split up the container, it will be a major engineering effort to do so. We want to have a working proof of concept as early as possible, so that we (and others) can start the important integration work (e.g. with Keycloak / RHSSO).</p>
<p>This “lift and shift” of a complex traditional application to OpenShift results in a container that needs to run several processes as a variety of users, including <code>root</code>. OpenShift isolates containers (actually pods, which consist of one or more containers) in their own PID namespace. This is good, but if we are to run container processes as <code>root</code> (in the container), we do not want them to also be <code>root</code> on the host. Rather, they should map to an unprivileged account. If we want secure multitenancy of multiple IDM servers on a single worker node, we want the user accounts in different IDM pods to map to disjoint sets of unprivileged users on the host.</p>
<p>Linux <code>user_namespaces(7)</code> provide this kind of isolation. To what extent are user namespaces supported in OpenShift? We needed to find out, in order to decide how to proceed with the FreeIPA OpenShift effort. In this blog post I discuss my investigation and findings.</p>
<h2 id="investigating-current-openshift-behaviour">Investigating current OpenShift behaviour</h2>
<p>To investigate the use (or not) of user namespaces I deployed pods on our team’s OpenShift cluster, running a simple command, and observed the effects on the worker node.</p>
<p>As cluster admin, I created a new project:</p>
<pre><code>% oc new-project test
Now using project &quot;test&quot; on server &quot;https://api.permanent.idmocp.lab.eng.rdu2.redhat.com:6443&quot;.
...</code></pre>
<p>To avoid the cluster admin user’s SCC bindings applying to pod creation, I created a user named <code>test</code> and granted it the <em>project</em> (not cluster) <code>admin</code> role. Subsequent pod creation operations were performed as user <code>test</code>.</p>
<pre><code>% oc create user test
user.user.openshift.io/test created

% oc adm policy add-role-to-user admin test
clusterrole.rbac.authorization.k8s.io/admin added: &quot;test&quot;</code></pre>
<p>Next I deployed a basic pod (as user <code>test</code>) and inspected it to find out which worker node it was scheduled on, and the CRI-O conatiner ID:</p>
<pre><code>% cat pod-test.yaml
apiVersion: v1
kind: Pod
metadata:
  name: test
spec:
  containers:
  - name: idm-test
    image: freeipa/freeipa-server:fedora-31
    command: [&quot;sleep&quot;, &quot;3600&quot;]

% oc --as test create -f pod-test.yaml
pod/test created

% oc get -o json pod test \
    | jq .spec.nodeName
&quot;permanent-bdd7p-worker-9r4b6&quot;

% oc get -o json pod test \
    | jq &quot;.status.containerStatuses[0].containerID&quot;
&quot;cri-o://a9c0cf0ac9c0c352b82a74cccf830dfa8c33aae28138808eb7bdd9d53aae2d1f&quot;</code></pre>
<p>Next, opening a debug shell on the worker node I inspected the container to find out the PID:</p>
<pre><code>% oc debug node/permanent-bdd7p-worker-9r4b6
Starting pod/permanent-bdd7p-worker-9r4b6-debug ...
To use host binaries, run `chroot /host`
Pod IP: 10.8.3.215
If you don&#39;t see a command prompt, try pressing enter.
sh-4.2# chroot /host
sh-4.4# crictl inspect a9c0cf0ac | jq .pid
1311115</code></pre>
<p>Next I looked at which user the process is running under, and the UID map of the process:</p>
<pre><code>sh-4.4# ls -l -d /proc/1311115
dr-xr-xr-x. 9 1000620000 root 0 Nov  5 05:34 /proc/1311115

sh-4.4# cat /proc/1311115/uid_map
         0          0 4294967295</code></pre>
<p>The process was running as user <code>1000620000</code>, and UID map has an offset of <code>0</code> and a size of <code>2^32</code>. Which is to say, this process is running in the same user namespace as the host. We can use the <code>lsns</code> command to confirm that everything on this node–including all container processes–shares the single user namespace:</p>
<pre><code>sh-4.4# lsns -t user
        NS TYPE  NPROCS PID USER COMMAND
4026531837 user     296   1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 18</code></pre>
<p>As a result, if we use <code>runAsUser</code> to specify a different user under which to run the container, the container will run as the specified user both in the container <strong>and on the host</strong>. The following transcript demonstrates this.</p>
<p>Delete the pod <code>test</code>:</p>
<pre><code>% oc delete pod test
pod &quot;test&quot; deleted</code></pre>
<p>Add the <code>anyuid</code> SCC to user <code>test</code>:</p>
<pre><code>% oc adm policy add-scc-to-user anyuid test
securitycontextconstraints.security.openshift.io/anyuid added to: [&quot;test&quot;]</code></pre>
<p>Create the pod (as user <code>test</code>):</p>
<pre><code>% oc --as test create -f pod-test.yaml
pod/test created</code></pre>
<p>Following the same procedure as earlier, find the PID (it was <code>1381728</code>) and observe that it is running as <code>root</code> (UID <code>0</code>) on the host:</p>
<pre><code>sh-4.4# ls -l -d /proc/1381728
dr-xr-xr-x. 9 root root 0 Nov  5 05:55 /proc/1381728</code></pre>
<h2 id="consequences-for-freeipa">Consequences for FreeIPA</h2>
<p>Traditional applications sometimes assume they will run as <code>root</code> or some other “reserved” user. FreeIPA is such a case. Likewise, running systemd in a container means running as UID 0 (from the container’s point of view).</p>
<p>The lack of user namespace use in OpenShift means that for a process to run under a particular UID in the container, it must run as that user on the host too. If you application needs to be <code>root</code>, it will be <code>root</code> on the host. Other kinds of namespaces (e.g. <code>pid</code>, <code>mnt</code>, <code>uts</code> among others) do mitigate the security risk. But if a rogue process can escalate privileges and escape the other sandbox(es) the result could be catastrophic.</p>
<p>FreeIPA, being composed of many components, some of which are large complex projects in their own right, and several of which are implemented in C or leverage C libraries, has a large attack surface. In the absense of user namespaces the risk of container host or co-tenant compromise—even by accident—seems high.</p>
<p>This all assumes that containers do not have user namespace isolation and that FreeIPA continues to require running processes in the FreeIPA container as fixed UIDs (probably including <code>root</code>). I will now discuss possible ways to eliminate these assumptions.</p>
<h2 id="user-namespace-support-in-kubernetes">User namespace support in Kubernetes</h2>
<p>OpenShift is built on the Kubernetes container platform. <em>Kubernetes Enhancement Proposal</em> <a href="https://github.com/kubernetes/enhancements/issues/127">KEP-127</a> proposes user namespace support. The ticket has been open for 4 years and has since seen several efforts to formalise the proposal, the most recent of which is <a href="https://github.com/kubernetes/enhancements/pull/2101">kubernetes/enhancements#2101</a> (<a href="https://github.com/kubernetes/enhancements/blob/9726c1a4cc5051d8be7eaf4cb64313df60ae8751/keps/sig-node/127-usernamespaces-support/README.md">rendered</a>). There have also been several experimental implementations (e.g. <a href="https://github.com/kubernetes/kubernetes/pull/55707">#55707</a>, <a href="https://github.com/kubernetes/kubernetes/pull/64005">#64005</a>), none of which was accepted (yet).</p>
<p>There has been a recent resurgence of activity on this KEP, and related discussions and pull requests. But that has happened before. I believe that every new (or resurrected) discussion or experiment can move you closer to the goal, and that there can be several false starts before things happen. Maybe this time it will happen… or maybe not.</p>
<p>Right now there is no final proposal and no implementation plan. As a team we cannot proceed on the assumption that Kubernetes will support user namespaces. We will certainly present our case to OpenShift engineering internally at Red Hat, but we have to look at other options.</p>
<h2 id="user-namespace-support-in-cri-o">User namespace support in CRI-O</h2>
<p>The <a href="https://cri-o.io/">CRI-O</a> container runtime <a href="https://github.com/cri-o/cri-o/pull/3944">recently implemented</a> support for running each pod in a separate user namespace, via <em>annotations</em> on the pod, e.g.:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb11-1"><a href="#cb11-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb11-2"><a href="#cb11-2"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb11-3"><a href="#cb11-3"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb11-4"><a href="#cb11-4"></a><span class="at">  </span><span class="fu">annotations</span><span class="kw">:</span></span>
<span id="cb11-5"><a href="#cb11-5"></a><span class="at">    </span><span class="fu">io.kubernetes.cri-o.userns-mode</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;auto&quot;</span></span>
<span id="cb11-6"><a href="#cb11-6"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb11-7"><a href="#cb11-7"></a><span class="at">  ...</span></span></code></pre></div>
<p>Using annotations means that no explicit support in Kubernetes is required. All that is required is that Kubernetes is using the CRI-O container runtime, and that CRI-O is configured to enable this feature. OpenShift 4.x does use CRI-O, so we’re halfway there. The remaining step is to enable the feature in <code>crio.conf</code>:</p>
<pre><code>allow_userns_annotation = true</code></pre>
<p>The developer Giuseppe Scrivano kindly published a <a href="https://asciinema.org/a/351396">screencast showing the feature in action</a> (2 minutes). This feature is not yet in a supported release but is available on the v1.20 branch and is included in OpenShift <a href="">nightly builds</a>.</p>
<h2 id="splitting-the-freeipa-container">Splitting the FreeIPA container</h2>
<p>If Kubernetes or CRI-O user namespace support to does not solve our problem (in our desired timeframe) then there is more pressure to abandon the monolithic container and devote our efforts to a “split-service” FreeIPA/IDM application. In this scenario, the various services that make up FreeIPA (LDAP, KDC, HTTP, CA and others) would each run as an unprivileged process in its own container.</p>
<p>This would be a big engineering effort. Apart from FreeIPA as a whole, most of the constituent services are also “traditional” applications that make assumptions about their environment and execution context—assumptions that do not hold in the OpenShift container paradigm.</p>
<p>There is a general (albeit unevenly distributed) feeling in the team that in the long run this effort is inevitable. I do hold this view myself, but also recognise that the sooner we can have a working proof of concept, the better. That is the main reason we are initially pursuing the monolithic container approach.</p>
<h2 id="next-steps">Next steps</h2>
<p>My next step will be to install an OpenShift cluster based on the nightly builds (which include CRI-O v1.20) and experiment with the annotation-based user namespace support. It seems to be what we want, or a big step in the right direction, but we need to confirm it. Expect a follow-up to this article with my findings, hopefully in the next week!</p>]]></summary>
</entry>
<entry>
    <title>Issuing certificates for long hostnames</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2020-10-20-ipa-cert-long-hostname.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2020-10-20-ipa-cert-long-hostname.html</id>
    <published>2020-10-20T00:00:00Z</published>
    <updated>2020-10-20T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="issuing-certificates-for-long-hostnames">Issuing certificates for long hostnames</h1>
<p>X.509, specified in <a href="https://tools.ietf.org/html/rfc5280">RFC 5280</a>, restricts the length of the <em>Common Name (CN)</em> attribute to 64 characters:</p>
<pre><code>X520CommonName ::= DirectoryName (SIZE (1..ub-common-name))
ub-common-name-length INTEGER ::= 64</code></pre>
<p>Although the use of the CN attribute to carry DNS names is deprecated, it is still common practice. Furthermore, FreeIPA still requires the CN to appear in a <em>Certificate Signing Request (CSR)</em> and validates that its value corresponds to the nominated <em>subject principal</em>.</p>
<p>As a consequence of this restriction, when a host or service has a DNS name longer than 64 characters, that name cannot be used as the CN. But it can still be included in the <em>Subject Alternative Name</em> extension, as a <em>dNSName</em> value.</p>
<p>How do we issue such a certificate in FreeIPA? The trick is to add a <em>principal alias</em> whose hostname is 64 characters or shorter. This shorter hostname will be the Common Name attribute value. The full hostname will appear in the Subject Alternative Name extension.</p>
<p>The following sections demonstrate the method. I conclude with an outline of what needs to be done to support certificates with empty Subject DN, which would avoid the problem and this workaround.</p>
<h2 id="creating-a-principal-with-a-long-hostname">Creating a principal with a long hostname</h2>
<p>To experiment and verify the workaround, I needed a principal with a hostname longer than 64 characters. The initial attempt failed:</p>
<pre><code>% ipa host-add --force \
    verylongverylongverylongverylongverylongverylonghostname.ipa.local
ipa: ERROR: invalid &#39;hostname&#39;: can be at most 64 characters</code></pre>
<p>FreeIPA has a default maximum hostname length of 64 characters, but this is configurable. After adjusting the limit, adding the host succeeded:</p>
<pre><code>% ipa config-mod --maxhostname 255
  Maximum username length: 32
  Maximum hostname length: 255
  ...

% ipa host-add --force \
    verylongverylongverylongverylongverylongverylonghostname.ipa.local
-------------------------------------------------------------------------------
Added host &quot;verylongverylongverylongverylongverylongverylonghostname.ipa.local&quot;
-------------------------------------------------------------------------------
  Host name: verylongverylongverylongverylongverylongverylonghostname.ipa.local
  Principal name: host/verylongverylongverylongverylongverylongverylonghostname.ipa.local@IPA.LOCAL
  Principal alias: host/verylongverylongverylongverylongverylongverylonghostname.ipa.local@IPA.LOCAL
  ...</code></pre>
<h2 id="adding-the-principal-alias">Adding the principal alias</h2>
<p>For a host principal, use the <code>ipa host-add-principal</code> command to add a principal alias. The alias must also be a host principal, i.e. must have the form <code>host/$hostname</code>:</p>
<pre><code>% ipa host-add-principal \
    verylongverylongverylongverylongverylongverylonghostname.ipa.local \
    host/longhostname.ipa.local
----------------------------------------------------------------------------------------------
Added new aliases to host &quot;verylongverylongverylongverylongverylongverylonghostname.ipa.local&quot;
----------------------------------------------------------------------------------------------
Host name: verylongverylongverylongverylongverylongverylonghostname.ipa.local
Principal alias: host/verylongverylongverylongverylongverylongverylonghostname.ipa.local@IPA.LOCAL,
                 host/longhostname.ipa.local@IPA.LOCAL</code></pre>
<p>For a service principal, use the <code>ipa service-add-principal</code> command. Ensure the principal alias has the same service type as the subject principal’s <em>canonical name</em> (i.e. the value its <code>krbcanonicalname</code> attribute). For example, if the canonical principal name is <code>HTTP/$LONGHOSTNAME</code>, then the principal alias should be <code>HTTP/$SHORTHOSTNAME</code>.</p>
<p>I omitted the realm parts of principal names (the default realm will be added automatically). For the avoidance of doubt, the princpial alias must have the same realm as the canonical principal.</p>
<h2 id="creating-a-csr">Creating a CSR</h2>
<p>There are many different ways to create a CSR. I will give a single example using OpenSSL. The private key already exists (file <code>key.pem</code>).</p>
<p>The configuration file:</p>
<pre><code>% cat longhostname.conf
[ req ]
prompt = no
encrypt_key = no

distinguished_name = dn
req_extensions = exts

[ dn ]
commonName = &quot;longhostname.ipa.local

[ exts ]
subjectAltName=DNS:verylongverylongverylongverylongverylongverylonghostname.ipa.local</code></pre>
<p>Create the CSR:</p>
<pre><code>% openssl req -new -key key.pem \
    -config longhostname.conf -extensions exts \
    &gt; longhostname.csr</code></pre>
<h2 id="issuing-the-certificate">Issuing the certificate</h2>
<p>Now we can issue the certificate:</p>
<pre><code>% ipa cert-request longhostname.csr \
    --principal host/verylongverylongverylongverylongverylongverylonghostname.ipa.local
  Issuing CA: ipa
  Certificate: MIIE...
  Subject: CN=longhostname.ipa.local,O=IPA.LOCAL 202009291726
  Subject DNS name: verylongverylongverylongverylongverylongverylonghostname.ipa.local,
                    longhostname.ipa.local
  Issuer: CN=Certificate Authority,O=IPA.LOCAL 202009291726
  Not Before: Mon Oct 19 13:46:16 2020 UTC
  Not After: Thu Oct 20 13:46:16 2022 UTC
  Serial number: 11
  Serial number (hex): 0xB</code></pre>
<p>The CN attribute contains the shorter host name, and the SAN extension contains both the long and shorter hostnames. (We did not include the short hostname in the CSR SAN extension, but the <code>CommonNameToSANDefault</code> profile component copied it there).</p>
<h2 id="supporting-san-only-certificates">Supporting SAN-only certificates</h2>
<p>This workaround is straightforward but it is not the ideal solution. A better approach is to enhance FreeIPA and Dogtag to support issuing certificates with an empty Subject DN, using only the Subject Alternative Name extension to carry subject information.</p>
<p>RFC 5280 allows an empty Subject DN in a certificate, in which case the certificate must include the SAN extension, which must be marked as <em>critical</em>. <a href="https://tools.ietf.org/html/rfc6125#section-2.3">RFC 6125</a> further clarifies that such a certificate is acceptable for use with TLS.</p>
<p><a href="https://pagure.io/freeipa/issue/5706">Upstream ticket #5706</a> requests support for SAN-only certificates. The work will involve:</p>
<ul>
<li>Change the <code>ipa cert-request</code> command to accept empty subjects. When the subject is empty ensure a non-empty SAN extension is present in the CSR, and that it is marked criticial. This is straightforward.</li>
<li>On the Dogtag side we must implement new behaviour in the request processor to ensure that the certificate to be issued satisfies the X.509 requirements about empty/non-empty Subject DN and the presence and criticality of the SAN extension.</li>
<li>It may be necessary to define a new profile default or constraint component that allows an empty subject DN.</li>
<li>It is likely that FreeIPA will need to either modify the default profile (<code>caIPAserviceCert</code>) to allow for an empty Subject DN, or ship a separate profile that is suitable.</li>
</ul>]]></summary>
</entry>
<entry>
    <title>Dogtag, number ranges and VLV indices</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2020-09-17-dogtag-vlv-corruption.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2020-09-17-dogtag-vlv-corruption.html</id>
    <published>2020-09-17T00:00:00Z</published>
    <updated>2020-09-17T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="dogtag-number-ranges-and-vlv-indices">Dogtag, number ranges and VLV indices</h1>
<p>In a <a href="2019-07-26-dogtag-replica-ranges.html">previous post</a> I explained Dogtag’s identifier range management. This is how a Dogtag replica knows what range it should use to assign serial numbers, request IDs, etc. What that article did not cover is how Dogtag at startup works out <em>where it is up to</em> in the range. In this post I explain how uses LDAP <em>Virtual List View</em> to do that, how it can break, and how to fix it.</p>
<h2 id="ldap-virtual-list-view">LDAP Virtual List View</h2>
<p>The LDAP protocol has an optional extension called <em>Virtual List View (VLV)</em>, which is specified in an <a href="https://datatracker.ietf.org/doc/draft-ietf-ldapext-ldapv3-vlv/">expired Internet-Draft</a>. VLV supports result <em>paging</em> and is an extension of the <em>Server Side Sort (SSS)</em> control (<a href="https://tools.ietf.org/html/rfc2891">RFC 2891</a>). For a search that is covered by a VLV index, a client can specify a page size and offset and get just that portion of the result. It can also seek a specified attribute value and return nearby results.</p>
<p>In 389DS / RHDS, a VLV index is defined by two objects under <code>cn=config</code>. One of the VLV indices used in Dogtag is the search of all certificates sorted by serial number:</p>
<pre class="ldif"><code>dn: cn=allCerts-pki-tomcat,
    cn=ipaca, cn=ldbm database, cn=plugins, cn=config
objectClass: top
objectClass: vlvSearch
cn: allCerts-pki-tomcat
vlvBase: ou=certificateRepository,ou=ca,o=ipaca
vlvScope: 1
vlvFilter: (certstatus=*)

dn: cn=allCerts-pki-tomcatIndex, cn=allCerts-pki-tomcat,
    cn=ipaca, cn=ldbm database, cn=plugins, cn=config
objectClass: top
objectClass: vlvIndex
cn: allCerts-pki-tomcatIndex
vlvSort: serialno
vlvEnabled: 0
vlvUses: 0</code></pre>
<p>The first object defines the search base and filter. When performing a VLV search, these <strong>must match</strong>. The second object declares which attribute is the sort key. To perform a VLV search the client must use both the SSS control (which chooses the sort key) and the VLV control (which selects the page or the value of interest).</p>
<h2 id="dogtag-range-initialisation">Dogtag range initialisation</h2>
<p>When Dogtag is starting up, for each active identifier range it has to determine the first unused number. It uses VLV searches to do this. For serial numbers, it uses the VLV index shown above. For request IDs and other ranges, there are other indices. The VLV search targets the upper limit of the range, and requests the preceding values. It then looks for the highest value in the result that is also within the active range. This is the last number that was used; we increment it to get the next available number.</p>
<p>To make it a bit more concrete, we can perform a VLV search ourselves using <code>ldapsearch</code>:</p>
<pre><code># ldapsearch -LLL -D &quot;cn=Directory Manager&quot; -w $DM_PASS \
    -b ou=certificateRepository,ou=ca,o=ipaca -s one \
    -E &#39;sss=serialno&#39; -E &#39;vlv=1/0:09267911168&#39; \
    &#39;(certStatus=*)&#39; 1.1
dn: cn=397,ou=certificateRepository,ou=ca,o=ipaca

dn: cn=267911185,ou=certificateRepository,ou=ca,o=ipaca

# sortResult: (0) Success
# vlvResultpos=2 count=177 context= (0) Success</code></pre>
<p>In this search the target value (end of the active range) is <code>09267911168</code>. This is the integer <code>267911168</code> preceded by a two-digit length value. This is needed because the <code>serialno</code> attribute has <code>Directory String</code> syntax, which is sorted lexicographically. The <code>1/0</code> part of the control is asking for one value preceding the target value, and zero values following it.</p>
<p>The result contains two objects: <code>397</code> (which precedes the target) and <code>267911185</code> (which follows it). Why did we get a number following the target value? The target entry is the first entry whose sort attribute value is <em>greater than or equal</em> the target value. In this way, results greater than the target can appear in the result, as happened here.</p>
<p>The search above relates to the range <code>1..267911168</code>. The result shows us to initialise the repository with <code>397</code> as the “last used” number. The next certificate issued by this replica will have serial number <code>398</code>.</p>
<h2 id="vlv-index-corruption">VLV index corruption</h2>
<p>If a VLV index is corrupt or incomplete, Dogtag could initialise a repository with a too-low “last used” number. This could happen for serial numbers, request IDs or any other kind of managed range. When that happens, CA operations including certificate issuance or CSR submission could fail.</p>
<p>In fact, the <code>ldapsearch</code> above is from a customer case. A full search of the <code>ou=certificateRepository</code> showed thousands of certificates that were not included in the VLV index. If CA operations are failing due to LDAP “Object already exists” errors, you can perform this check to confirm or rule out VLV index corruption as the source of the problem. Keep in mind that VLV indices are maintained separately on each replica. Checks have to be performed on the replica where the problem is occurring.</p>
<h2 id="rebuilding-vlv-indices">Rebuilding VLV indices</h2>
<p>389DS makes it easy to rebuild a VLV index. You create a <em>task</em> object and the DS takes care of it. For Dogtag, we even provide a template LDIF file for a task that reindexes <em>all</em> the VLV indices that Dogtag creates and uses.</p>
<p>First, copy and fill the template:</p>
<pre><code>$ /bin/cp /usr/share/pki/ca/conf/vlvtasks.ldif .
$ sed -i &quot;s/{instanceId}/pki-tomcat/g&quot; vlvtasks.ldif
$ sed -i &quot;s/{database}/ipaca/g&quot; vlvtasks.ldif</code></pre>
<p>Note that <code>{database}</code> should be replaced with <code>ipaca</code> in a FreeIPA instance, but for a standalone Dogtag deployment the correct value is usually <code>ca</code>. Now let’s look at the LDIF file:</p>
<pre class="ldif"><code>dn: cn=index1160589769, cn=index, cn=tasks, cn=config
objectclass: top
objectclass: extensibleObject
cn: index1160589769
ttl: 10
nsinstance: ipaca
nsindexVLVAttribute: allCerts-pki-tomcatIndex
# ... 33 more nsindexVLVAttribute values</code></pre>
<p>The <code>cn</code> is just a name for the task. I think you can put anything here. <code>ttl</code> specifies how many seconds 389DS will wait after the task finishes, before deleting it.</p>
<p>This task object refers to VLV indices in the Dogtag database. But you can see all that is needed to rebuild <em>any</em> VLV index is the <code>nsinstance</code> (name of the database) and the <code>nsindexVLVAttribute</code> (name of a VLV index).</p>
<p>Now we add the object, wait a few seconds, and have a look at it:</p>
<pre><code>$ ldapadd -x -D &quot;cn=Directory Manager&quot; -w $DM_PASS \
    -f vlvtasks.ldif
$ sleep 5
$ ldapsearch -x -D &quot;cn=Directory Manager&quot; -w $DM_PASS \
  -b &quot;cn=index1160589769,cn=index,cn=tasks,cn=config&quot;</code></pre>
<pre class="ldif"><code>dn: cn=index1160589769,cn=index,cn=tasks,cn=config
objectClass: top
objectClass: extensibleObject
cn: index1160589769
ttl: 10
nsinstance: ipaca
nsindexvlvattribute: allCerts-pki-tomcatIndex
# .. 33 more nsindexvlvattribute values
nsTaskCurrentItem: 0
nsTaskTotalItems: 1
nsTaskCreated: 20200916021128Z
nsTaskLog:: aXBhY2E6IEluZGV4aW #... (base64-encoded log)
nsTaskStatus: ipaca: Finished indexing.
nsTaskExitCode: 0</code></pre>
<p>We can see that the task finished successfully, and there is some (truncated) log output if we want more details. After a few more seconds, 389DS will delete the object. You can increase the <code>ttl</code> if you want to keep the objects for longer.</p>
<h2 id="discussion">Discussion</h2>
<p>This year I have encountered variations of this problem on several occasions. I don’t know what the cause(s) are, i.e. why VLV indices get corrupted or stop updating. Hopefully DS experts will be able to shed more light on the issue.</p>
<p>We are considering adding an automated check to the FreeIPA <em>Health Check</em> system, specifically for the range management VLVs. The <a href="">GitHub ticket</a> already contains some discussion and high level steps of how the check would work.</p>
<p>The proper fix for this issue is to move to UUIDs for all object identifiers. Serial numbers might need something different but it is the same idea. This work is on the roadmap. <em>So many problems</em> will go away when we make this change.</p>
<p>Historical commentary: I don’t know why the <code>serialno</code>, <code>requestId</code> and other attributes use Directory String syntax, which necessitates the length prefixing hack. Maybe SSS/VLV only work on strings (or it was thus in the past). The code predates our current VCS and the reasons are lost in time. The implication of this is that we can only handle numbers up to 99 decimal digits. Assumptions like this do bother me, but I think we are probably OK here. For my lifetime, anyway.</p>]]></summary>
</entry>
<entry>
    <title>Dynamic volume provisioning with OpenShift storage classes</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2020-08-13-openshift-storage-classes.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2020-08-13-openshift-storage-classes.html</id>
    <published>2020-08-13T00:00:00Z</published>
    <updated>2020-08-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="dynamic-volume-provisioning-with-openshift-storage-classes">Dynamic volume provisioning with OpenShift storage classes</h1>
<p>For containerised applications that require persistent storage, the Kubernetes <code>PersistentVolumeClaim</code> (PVC) object provides the link between a <code>PersistentVolume</code> (PV) and the pod. When scaling such an application or even deploying it the first time, the operator (human or otherwise) has to create the PVC; the pod specification can then refer to it.</p>
<p>For example, a <code>StatefulSet</code> object can optionally specify <code>volumeClaimTemplates</code> alongside the pod <code>template</code>. As the application creates pods, so will it create the associated PVCs according to the defined templates.</p>
<p>But PVCs need PVs to bind to. Can these also be created on the fly? And if so, how can we abstract over the details of the underlying storage provider(s), which may vary from cluster to cluster? In this post I provide an overview of <em>storage classes</em>, which solve these problems.</p>
<h2 id="creating-volumes">Creating volumes</h2>
<p>A cluster can provide a variety of types of volumes: Ceph, NFS, <code>hostPath</code>, iSCSI and several more. Storage types of the infrastructure the cluster is deployed in may also be available, e.g. AWS EBS, Azure Disk, GCE PersistentDisk (PD), Cinder (OpenStack), etc.</p>
<p>Creating a <code>PersistentVolume</code> requires knowing about what volume types are supported, and possibly additional details about that storage type. For example, to create a PV based on a GCE PD:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb1-1"><a href="#cb1-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> PersistentVolume</span></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> pv-test</span></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="at">  </span><span class="fu">capacity</span><span class="kw">:</span></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="at">    </span><span class="fu">storage</span><span class="kw">:</span><span class="at"> 100Gi</span></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="at">  </span><span class="fu">accessModes</span><span class="kw">:</span></span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="at">  </span><span class="kw">-</span><span class="at"> ReadWriteOnce</span></span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="at">  </span><span class="fu">gcePersistentDisk</span><span class="kw">:</span></span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="at">    </span><span class="fu">pdName</span><span class="kw">:</span><span class="at"> my-data-disk</span></span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="at">    </span><span class="fu">fsType</span><span class="kw">:</span><span class="at"> ext4</span></span>
<span id="cb1-13"><a href="#cb1-13"></a><span class="at">  </span><span class="fu">nodeAffinity</span><span class="kw">:</span></span>
<span id="cb1-14"><a href="#cb1-14"></a><span class="at">    </span><span class="fu">required</span><span class="kw">:</span></span>
<span id="cb1-15"><a href="#cb1-15"></a><span class="at">      </span><span class="fu">nodeSelectorTerms</span><span class="kw">:</span></span>
<span id="cb1-16"><a href="#cb1-16"></a><span class="at">      </span><span class="kw">-</span><span class="at"> </span><span class="fu">matchExpressions</span><span class="kw">:</span></span>
<span id="cb1-17"><a href="#cb1-17"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">key</span><span class="kw">:</span><span class="at"> failure-domain.beta.kubernetes.io/zone</span></span>
<span id="cb1-18"><a href="#cb1-18"></a><span class="at">          </span><span class="fu">operator</span><span class="kw">:</span><span class="at"> In</span></span>
<span id="cb1-19"><a href="#cb1-19"></a><span class="at">          </span><span class="fu">values</span><span class="kw">:</span></span>
<span id="cb1-20"><a href="#cb1-20"></a><span class="at">          </span><span class="kw">-</span><span class="at"> us-central1-a</span></span>
<span id="cb1-21"><a href="#cb1-21"></a><span class="at">          </span><span class="kw">-</span><span class="at"> us-central1-b</span></span></code></pre></div>
<p>Creating this PV required:</p>
<ul>
<li>knowing that the cluster provides the GCE PD volume type</li>
<li>knowing the name and region/zones of the PD to use</li>
</ul>
<p>Having to know these details and encoding them into an application’s deployment manifests imposes a greater burden on administrators, or necessitates more complex operators, or results in a less portable application. Or some combination of those outcomes.</p>
<h2 id="storage-classes">Storage classes</h2>
<p>What we really want is to abstract over the storage implementations. We want to able to specify some high-level characteristics of the storage (e.g. block or file, fast or slow?). This is what <em>storage classes</em> provide. Then when we create a PVC, we can specify the desired capacity and class, and the cluster should <em>dynamically provision</em> an appropriate volume. As a result, applications are simpler to deploy and more portable.</p>
<p>To see the storage classes available in a cluster:</p>
<pre><code>ftweedal% oc get storageclass
NAME                 PROVISIONER            RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
standard (default)   kubernetes.io/cinder   Delete          WaitForFirstConsumer   true                   28d</code></pre>
<p>This cluster has only one storage class, called <code>standard</code>. It is also the default storage class for this cluster. To use dynamic provisioning, in the PVC spec instead of <code>volumeName</code> specify <code>storageClassName</code>:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb3-1"><a href="#cb3-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> PersistentVolumeClaim</span></span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> pvc-test</span></span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb3-6"><a href="#cb3-6"></a><span class="at">  </span><span class="fu">accessModes</span><span class="kw">:</span></span>
<span id="cb3-7"><a href="#cb3-7"></a><span class="at">    </span><span class="kw">-</span><span class="at"> ReadWriteOnce</span></span>
<span id="cb3-8"><a href="#cb3-8"></a><span class="at">  </span><span class="fu">resources</span><span class="kw">:</span></span>
<span id="cb3-9"><a href="#cb3-9"></a><span class="at">    </span><span class="fu">requests</span><span class="kw">:</span></span>
<span id="cb3-10"><a href="#cb3-10"></a><span class="at">      </span><span class="fu">storage</span><span class="kw">:</span><span class="at"> 10Gi</span></span>
<span id="cb3-11"><a href="#cb3-11"></a><span class="at">  </span><span class="fu">storageClassName</span><span class="kw">:</span><span class="at"> standard</span></span></code></pre></div>
<p>If you want to use the default storage class, you can even omit the <code>storageClassName</code> field:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb4-1"><a href="#cb4-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> PersistentVolumeClaim</span></span>
<span id="cb4-3"><a href="#cb4-3"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb4-4"><a href="#cb4-4"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> pvc-test</span></span>
<span id="cb4-5"><a href="#cb4-5"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb4-6"><a href="#cb4-6"></a><span class="at">  </span><span class="fu">accessModes</span><span class="kw">:</span></span>
<span id="cb4-7"><a href="#cb4-7"></a><span class="at">    </span><span class="kw">-</span><span class="at"> ReadWriteOnce</span></span>
<span id="cb4-8"><a href="#cb4-8"></a><span class="at">  </span><span class="fu">resources</span><span class="kw">:</span></span>
<span id="cb4-9"><a href="#cb4-9"></a><span class="at">    </span><span class="fu">requests</span><span class="kw">:</span></span>
<span id="cb4-10"><a href="#cb4-10"></a><span class="at">      </span><span class="fu">storage</span><span class="kw">:</span><span class="at"> 10Gi</span></span></code></pre></div>
<h2 id="dynamic-provisioning-in-action">Dynamic provisioning in action</h2>
<p>Let’s see what actually happens when we use dynamic provisioning. We will observe what objects are created and how their status changes as we create, use and delete a PVC that uses the default storage class.</p>
<p>First let’s see what PVs exist:</p>
<pre><code>ftweedal% oc get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                                             STORAGECLASS   REASON    AGE
pvc-d3bc7c81-8a24-4318-a914-296dbdc5ec3f   100Gi      RWO            Delete           Bound     openshift-image-registry/image-registry-storage   standard                 7d22h</code></pre>
<p>There is one PV, with a 100Gi capacity. It is used for the image registry.</p>
<p>Now, lets create <code>pvc-test</code> as specified above:</p>
<pre><code>ftweedal% oc create -f deploy/pvc-test.yaml
persistentvolumeclaim/pvc-test created

ftweedal% oc get pvc pvc-test
NAME       STATUS    VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-test   Pending                                       standard       11s

ftweedal% oc get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                                             STORAGECLASS   REASON    AGE
pvc-d3bc7c81-8a24-4318-a914-296dbdc5ec3f   100Gi      RWO            Delete           Bound     openshift-image-registry/image-registry-storage   standard                 7d22h

ftweedal% oc get pvc pvc-test -o yaml |grep storageClassName
storageClassName: standard</code></pre>
<p>The PVC <code>pvc-test</code> was created and has status <code>pending</code>. No new PV has been created yet. Finally note that the PVC has <code>storageClassName: standard</code> (which is the cluster default).</p>
<p>Now lets create a pod that uses <code>pvc-test</code>, mounting it at <code>/data</code>. The pod spec is:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb7-1"><a href="#cb7-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Pod</span></span>
<span id="cb7-3"><a href="#cb7-3"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb7-4"><a href="#cb7-4"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> pod-test</span></span>
<span id="cb7-5"><a href="#cb7-5"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb7-6"><a href="#cb7-6"></a><span class="at">  </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb7-7"><a href="#cb7-7"></a><span class="at">    </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> pod-test-container</span></span>
<span id="cb7-8"><a href="#cb7-8"></a><span class="at">      </span><span class="fu">image</span><span class="kw">:</span><span class="at"> freeipa/freeipa-server:fedora-31</span></span>
<span id="cb7-9"><a href="#cb7-9"></a><span class="at">      </span><span class="fu">volumeMounts</span><span class="kw">:</span></span>
<span id="cb7-10"><a href="#cb7-10"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">mountPath</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;/data&quot;</span></span>
<span id="cb7-11"><a href="#cb7-11"></a><span class="at">          </span><span class="fu">name</span><span class="kw">:</span><span class="at"> data</span></span>
<span id="cb7-12"><a href="#cb7-12"></a><span class="at">      </span><span class="fu">command</span><span class="kw">:</span></span>
<span id="cb7-13"><a href="#cb7-13"></a><span class="at">        </span><span class="kw">-</span><span class="at"> sleep</span></span>
<span id="cb7-14"><a href="#cb7-14"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="st">&quot;3600&quot;</span></span>
<span id="cb7-15"><a href="#cb7-15"></a><span class="at">  </span><span class="fu">volumes</span><span class="kw">:</span></span>
<span id="cb7-16"><a href="#cb7-16"></a><span class="at">    </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> data</span></span>
<span id="cb7-17"><a href="#cb7-17"></a><span class="at">      </span><span class="fu">persistentVolumeClaim</span><span class="kw">:</span></span>
<span id="cb7-18"><a href="#cb7-18"></a><span class="at">        </span><span class="fu">claimName</span><span class="kw">:</span><span class="at"> pvc-test</span></span></code></pre></div>
<p>After creating the pod we will write a file under <code>/data</code>, delete then re-create the pod, and observe that the file we wrote persists.</p>
<pre><code>ftweedal% oc create -f deploy/pod-test.yaml
pod/pod-test created

ftweedal% oc exec pod-test -- sh -c &#39;echo &quot;hello world&quot; &gt; /data/foo&#39;

ftweedal% oc delete pod pod-test
pod &quot;pod-test&quot; deleted

ftweedal% oc create -f deploy/pod-test.yaml
pod/pod-test created

ftweedal% oc exec pod-test -- cat /data/foo
hello world

ftweedal% oc delete pod pod-test
pod &quot;pod-test&quot; deleted</code></pre>
<p>This confirms that the PVC works as intended. Let’s check the status of the PVC and PVs to see what happened behind the scenes:</p>
<pre><code>ftweedal% oc get pvc pvc-test
NAME       STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-test   Bound     pvc-26d82d50-8e66-4938-bdee-f28ff2bcb49c   10Gi       RWO            standard       16m

ftweedal% oc get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                                             STORAGECLASS   REASON    AGE
pvc-26d82d50-8e66-4938-bdee-f28ff2bcb49c   10Gi       RWO            Delete           Bound     ftweedal-operator/pvc-test                        standard                 4m53s
pvc-d3bc7c81-8a24-4318-a914-296dbdc5ec3f   100Gi      RWO            Delete           Bound     openshift-image-registry/image-registry-storage   standard                 7d23h</code></pre>
<p>Before creating the pod <code>pvc-test</code> had status <code>Pending</code>. Now it is <code>Bound</code> to the volume <code>pvc-26d82d50-8e66-4938-bdee-f28ff2bcb49c</code> which was dynamically provisioned with capacity 10Gi as required by <code>pvc-test</code>.</p>
<p>Finally as we delete <code>pvc-test</code>, observe the automatic deletion of the dynamically provisioned volume:</p>
<pre><code>ftweedal% oc delete pvc pvc-test
persistentvolumeclaim &quot;pvc-test&quot; deleted

ftweedal% oc get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                                             STORAGECLASS   REASON    AGE
pvc-d3bc7c81-8a24-4318-a914-296dbdc5ec3f   100Gi      RWO            Delete           Bound     openshift-image-registry/image-registry-storage   standard                 7d23h</code></pre>
<p><code>pvc-26d82d50-8e66-4938-bdee-f28ff2bcb49c</code> went away, as expected.</p>
<h2 id="conclusion">Conclusion</h2>
<p>As we work toward operationalising FreeIPA in OpenShift, I am interested in how we can use storage classes to make for a smooth deployment across different environments and especially those for which OpenShift Dedicated is available.</p>
<p>I also need to learn more about the best practices or common idioms for representing in storage classes the application suitability (e.g. file versus block storage) or performance characteristics of supported volume types in a cluster. To make it a bit more concrete, consider that for performance reasons we might require low-latency/high-throughput block storage for the 389 DS LDAP database storage. How can we express this abstract requirement such that we get a satisfactory result across a variety of “clouds” with no administrator effort? Hopefully storage classes are the answer. But if they are not the whole solution, from what I have learned so far I have a strong feeling that they will be a bit part of the solution.</p>]]></summary>
</entry>
<entry>
    <title>CRLs for Dogtag Lightweight CAs</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2020-06-19-dogtag-lightweight-ca-crl.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2020-06-19-dogtag-lightweight-ca-crl.html</id>
    <published>2020-06-19T00:00:00Z</published>
    <updated>2020-06-19T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="crls-for-dogtag-lightweight-cas">CRLs for Dogtag Lightweight CAs</h1>
<p>A few years ago I implemented <em>lightweight CAs</em> in Dogtag. This feature allows multiple CAs to be hosted in a single Dogtag server instance. For now these are restricted to sub-CAs of the <em>main CA</em> but this is not a fundamental restriction.</p>
<p>An important aspect of CA operation is <em>revocation</em>: the ability to revoke a certificate because of (suspected) key compromise, cessation of operation, it was superseded, etc. There are currently two main ways of conveying revocation status to clients: <em>Certificate Revocation Lists (CRLs)</em> and <em>Online Certificate Status Protocol (OCSP)</em>. CRLs and OCSP have their respective advantages and drawbacks. Suffice to say, for many security-conscious organisations CRLs are important (as is OCSP).</p>
<p>There is currently no support for lightweight CA certificates in CRLs produced by Dogtag. The purpose of this post is to discuss the challenges and possible approaches to closing this gap.</p>
<h2 id="overview-of-ocsp-and-crls">Overview of OCSP and CRLs</h2>
<p>OCSP (defined in <a href="https://tools.ietf.org/html/rfc6960">RFC 6960</a>) is a network protocol for determining certificate revocation status. Any relying party (e.g. a web browser validating a server certificate) can ask the CA’s <em>OCSP responder</em> for a signed assertion of whether or not the certificate is revoked. For scalability and performance reasons, TLS servers can periodically obtain OCSP responses for their certificate and convey them to clients in the TLS handshake; this feature is called <a href="https://en.wikipedia.org/wiki/OCSP_stapling">OCSP stapling</a>.</p>
<p>On the other hand, CRLs are a more <em>passive</em> technology. X.509 CRLs are defined alongside X.509 certificates in <a href="https://tools.ietf.org/html/rfc5280">RFC 5280</a>. In the simple case a CRL is a signed, timestamped list of all revoked, non-exired certificates issued by a CA. The CA produces new CRLs on a fixed schedule (e.g. every 4 hours) and publishes them (e.g. on HTTP, in an LDAP directory, etc). Clients <em>somehow</em> obtain and refresh their CRL cache, and consult it when validating certificates. The CRL grows linearly in the number of revoked certificates so on a busy CA the CRL can become <em>huge</em>. Retrieving a large CRL takes time and bandwidth, storing it takes space, and consulting it takes time. The advantage is that validation requires no (additional) network traffic. The assumption is that the clients CRL cache is up to date.</p>
<p>One further downside of CRLs is that they are only as good as their most recent update. What if your CRL is 3 hours old, the certificate of interest was revoked 1 hour ago, and it is still 1 hour until the next CRL gets published? In practice, every approach to revocation suffers from such a delay. Also in practice, the delay duration is often much greater for CRLs than for OCSP.</p>
<h2 id="ocsp-support-for-lightweight-cas">OCSP support for Lightweight CAs</h2>
<p>The initial release of the Dogtag lightweight CAs feature had OCSP support for certificates issued by lightweight CAs. It works properly and there is nothing more to be said about it.</p>
<h2 id="crl-support-for-lightweight-cas">CRL support for lightweight CAs</h2>
<p>As mentioned in the introduction, certificates issued by lightweight CAs are not included in the CRLs produced by Dogtag. <a href="https://pagure.io/dogtagpki/issue/1627">Ticket #1627</a> in the upstream Pagure tracks this issue.</p>
<p>The reason this was not implemented in the initial release (or since) is that in the baseline case, a CRL can only include certificates from a single CA. Say we have the main CA <code>CN=MainCA</code> and lightweight sub-CA <code>CN=SubCA</code>. The CRL cannot include certificates from both CAs, because a CRL is just a list of serial numbers.</p>
<h3 id="indirect-crls">Indirect CRLs</h3>
<p>There is a way around this limitation. The <a href="https://tools.ietf.org/html/rfc5280#section-5.3.3">Certificate Issuer</a> CRL entry extension, if some other extensions on both the certificate and CRL are set up <em>just right</em>, allows a CRL to include certificates from multiple issuers. Such CRLs are called <em>indirect CRLs</em>. Conforming applications are not required to support indirect CRLs, and the extension is <em>critical</em> so there is a risk of compatibility issues if we were to use indirect CRLs for conveying revocation status of certificates issued by lightweight CAs.</p>
<p>Apart from client support for the Certificate Issuer extension the other requirements for indirect CRLs to work are:</p>
<ul>
<li>The certificate’s <em>CRL Distribution Points (CRLDP)</em> extension must include the <code>cRLIssuer</code> field and its value must match the issuer of the CRL.</li>
<li>The CRL must include the <em>Issuing Distribution Point</em> CRL extension that asserts the <code>indirectCRL</code> boolean. This is a critical extension.</li>
<li>The trust anchor for the CRL must be the same as the trust anchor for the certificate. This means that indirect CRLs cannot work for lightweight CAs that do not chain to the same CA. This is only a potential problem if the lightweight CAs feature is enhanced to support hosting unrelated CAs (rather than sub-CAs).</li>
</ul>
<p>So to use indirect CRLs some minor changes to certificate profiles would be required. But the changes would be the same for all profiles and the content of the CRL Distribution Point extension would be the same regardless of which lightweight CA issues the certificate.</p>
<h3 id="separate-crls">Separate CRLs</h3>
<p>An alternative approach is to create a separate CRL for each lightweight CA. This would avoid compatibility issues caused by the use of critical extensions that clients are not required to support. It also avoids the trust anchor limitations that would arise when hosting a lightweight CA that does not share a common trust root with the CRL issuer.</p>
<p>From an implementation point of view there are two major challenges with this approach.</p>
<ol type="1">
<li>Dogtag does not generate CRLs implicitly but currently requires explicit configuration for each CRL. The configuration is not stored in LDAP but in the <code>CS.cfg</code> configuration file, so there is no way to dynamically configure new CRLs as new lightweight CAs are created.</li>
<li>The content of the CRL Distribution Point extension will differ according to the CA that is issuing the certificate. The CRLDP content is currently configured per-profile. New profile components or enhancements to the existing CRLDP profile component will be required.</li>
</ol>
<p>In my view it is not acceptable to have to define multiple profiles differing only the CRL Distribution Point extension. The CA issuing the certificate should, by default, set any extensions that relate specifically to itself, including the CRLDP (also <em>Authority Key Identifier</em> and <em>Authority Information Access</em>). For more specialised use cases, the CRLDP content could be <em>overridden</em> or <em>suppressed</em> on a per-profile basis.</p>
<h3 id="deciding-the-approach">Deciding the approach</h3>
<p>Indirect CRLs is the lower-effort approach. But before choosing it, we ought to audit certificate verification libraries (especially OpenSSL, NSS and other libraries used in Fedora, RHEL and other Red Hat products) to see if they support indirect CRLs. If support is widespread, the approach is viable. If support is not widespread, it is not a good idea.</p>
<p>Thinking longer-term, this is a good opportunity to improve the administrator experience. Maybe now is a good time to implement useful features like automatic CRL generation for each CA in a Dogtag instance, and profile components that create a CRL Distribution Point extension that points to the CRL for the CA that is issuing the certificate. The current configuration approach is versatile and can handle all kinds of wild CRL scenarios. But it is <em>hostile</em> to getting things right for the common case.</p>
<p>This decision will probably not be mine to make because I will soon be leaving the Dogtag team. But I hope this post is useful to whoever is involved in the eventual decision.</p>
<h3 id="profile-changes">Profile changes</h3>
<p>Both of the discussed approaches require some changes to profile configuration. Required profile changes means upgrade steps to update them. This can be tricky especially in mixed-version topologies when new profile components (if any) are present on some servers but not others.</p>
<h3 id="the-do-nothing-option">The “do nothing” option</h3>
<p>Lightweight CAs have been available for nearly 4 years. I can only recall one or two queries about lightweight CA CRL support. To be clear, it is a fair ask. But it seems that OCSP is sufficient for most customers. Or perhaps there is a lack of awareness that CRLs do not include certificates issued by lightweight CAs. Whatever the case, the low demand aligns with my own opinion that although CRL support for lightweight CAs is a nice-to-have, it is not of critical importance to many users or customers.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In this post I identified two possible approaches to CRL support for lightweight CAs. Each approach has advantages, drawbacks and unique challenges. Never implementing it is also an option to be considered because demand, though it does exist, seems low.</p>
<p>I haven’t often discussed revocation in detail, so it is probably worth mentioning other approaches besides CRLs and OCSP.</p>
<p><em>Ephemeral PKI</em> avoids the problem by only issuing very short lived certificates, e.g. one week, one day or even less! Assuming keys are rotated just as frequently, when certificate lifetimes approach the “lag” time revocation solutions, the revocation solution is not needed.</p>
<p><em>CRLite</em> is an experimental revocation solution currently in development. It achieves fast and scalable revocation checking through cascading Bloom filters produced by an <em>aggregator</em> that records certificate revocations from one or more CAs. The target use case is in fact <em>all publicly trusted CAs</em> and Firefox Nightly already uses the system (non-enforcing, telemetry-only by default). Scott Helme wrote an <a href="https://scotthelme.co.uk/crlite-finally-a-fix-for-broken-revocation/">excellent blog post</a> about it and you can read the <a href="https://obj.umiacs.umd.edu/papers_for_stories/crlite_oakland17.pdf">original paper</a> for the gory details.</p>
<p>One final note. I found some compliance issues with how the CRL Distribution Point extension is configured in the default FreeIPA certificate profiles. A strict reading of <a href="https://tools.ietf.org/html/rfc5280">RFC 5280</a> suggests that the CRL Distribution Point extension data produced by the default FreeIPA profiles would lead to the certificate not being considered in scope of the CRLs produced by Dogtag. This issue is particular to FreeIPA configuration, not a general problem with FreeIPA. More investiation is required and I will probably write a separate post about this in the future.</p>]]></summary>
</entry>
<entry>
    <title>ACME DNS challenges and FreeIPA</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2020-05-13-ipa-acme-dns.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2020-05-13-ipa-acme-dns.html</id>
    <published>2020-05-13T00:00:00Z</published>
    <updated>2020-05-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="acme-dns-challenges-and-freeipa">ACME DNS challenges and FreeIPA</h1>
<p><em>This post is part of a series of ACME client demonstrations. See also the posts about</em> <a href="2020-05-06-ipa-acme-intro.html">Certbot standalone HTTP</a> <em>and</em> <a href="2020-05-07-ipa-acme-mod_md.html">mod_md for Apache</a></p>
<p>The ACME protocol defined in <a href="https://tools.ietf.org/html/rfc8555">RFC 8555</a> defines a <a href="https://tools.ietf.org/html/rfc8555#section-8.4">DNS challenge</a> for proving control of a domain name. In this post I’ll explain how the DNS challenge works and demonstrate how to use the <a href="https://certbot.eff.org/">Certbot</a> ACME client with the FreeIPA integrated DNS service.</p>
<h2 id="the-dns-challenge">The DNS challenge</h2>
<p>To prove control of a domain name (the <code>dns</code> identifier type) ACME defines the <code>dns-01</code> challenge type. It is up to ACME servers which challenges to create for a given identifier. If a server offers multiple challenges (e.g. <code>http-01</code> and <code>dns-01</code>) the client can choose which one to attempt.</p>
<p>A DNS challenge object looks like:</p>
<pre><code>{
  &quot;type&quot;: &quot;dns-01&quot;,
  &quot;url&quot;: &quot;https://example.com/acme/chall/Rg5dV14Gh1Q&quot;,
  &quot;status&quot;: &quot;pending&quot;,
  &quot;token&quot;: &quot;evaGxfADs6pSRb2LAv9IZf17Dt3juxGJ-PCt92wr-oA&quot;
}</code></pre>
<p>The <code>token</code> field is a base64url-encoded high-entropy random value. Due to the use of TLS this value should be known only to the server and client.</p>
<p>The client responds to a <code>dns-01</code> challenge by provisioning a DNS <strong>TXT</strong> record containing the SHA-256 digest of the <em>key authorisation</em> value, which is the concatenation of the <code>token</code> value from the challenge object and the JWK Thumbprint of the account key. For example:</p>
<pre><code>_acme-challenge.www.example.org. 300 IN TXT &quot;gfj9Xq...Rg85nM&quot;</code></pre>
<p>The client then informs the ACME server that it can validate the challenge:</p>
<pre><code>POST /acme/chall/Rg5dV14Gh1Q
Host: example.com
Content-Type: application/jose+json

{
  &quot;protected&quot;: base64url({
    &quot;alg&quot;: &quot;ES256&quot;,
    &quot;kid&quot;: &quot;https://example.com/acme/acct/evOfKhNU60wg&quot;,
    &quot;nonce&quot;: &quot;SS2sSl1PtspvFZ08kNtzKd&quot;,
    &quot;url&quot;: &quot;https://example.com/acme/chall/Rg5dV14Gh1Q&quot;
  }),
  &quot;payload&quot;: base64url({}),
  &quot;signature&quot;: &quot;Q1bURgJoEslbD1c5...3pYdSMLio57mQNN4&quot;
}</code></pre>
<p>The ACME server will query the DNS. When it sees that the expected TXT record, the challenge (and corresponding identifier authorisation) are completed.</p>
<p>Because DNSSEC is not widely deployed, ACME servers can mitigate against DNS-based attacks by querying DNS from mutiple vantage points. This increases attack cost and complexity.</p>
<h2 id="dns-and-certbot">DNS and Certbot</h2>
<p>Certbot provides the <code>--preferred-challenges={dns,http}</code> CLI option to specify which challenge type to prefer if the server offers multiple challenges.</p>
<p>There are several <a href="https://certbot.eff.org/docs/using.html#dns-plugins">DNS plugins</a> available for using Certbot with particular DNS services. For example there are plugins for Cloudflare, Route53 and many other services. At a glance, many of them are packaged for Fedora. Each DNS plugin has different options to activate and configure it. Because we are not using any of these services I won’t go into further details here.</p>
<p>Certbot also provides <a href="https://certbot.eff.org/docs/using.html#pre-and-post-validation-hooks">pre and post validation hooks</a> for the <code>--manual</code> strategy. These let the user specify scripts to carry out challenge provisioning and cleanup steps. The command line options are <code>--manual-auth-hook</code> and <code>--manual-cleanup-hook</code>.</p>
<h2 id="certbot-and-freeipa-dns">Certbot and FreeIPA DNS</h2>
<p>You can use the CLI options described above to implement arbitrary means of responding to ACME challenges. And I have done just that for responding to the <code>dns-01</code> challenge using the FreeIPA integrated DNS service.</p>
<p>The FreeIPA integrated DNS is an optional component of FreeIPA. It is implmented using the BIND DNS server and a database plugin causing BIND to read from the FreeIPA replicated LDAP database. The DNS service can be installed at server install time, or afterwards via the <code>ipa-dns-install</code> command. The <code>freeipa-server-dns</code> (Fedora) or <code>ipa-server-dns</code> (RHEL) package provides this feature. The rest of this section assumes that the FreeIPA integrated DNS server is installed and FreeIPA-enrolled client machines are configured to use it.</p>
<p>The <code>ipa dnsrecord-add &lt;zone&gt; &lt;name&gt; ...</code> command adds record(s) to the zone. The resource types and values are given in options like <code>--aaaa-rec=&lt;ip6addr&gt;</code> or <code>--txt-rec=&lt;string&gt;</code>. The corresponding command <code>dnsrecord-del</code> command has the same format. Knowing that we can also interact with the FreeIPA server via the <code>ipalib</code> Python library, we have everything we need to implement the Certbot hook script(s) that will use FreeIPA’s DNS to satisfy the ACME <code>dns-01</code> challenge.</p>
<h3 id="hook-script">Hook script</h3>
<p>The script is so short I will just include the whole thing here. I have broken it into chunks with commentary.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="co">#!/usr/bin/python3</span></span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="im">import</span> os</span>
<span id="cb4-3"><a href="#cb4-3"></a><span class="im">from</span> dns <span class="im">import</span> resolver</span>
<span id="cb4-4"><a href="#cb4-4"></a><span class="im">from</span> ipalib <span class="im">import</span> api </span>
<span id="cb4-5"><a href="#cb4-5"></a><span class="im">from</span> ipapython <span class="im">import</span> dnsutil</span></code></pre></div>
<p>Shebang, imports. Trivial.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a>certbot_domain <span class="op">=</span> os.environ[<span class="st">&#39;CERTBOT_DOMAIN&#39;</span>]</span>
<span id="cb5-2"><a href="#cb5-2"></a>certbot_validation <span class="op">=</span> os.environ[<span class="st">&#39;CERTBOT_VALIDATION&#39;</span>]</span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="cf">if</span> <span class="st">&#39;CERTBOT_AUTH_OUTPUT&#39;</span> <span class="kw">in</span> os.environ:</span>
<span id="cb5-4"><a href="#cb5-4"></a>    command <span class="op">=</span> <span class="st">&#39;dnsrecord_del&#39;</span></span>
<span id="cb5-5"><a href="#cb5-5"></a><span class="cf">else</span>:</span>
<span id="cb5-6"><a href="#cb5-6"></a>    command <span class="op">=</span> <span class="st">&#39;dnsrecord_add&#39;</span></span></code></pre></div>
<p>Certbot provides the domain name and the <em>authorisation string</em> via environment variables. In the cleanup phase it also sets the <code>CERTBOT_AUTH_OUTPUT</code> environment variable. Therefore I use this same script for both the authorisation and cleanup phases. Because the commands are so similar, the only thing that changes during cleanup is the command name.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a>validation_domain <span class="op">=</span> <span class="ss">f&#39;_acme-challenge.</span><span class="sc">{</span>certbot_domain<span class="sc">}</span><span class="ss">&#39;</span></span>
<span id="cb6-2"><a href="#cb6-2"></a>fqdn <span class="op">=</span> dnsutil.DNSName(validation_domain).make_absolute()</span>
<span id="cb6-3"><a href="#cb6-3"></a>zone <span class="op">=</span> dnsutil.DNSName(resolver.zone_for_name(fqdn))</span>
<span id="cb6-4"><a href="#cb6-4"></a>name <span class="op">=</span> fqdn.relativize(zone)</span></code></pre></div>
<p>Construct the validation domain name and find the corresponding DNS zone, i.e. the zone in which we must create the TXT record. Then we relativise the validation domain name against the zone.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a>api.bootstrap(context<span class="op">=</span><span class="st">&#39;cli&#39;</span>)</span>
<span id="cb7-2"><a href="#cb7-2"></a>api.finalize()</span>
<span id="cb7-3"><a href="#cb7-3"></a>api.Backend.rpcclient.<span class="ex">connect</span>()</span>
<span id="cb7-4"><a href="#cb7-4"></a></span>
<span id="cb7-5"><a href="#cb7-5"></a>api.Command[command](</span>
<span id="cb7-6"><a href="#cb7-6"></a>  zone,</span>
<span id="cb7-7"><a href="#cb7-7"></a>  name,</span>
<span id="cb7-8"><a href="#cb7-8"></a>  txtrecord<span class="op">=</span>[certbot_validation],</span>
<span id="cb7-9"><a href="#cb7-9"></a>  dnsttl<span class="op">=</span><span class="dv">60</span>)</span></code></pre></div>
<p>Initialise the API and execute the command. Note that names of the keyword arguments are different from the corresponding CLI options.</p>
<p>There are some important <strong>caveats</strong>. There must be latent, non-expired Kerberos credentials in the execution environment. These can be in the default credential cache or specified via the <code>KRB5CCNAME</code> environment variable (e.g. to point to a keytab file). The principal must also have permissions to add and remove DNS records.</p>
<h2 id="demo">Demo</h2>
<p>As in previous ACME demos the client machine is enrolled as a FreeIPA client and trusts the FreeIPA CA. For this demo Certbot does not need to run as <code>root</code>. But by default Certbot tries to read and write files under <code>/etc/letsencrypt</code>. I had to override this behaviour with the following command line options:</p>
<dl>
<dt><code>--config-dir DIR</code></dt>
<dd><p>Configuration directory. (default: <code>/etc/letsencrypt</code>)</p>
</dd>
<dt><code>--work-dir DIR</code></dt>
<dd><p>Working directory. (default: <code>/var/lib/letsencrypt</code>)</p>
</dd>
<dt><code>--logs-dir LOGS_DIR</code></dt>
<dd><p>Logs directory. (default: <code>/var/log/letsencrypt</code>)</p>
</dd>
</dl>
<p>I defined these options in a shell array variable for use in subsequent commands. I included the ACME server configuration too:</p>
<pre><code>[f31-0:~] ftweedal% CERTBOT_ARGS=( 
array&gt; --logs-dir ~/certbot/log
array&gt; --work-dir ~/certbot/work
array&gt; --config-dir ~/certbot/config
array&gt; --server https://ipa-ca.ipa.local/acme/directory
array&gt; )</code></pre>
<p>Next I registered an account:</p>
<pre><code>[f31-0:~] ftweedal% certbot $CERTBOT_ARGS \
    register --email ftweedal@redhat.com \
    --agree-tos --no-eff-email --quiet
Saving debug log to /home/ftweedal/certbot/log/letsencrypt.log

IMPORTANT NOTES:
 - Your account credentials have been saved in your Certbot
   configuration directory at /home/ftweedal/certbot/config. You
   should make a secure backup of this folder now. This configuration
   directory will also contain certificates and private keys obtained
   by Certbot so making regular backups of this folder is ideal.</code></pre>
<p>The <code>--no-eff-email</code> option suppressed the <em>“Would you be willing to share your email address with the Electronic Frontier Foundation?”</em> prompt.</p>
<p>The FreeIPA hook script requires Kerberos credentials so I executed <code>kinit admin</code>. <strong>In production use a less privileged account</strong> with permissions to add and delete DNS records.</p>
<pre><code>[f31-0:~] ftweedal% kinit admin
Password for admin@IPA.LOCAL: XXXXXXXX</code></pre>
<p>Now I was ready to request the certificate. Alongside executing <code>certbot</code>, in another terminal I executed DNS queries to observe the creation and deletion of the TXT record.</p>
<pre><code>[root@f31-0 ~]# certbot $CERTBOT_ARGS \
    certonly --domain $(hostname) \
    --preferred-challenges dns \
    --manual --manual-public-ip-logging-ok \
    --manual-auth-hook /home/ftweedal/certbot-dns-ipa.py \
    --manual-cleanup-hook /home/ftweedal/certbot-dns-ipa.py
Saving debug log to /home/ftweedal/certbot/log/letsencrypt.log 
Plugins selected: Authenticator manual, Installer None                                                            
Obtaining a new certificate                                                                                       
Performing the following challenges:
dns-01 challenge for f31-0.ipa.local
Running manual-auth-hook command: /home/ftweedal/certbot-dns-ipa.py
Waiting for verification...
Cleaning up challenges
Running manual-cleanup-hook command: /home/ftweedal/certbot-dns-ipa.py

IMPORTANT NOTES:
 - Congratulations! Your certificate and chain have been saved at:
   /home/ftweedal/certbot/config/live/f31-0.ipa.local/fullchain.pem
   Your key file has been saved at:
   /home/ftweedal/certbot/config/live/f31-0.ipa.local/privkey.pem
   Your cert will expire on 2020-08-11. To obtain a new or tweaked
   version of this certificate in the future, simply run certbot
   again. To non-interactively renew *all* of your certificates, run
   &quot;certbot renew&quot;
 - If you like Certbot, please consider supporting our work by:

   Donating to ISRG / Let&#39;s Encrypt:   https://letsencrypt.org/donate
   Donating to EFF:                    https://eff.org/donate-le</code></pre>
<p>The certificate was issued and the process took about 10 seconds. In the other terminal, running <code>dig</code> every couple of seconds let me observe the TXT record that was created and then deleted:</p>
<pre><code>[f31-0:~] ftweedal% dig +short TXT _acme-challenge.f31-0.ipa.local

[f31-0:~] ftweedal% dig +short TXT _acme-challenge.f31-0.ipa.local
&quot;5qkVb3ykx8nRdJOKbKf-xDtoySFl-B2W37bBBOHGoyc&quot;

[f31-0:~] ftweedal% dig +short TXT _acme-challenge.f31-0.ipa.local
&lt;&lt; no output; record is gone &gt;&gt;</code></pre>
<h2 id="error-handling">Error handling</h2>
<p>To my surprise, a failure (non-zero exit status) of the authorisation hook script <em>does not</em> cause Certbot to halt. For example, after deleting my credential cache with <code>kdestroy</code> and running <code>certbot</code> with the same options as above, Certbot output an error message and the standard error output from the hook script:</p>
<pre><code>...
Running manual-auth-hook command: /home/ftweedal/certbot-dns-ipa.py                                               
manual-auth-hook command &quot;/home/ftweedal/certbot-dns-ipa.py&quot;
returned error code 1                                
Error output from manual-auth-hook command certbot-dns-ipa.py:                                                    
Traceback (most recent call last):                                                                                
  File &quot;/usr/lib/python3.7/site-packages/ipalib/rpc.py&quot;, line 647,
  in get_auth_info                               
      response = self._sec_context.step()                                          
  ...</code></pre>
<p>Nevertheless Certbot proceeded to indicating to the server that the challenge is ready for verification:</p>
<pre><code>Waiting for verification...                                                                                       
&lt; 20 seconds elapse &gt;</code></pre>
<p>It then cleaned up the challenges and ran the cleanup hook (which also failed, as expected, due to no Kerberos credentials):</p>
<pre><code>Cleaning up challenges   
Cleaning up challenges                                                                                            
Running manual-cleanup-hook command: /home/ftweedal/certbot-dns-ipa.py
manual-cleanup-hook command &quot;/home/ftweedal/certbot-dns-ipa.py&quot; returned error code 1                             
Error output from manual-cleanup-hook command certbot-dns-ipa.py:                                                 
Traceback (most recent call last):   
  ...</code></pre>
<p>Finally it output the error from the ACME service:</p>
<pre><code>An unexpected error occurred:                                                                                     
There was a problem with a DNS query during identifier validation ::
  Unable to validate DNS-01 challenge at _acme-challenge.f31-0.ipa.local                                                                                         
Error: DNS name not found [response code 3]                                                                       
Please see the logfiles in /home/ftweedal/certbot/log for more details. </code></pre>
<p>Responding to a challenge after an abnormal exit of the authorisation hook seems to infringe RFC 8555 §8.2 which states:</p>
<blockquote>
<p>Clients SHOULD NOT respond to challenges until they believe that the server’s queries will succeed.</p>
</blockquote>
<p>I <a href="https://github.com/certbot/certbot/issues/7990">reported this issue</a> against the Certbot GitHub repository.</p>
<h2 id="discussion">Discussion</h2>
<p>The <code>certbot-dns-ipa.py</code> script is <a href="https://gist.github.com/frasertweedale/ca42ff31d5f5b8d3c6d4d3a94f9fbd0e">available in a Gist</a>. It is trivial so consider it public domain.</p>
<p>The script is an artifact of work that is partly an exploration of ACME use cases, and partly for verifying the PKI and FreeIPA ACME services. I encountered no issues on the ACME server side which was pleasing.</p>
<p>From the client point of view it was good to confirm that what <em>sounded</em> like a valid use case was indeed valid. Not only that, it was straightforward thanks to the FreeIPA Python API and the design of the DNS plugin. The success of this use case exploration leads to to a couple of related questions:</p>
<ul>
<li>Should we build a “proper” Certbot plugin for FreeIPA DNS?</li>
<li>Should we distribute and support the manual hook script?</li>
</ul>
<p>These questions don’t need answers today. But it is good to outline and compare the options.</p>
<p>From a technical standpoint these are not mutually exclusive; you could do both. But from a usage standpoint you only really need one or the other. A proper plugin might have better UX and discoverability but it would be additional work (how much more I’m not sure yet). On the other hand the hook script is pretty much already “done”. We would just need to distribute it, e.g. install it under <code>/usr/libexec/ipa/</code>.</p>
<p>This post concludes my “trilogy” of ACME client use case demos. In the future I will probably explore the intersection of ACME, OpenShift and FreeIPA. If so, expect the “sequel trilogy”. But my immediate focus must be to finish the FreeIPA ACME service and get it merged upstream.</p>]]></summary>
</entry>
<entry>
    <title>ACME for Apache httpd with mod_md</title>
    <link href="https://frasertweedale.github.io/blog-redhat/posts/2020-05-07-ipa-acme-mod_md.html" />
    <id>https://frasertweedale.github.io/blog-redhat/posts/2020-05-07-ipa-acme-mod_md.html</id>
    <published>2020-05-07T00:00:00Z</published>
    <updated>2020-05-07T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="acme-for-apache-httpd-with-mod_md">ACME for Apache httpd with mod_md</h1>
<p><em>This post is part of a series of ACME client demonstrations. See also the posts about</em> <a href="2020-05-06-ipa-acme-intro.html">Certbot standalone HTTP</a> <em>and</em> <a href="2020-05-13-ipa-acme-dns.html">Certbot with FreeIPA DNS</a>.</p>
<p><a href="">mod_md</a> is an ACME client module for Apache httpd. In this post I demonstrate the use mod_md with the FreeIPA ACME service to automatically acquire certificates for <strong>m</strong>anaged <strong>d</strong>omains from the FreeIPA CA.</p>
<p>mod_md supports the <code>http-01</code> and <code>tls-alpn-01</code> challenges (also <code>dns-01</code> via external programs). The FreeIPA ACME service does not implement <code>tls-alpn-01</code> so we will use the HTTP-based challenge. For this httpd needs to be listening on port 80, which is the case in the default Fedora configuration:</p>
<pre><code>[root@f31-0 ~]# grep ^Listen /etc/httpd/conf/httpd.conf
Listen 80</code></pre>
<p>First step was to install the module:</p>
<pre><code>[root@f31-0 ~]# dnf install -y mod_md
  &lt;stuff happens&gt;
Complete!</code></pre>
<p>Looking at the installed configuration files and their contents, I see the relevant load directives already in place:</p>
<pre><code>[root@f31-0 ~]# rpm -qc mod_md
/etc/httpd/conf.modules.d/01-md.conf

[root@f31-0 ~]# cat /etc/httpd/conf.modules.d/01-md.conf
LoadModule md_module modules/mod_md.so</code></pre>
<p>I created a minimal <code>VirtualHost</code> configuration:</p>
<pre><code>[root@f31-0 ~]# cat &gt;/etc/httpd/conf.d/acme.conf &lt;&lt;EOF
LogLevel warn md:notice

MDCertificateAuthority https://ipa-ca.ipa.local/acme/directory
MDCertificateAgreement accepted

MDomain f31-0.ipa.local

&lt;VirtualHost *:443&gt;
    ServerName f31-0.ipa.local

    SSLEngine on
    # no certificates specification
&lt;/VirtualHost&gt;
EOF</code></pre>
<p>Starting httpd and watching the error log, I observed that shortly after startup it only took mod_md about 5 seconds to create an account, submit an order, prove control of the <code>f31-0.ipa.local</code> DNS name and retrieve the issued certificate:</p>
<pre><code>[Wed May 06 15:51:37.371414 2020] [core:notice] [pid 82766:tid
  140661368246592] AH00094: Command line: &#39;/usr/sbin/httpd -D
  FOREGROUND&#39;
[Wed May 06 15:51:43.086719 2020] [md:notice] [pid 82778:tid
  140661321930496] AH10059: The Managed Domain f31-0.ipa.local has
  been setup and changes will be activated on next (graceful) server
  restart.</code></pre>
<p>The notice that we still need to perform a (graceful) restart is important. Indeed a requests from another host still fails with a self-signed certificate warning:</p>
<pre><code>[f31-1:~] ftweedal% curl https://f31-0.ipa.local/
curl: (60) SSL certificate problem: self signed certificate
More details here: https://curl.haxx.se/docs/sslcerts.html

curl failed to verify the legitimacy of the server and therefore
could not establish a secure connection to it. To learn more about
this situation and how to fix it, please visit the web page
mentioned above.</code></pre>
<p>After preforming a (graceful) restart of httpd:</p>
<pre><code>[f31-0:~] ftweedal% sudo systemctl reload httpd</code></pre>
<p>Requests now work (never mind the 403 response status):</p>
<pre><code>[f31-1:~] ftweedal% curl --head https://f31-0.ipa.local/
HTTP/1.1 403 Forbidden
Date: Wed, 06 May 2020 06:11:43 GMT
Server: Apache/2.4.43 (Fedora) OpenSSL/1.1.1d mod_auth_gssapi/1.6.1 mod_wsgi/4.6.6 Python/3.7
Last-Modified: Thu, 25 Jul 2019 05:18:03 GMT
ETag: &quot;15bc-58e7a8ccdb8c0&quot;
Accept-Ranges: bytes
Content-Length: 5564
Content-Type: text/html; charset=UTF-8</code></pre>
<p><code>curl -v</code> output included the following certificate detail:</p>
<pre><code>* Server certificate:
*  subject: CN=f31-0.ipa.local
*  start date: May  6 05:51:41 2020 GMT
*  expire date: Aug  4 05:51:41 2020 GMT
*  subjectAltName: host &quot;f31-0.ipa.local&quot; matched cert&#39;s &quot;f31-0.ipa.local&quot;
*  issuer: O=IPA.LOCAL 202004011654; CN=Certificate Authority
*  SSL certificate verify ok.</code></pre>
<p>Observe that it is a short-lived certificate issued by the FreeIPA CA.</p>
<p>The fact that a graceful restart was required suggests that if you are using mod_md in production, you should configure a cron job (or equivalent) to execute that on a regular schedule. The <code>MDRenewWindow</code> directive defines the remaining certificate lifetime at which mod_md will first attempt to renew the certificate. The default value is <code>33%</code> which for 90 day certificates is 30 days. Therefore with 90 days certificates and the default <code>MDRenewWindow 33%</code>, restarting weekly seems reasonable.</p>
<p>One last curiousity: by default mod_md publishes a “certificate status” resource at <code>.httpd/certificate-status</code> for each managed domain:</p>
<pre><code>[f31-1:~] ftweedal% curl \
    https://f31-0.ipa.local/.httpd/certificate-status
{
  &quot;valid&quot;: {
    &quot;until&quot;: &quot;Tue, 04 Aug 2020 05:51:41 GMT&quot;,
    &quot;from&quot;: &quot;Wed, 06 May 2020 05:51:41 GMT&quot;
  },
  &quot;serial&quot;: &quot;1E&quot;,
  &quot;sha256-fingerprint&quot;: &quot;a70d2182f347cf9dddfbd19a14243c5efe24df55fa5728297c667494a28e7d2e&quot;
}</code></pre>
<p>This can be suppressed by <code>MDCertificateStatus off</code> which is a server-wide setting.</p>
<h2 id="discussion">Discussion</h2>
<p>Confession time. The above scenario did not go anywhere near as smoothly as portrayed above. In fact, mod_md was failing immediately after retrieving the directory resource:</p>
<pre><code>[Tue May 05 22:28:32.462108 2020] [md:warn] [pid 68047:tid
140418815502080] (22)Invalid argument: md[f31-0.ipa.local]
while[Contacting ACME server for f31-0.ipa.local at
https://ipa-ca.ipa.local/acme/directory] detail[Unable to understand
ACME server response from &lt;https://ipa-ca.ipa.local/acme/directory&gt;.
Wrong ACME protocol version or link?]</code></pre>
<p>I went to the mod_md source code to investigate. The problem was that mod_md required the ACME <code>revokeCert</code> and <code>keyChange</code> (account key rollover) resources to be defined in the resource document, even though mod_md does not use those capabilities (at this time). The Dogtag ACME responder has not yet implemented key rollover. As a consequence, mod_md refused to interact with it.</p>
<p>What does RFC 8555 have to say about this? §7.1 states:</p>
<blockquote>
<p>The server MUST provide “directory” and “newNonce” resources.</p>
</blockquote>
<p>But there is no explicit statement about whether other resources are, or are not, required (with the exception of the <code>newAuthz</code> resource other resource which is optional). My conclusion is that mod_md, in checking for resources it doesn’t even use, is too strict. I submitted <a href="https://github.com/icing/mod_md/pull/214">a pull request</a> to <a href="https://github.com/icing/mod_md">https://github.com/icing/mod_md</a> to relax the check. It was accepted and merged the next day.</p>
<p>Note that mod_md has also been pulled into the httpd codebase, although it does not seem to be as actively maintained there at this point in time. I suppose that the httpd code is periodically updated with the code from the <em>icing</em> respository. Nevertheless I also submitted a <a href="https://github.com/apache/httpd/pull/122">pull request to httpd</a>. At time of publication of this post there has been no activity. I have also submitted bugs against the Fedora and RHEL mod_md packages.</p>
<p>In the meantime I built a version of the Fedora package containing my patch. This time mod_md was able to successfully validate the identifier and finalise the order, causing the certificate to be issued. But it was not able to retrieve the certificate; mod_md does not handle the absense of the <code>Location</code> header in the response to the finalise request. This header was required in an earlier (pre-RFC) draft of the ACME protocol, but it is not required any more. <em>Boulder</em> (the ACME server implementation used by Let’s Encrypt) does set it so mod_md works fine with Boulder. But the Dogtag ACME service did not set it and mod_md fails at this point, putting the client-side order data into an unrecoverable state.</p>
<p>The quick fix was to update the Dogtag ACME service to include the Location header. I also <a href="https://github.com/icing/mod_md/issues/216">reported the issue</a> in the upstream repository.</p>
<p>That’s it for this demo. For my next FreeIPA ACME demo I’m going to attempt DNS-based identifier validation challenges with Certbot and FreeIPA’s integrated DNS.</p>]]></summary>
</entry>

</feed>
