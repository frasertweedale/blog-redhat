<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>Fraser's IdM Blog - User namespaces in OpenShift via CRI-O annotations</title>
        <link rel="stylesheet" type="text/css" href="../css/default.css" />
    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="../">Fraser's IdM Blog</a>
            </div>
            <div id="navigation">
                <a href="../">Home</a>
                <a href="../archive.html">Archive</a>
            </div>
        </div>

        <div id="content">
            <div class="info">
    
    Tags: <a title="All pages tagged 'openshift'." href="../tags/openshift.html">openshift</a>, <a title="All pages tagged 'security'." href="../tags/security.html">security</a>
    
</div>

<div id="postContent">
    <h1 id="user-namespaces-in-openshift-via-cri-o-annotations">User namespaces in OpenShift via CRI-O annotations</h1>
<p>In a recent post I covered the lack of user namespace support in OpenShift, and discussed the <a href="https://github.com/cri-o/cri-o/pull/3944">upcoming CRI-O feature</a> for user namespacing of containers, controlled by annotations.</p>
<p>I now have an OpenShift nightly cluster deployed. It uses a prerelease version of CRI-O v1.20, which includes this new feature. So it’s time to experiment! This post records my investigation of this feature.</p>
<h2 id="preliminaries">Preliminaries <a href="#preliminaries" class="section">§</a></h2>
<p>I’ll skip the details of deploying the nightly (4.7) cluster (because they are not important). What <em>is</em> important is that I created a <code>MachineConfig</code> to enable the CRI-O user namespace annotation feature, <a href="2020-11-30-openshift-machine-config-operator.html">as described in my previous post</a>.</p>
<p>As in the initial investigation, I created a new user account and project namespace for the experiments:</p>
<pre><code>% oc new-project test
Now using project &quot;test&quot; on server &quot;https://api.permanent.idmocp.lab.eng.rdu2.redhat.com:6443&quot;.

% oc create user test
user.user.openshift.io/test created

% oc adm policy add-role-to-user admin test
clusterrole.rbac.authorization.k8s.io/admin added: &quot;test&quot;</code></pre>
<h2 id="creating-a-user-namespaced-pod---attempt-1">Creating a user namespaced pod - Attempt 1 <a href="#creating-a-user-namespaced-pod---attempt-1" class="section">§</a></h2>
<p>I defined a pod that just runs <code>sleep</code>, but uses the new annotation to run it in a user namespace. The <code>map-to-root=true</code> directive says that the “beginning” of the host uid range assigned to the container should maps to uid 0 (i.e. <code>root</code>) in the container.</p>
<pre><code>$ cat userns-test.yaml
apiVersion: v1
kind: Pod
metadata:
  name: userns-test
  annotations:
    io.kubernetes.cri-o.userns-mode: &quot;auto:map-to-root=true&quot;
spec:
  containers:
  - name: userns-test
    image: freeipa/freeipa-server:fedora-31
    command: [&quot;sleep&quot;, &quot;3601&quot;]</code></pre>
<p>Create the pod:</p>
<pre><code>$ oc --as test create -f userns-test.yaml
pod/userns-test created</code></pre>
<p>After a few seconds, does everything look OK?</p>
<pre><code>$ oc get pod userns-test
NAME          READY   STATUS              RESTARTS   AGE
userns-test   0/1     ContainerCreating   0          14s</code></pre>
<p>Hm, 14 seconds seems a long time to be stuck at <code>ContainerCreating</code>. What does <code>oc describe</code> reveal?</p>
<pre><code>$ oc describe pod/userns-test
Name:         userns-test
Namespace:    test
Priority:     0
Node:         ft-47dev-2-27h8r-worker-0-j4jjn/10.8.1.106
Start Time:   Mon, 30 Nov 2020 12:41:34 +0000
Labels:       &lt;none&gt;
Annotations:  io.kubernetes.cri-o.userns-mode: auto:map-to-root=true
              openshift.io/scc: restricted
Status:       Pending

...

Events:
  Type     Reason                  Age                       From                                      Message
  ----     ------                  ----                      ----                                      -------
  Normal   Scheduled               &lt;unknown&gt;                                                           Successfully assigned test/userns-test to ft-47dev-2-27h8r-worker-0-j4jjn
  Warning  FailedCreatePodSandBox  &lt;invalid&gt; (x96 over 20m)  kubelet, ft-47dev-2-27h8r-worker-0-j4jjn  Failed to create pod sandbox: rpc error: code = Unknown desc = error creating pod sandbox with name &quot;k8s_userns-test_test_e4f69d50-e061-46ca-b933-000bcea3363a_0&quot;: could not find enough available IDs</code></pre>
<p>The node failed to create the pod sandbox. To spare you scrolling to read the unwrapped error message, I’ll reproduce it:</p>
<pre><code>Failed to create pod sandbox: rpc error: code = Unknown
desc = error creating pod sandbox with name
&quot;k8s_userns-test_test_e4f69d50-e061-46ca-b933-000bcea3363a_0&quot;:
could not find enough available IDs</code></pre>
<p>My initial reaction to this error is: <strong>this is good!</strong> It <em>seems</em> that CRI-O is attempting to create a user namespace for the container, but cannot. Another problem to solve, but we seem to be on the right track.</p>
<h2 id="etcsubuid"><code>/etc/subuid</code> <a href="#etcsubuid" class="section">§</a></h2>
<p>I had not yet done any host configuration related to user namespace mappings. But I had a feeling that the <code>/etc/subuid</code> and <code>/etc/subgid</code> files would come into play. According to <code>subuid(5)</code>:</p>
<blockquote>
<p>Each line in /etc/subuid contains a user name and a range of subordinate user ids that user is allowed to use.</p>
</blockquote>
<p>The description in <code>subgid(5)</code> is similar.</p>
<p>If the user that is attempting to create the containers doesn’t have an sufficient range of unused host uids and gids to use, it follows that it will not be able to create the user namespace for the pod.</p>
<p>I used a debug shell to observe the current contents of <code>/etc/subuid</code> and <code>/etc/subgid</code> on worker nodes:</p>
<pre><code>sh-4.4# cat /etc/subuid
core:100000:65536
sh-4.4# cat /etc/subgid
core:100000:65536</code></pre>
<p>The user <code>core</code> owns a uid and gid range of size 65536, starting at uid/gid 100000. There are no other ranges defined.</p>
<p>At this point, I have a strong feeling we need to define uid and gid ranges for the appropriate user, and then things will hopefully start working. The next question is: <em>who is the appropriate user</em>? That is, in OpenShift which user is responsible for creating the containers and, in this case, the user namespaces? Again on the worker node debug shell, I queried which user is running <code>crio</code>:</p>
<pre><code>sh-4.4# ps -o user,pid,cmd -p $(pgrep crio)
USER         PID CMD
root        1791 /usr/bin/crio --enable-metrics=true --metrics-port=9537</code></pre>
<p><code>crio</code> is running as the <code>root</code> user, which is not surprising. So we will need to add mappings for the <code>root</code> user to the mapping files.</p>
<h3 id="machineconfig-for-modifying-etcsubugid"><code>MachineConfig</code> for modifying <code>/etc/sub[ug]id</code> <a href="#machineconfig-for-modifying-etcsubugid" class="section">§</a></h3>
<p>I will create a <code>MachineConfig</code> to append the mappings <code>/etc/subuid</code> and <code>/etc/subgid</code>. First we need the base64 encoding of the line we want to add:</p>
<pre><code>$ echo &quot;root:200000:268435456&quot; | base64
cm9vdDoyMDAwMDA6MjY4NDM1NDU2Cg==</code></pre>
<p>The <code>MachineConfig</code> definition (note that it is scoped to the <code>worker</code> role):</p>
<pre><code>$ cat machineconfig-subuid-subgid.yaml 
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: subuid-subgid
spec:
  config:
    ignition:
      version: 3.1.0
    storage:
      files:
      - path: /etc/subuid
        append:
          - source: data:text/plain;charset=utf-8;base64,cm9vdDoyMDAwMDA6MjY4NDM1NDU2Cg==
      - path: /etc/subgid
        append:
          - source: data:text/plain;charset=utf-8;base64,cm9vdDoyMDAwMDA6MjY4NDM1NDU2Cg==</code></pre>
<p>Creating the <code>MachineConfig</code> object:</p>
<pre><code>$ oc create -f machineconfig-subuid-subgid.yaml
machineconfig.machineconfiguration.openshift.io/subuid-subgid created</code></pre>
<p>After a few moments, checking the <code>machineconfigpool/worker</code> object revealed that cluster is in a degraded state:</p>
<pre><code>$ oc get -o json mcp/worker |jq '.status.conditions[-2:]'
[
  {
    &quot;lastTransitionTime&quot;: &quot;2020-12-01T02:55:52Z&quot;,
    &quot;message&quot;: &quot;Node ft-47dev-2-27h8r-worker-0-f8bnl is reporting: \&quot;can't reconcile config rendered-worker-a37679c5cfcefb5b0af61bb3674dccc4 with rendered-worker-3cbd4cabeedd441500c83363dbf505fd: ignition file /etc/subuid includes append: unreconcilable\&quot;&quot;,
    &quot;reason&quot;: &quot;1 nodes are reporting degraded status on sync&quot;,
    &quot;status&quot;: &quot;True&quot;,
    &quot;type&quot;: &quot;NodeDegraded&quot;
  },
  {
    &quot;lastTransitionTime&quot;: &quot;2020-12-01T02:55:52Z&quot;,
    &quot;message&quot;: &quot;&quot;,
    &quot;reason&quot;: &quot;&quot;,
    &quot;status&quot;: &quot;True&quot;,
    &quot;type&quot;: &quot;Degraded&quot;
  }
]</code></pre>
<p>The error message is:</p>
<pre><code>Node ft-47dev-2-27h8r-worker-0-f8bnl is reporting: \&quot;can't
reconcile config rendered-worker-a37679c5cfcefb5b0af61bb3674dccc4
with rendered-worker-3cbd4cabeedd441500c83363dbf505fd: ignition
file /etc/subuid includes append: unreconcilable\&quot;&quot;,</code></pre>
<p>Upon further investigation, I learned that the Machine Config Operator does not support <code>append</code> operations. This is because appends, in general, are not idempotent and commutative. So I will try again with a new machine config that completely replaces the <code>/etc/subuid</code> and <code>/etc/subgid</code> files.</p>
<p>The new content shall be:</p>
<pre><code>core:100000:65536
root:200000:268435456</code></pre>
<p>The updated <code>MachineConfig</code> definition is:</p>
<pre><code>$ cat machineconfig-subuid-subgid.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: subuid-subgid
spec:
  config:
    ignition:
      version: 3.1.0
    storage:
      files:
      - path: /etc/subuid
        overwrite: true
        contents:
          source: data:text/plain;charset=utf-8;base64,Y29yZToxMDAwMDA6NjU1MzYKcm9vdDoyMDAwMDA6MjY4NDM1NDU2Cg==
      - path: /etc/subgid
        overwrite: true
        contents:
          source: data:text/plain;charset=utf-8;base64,Y29yZToxMDAwMDA6NjU1MzYKcm9vdDoyMDAwMDA6MjY4NDM1NDU2Cg==</code></pre>
<p>I replaced the <code>MachineConfig</code> object:</p>
<pre><code>$ oc replace -f machineconfig-subuid-subgid.yaml
machineconfig.machineconfiguration.openshift.io/subuid-subgid replaced</code></pre>
<p>After a few moments, the cluster is no longer degraded and the worker nodes will be updated over the next several minutes:</p>
<pre><code>$ oc get mcp/worker
NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
worker   rendered-worker-a37679c5cfcefb5b0af61bb3674dccc4   False     True       False      4              0                   0                     0                      3d20h</code></pre>
<p>After <code>READYMACHINECOUNT</code> reached <code>4</code> (all machines in the <code>worker</code> pool), I used a debug shell on one of the worker nodes to confirm that the changes had been applied:</p>
<pre><code>$ oc debug node/ft-47dev-2-27h8r-worker-0-j4jjn
Starting pod/ft-47dev-2-27h8r-worker-0-j4jjn-debug ...
To use host binaries, run `chroot /host`
Pod IP: 10.8.1.106
If you don't see a command prompt, try pressing enter.
sh-4.2# chroot /host
sh-4.4# cat /etc/subuid
core:100000:65536
root:200000:268435456
sh-4.4# cat /etc/subgid
core:100000:65536
root:200000:268435456</code></pre>
<p>Looks good!</p>
<h2 id="creating-a-user-namespaced-pod---attempt-2">Creating a user namespaced pod - Attempt 2 <a href="#creating-a-user-namespaced-pod---attempt-2" class="section">§</a></h2>
<p>It’s time to create the user namespaced pod again, and see if it succeeds this time.</p>
<pre><code>$ oc --as test create -f userns-test.yaml
pod/userns-test created</code></pre>
<p>Unfortunately, the same <code>FailedCreatePodSandBox</code> error occurred. My <code>subuid</code> remedy was either incorrect, or insufficient. I decided to use a debug shell on the worker node to examine the system journal. I searched for the error string <code>could not find enough available IDs</code>, and found the error in the output of the <code>hyperkube</code> unit. A few lines above that, there are some <code>crio</code> log messages, including:</p>
<pre><code>Cannot find mappings for user \&quot;containers\&quot;: No subuid
ranges found for user \&quot;containers\&quot; in /etc/subuid&quot;</code></pre>
<p>So, my mistake was defining ID map ranges for the <code>root</code> user. I should have used the <code>containers</code> user. I fixed the <code>MachineConfig</code> definition to use the file content:</p>
<pre><code>core:100000:65536
containers:200000:268435456</code></pre>
<p>Then I replaced the <code>subuid-subgid</code> object and again waited for Machine Config Operator to update the worker nodes.</p>
<h2 id="creating-a-user-namespaced-pod---attempt-3">Creating a user namespaced pod - Attempt 3 <a href="#creating-a-user-namespaced-pod---attempt-3" class="section">§</a></h2>
<p>Once again, the container remained at <code>ContainerCreating</code>. But the error was different (lines wrapped for readability):</p>
<pre><code>Failed to create pod sandbox: rpc error:
code = Unknown
desc = container create failed:
  time=&quot;2020-12-01T06:40:49Z&quot;
  level=warning
  msg=&quot;unable to terminate initProcess&quot;
  error=&quot;exit status 1&quot;

time=&quot;2020-12-01T06:40:49Z&quot;
level=error
msg=&quot;container_linux.go:366: starting container process caused:
  process_linux.go:472: container init caused:
    write sysctl key net.ipv4.ping_group_range:
      write /proc/sys/net/ipv4/ping_group_range: invalid argument&quot;</code></pre>
<p>After a bit of research, here is my understanding of the situation: CRI-O successfully created the pod sandbox (which includes the user namespace) and is now initialising it. One of the initialisation steps is to set the <code>net.ipv4.ping_group_range</code> sysctl (the subroutine is part of <code>runc</code>), and this is failing. This step is performed for all pods, but it is only failing when the pod is using a user namespace.</p>
<h2 id="net.ipv4.ping_group_range-and-user-namespaces"><code>net.ipv4.ping_group_range</code> and user namespaces <a href="#net.ipv4.ping_group_range-and-user-namespaces" class="section">§</a></h2>
<p>The <code>net.ipv4.ping_group_range</code> sysctl defines the range of group IDs that are allowed to send ICMP Echo packets. Setting it to the full gid range allows <code>ping</code> to be used in rootless containers, without setuid or the <code>CAP_NET_ADMIN</code> and <code>CAP_NET_RAW</code> capabilities.</p>
<p>The CRI-O config key <code>crio.runtime.default_sysctls</code> declares the default sysctls that will be set in all containers. The default OpenShift CRI-O configuration sets it to the full gid range:</p>
<pre><code>sh-4.4# cat /etc/crio/crio.conf.d/00-default \
    | grep -A2 default_sysctls
default_sysctls = [
    &quot;net.ipv4.ping_group_range=0 2147483647&quot;,
]</code></pre>
<p>My working hypothesis is that setting the sysctl in the user-namespaced container fails because the gid range in the sandbox is not <code>0–2147483647</code> but much smaller. This could explain the <code>invalid argument</code> part of the error message.</p>
<p>How to overcome this? I first thought to update the pod spec to specify a different value for the sysctl that reflects the actual gid range in the sandbox. And to do that, I have to calculate what that gid range is.</p>
<h3 id="computing-the-gid-range">Computing the gid range <a href="#computing-the-gid-range" class="section">§</a></h3>
<p>I will work on the assumption that I must refer to the range as it appears <em>in the namespace</em>. That assumption could be wrong, but that’s where I’m starting.</p>
<p>Because I am using <code>map-to-root=true</code>, the start value of the range should be <code>0</code>. The second number in the <code>ping_group_range</code> sysctl value is not the range size but the end gid (inclusive). CRI-O currently hard-codes a default user namespace size of <code>65536</code>.</p>
<p>Because the size of the uid range is a critical parameter, I shall from now on explicitly declare the desired size in the <code>userns-mode</code> annotation. This will protect the solution from change to the default range size. I probably won’t need 65536 uids/gids but I’ll stick with the default for now.</p>
<pre><code>io.kubernetes.cri-o.userns-mode: &quot;auto:size=65536;map-to-root=true&quot;</code></pre>
<p>With a range of <code>65536</code> starting at <code>0</code>, the desired sysctl setting is <code>net.ipv4.ping_group_range=0 65535</code>.</p>
<h3 id="configuring-the-sysctl">Configuring the sysctl <a href="#configuring-the-sysctl" class="section">§</a></h3>
<p>We need <code>ping</code> to continue working in containers that are not namespaced. Therefore, overriding or clearing the CRI-O <code>default_sysctls</code> config is not an option. Instead I need a way to optionally set the <code>net.ipv4.ping_group_range</code> sysctl to a specified value on a per-pod basis.</p>
<p>You can specify sysctls to be set in a pod via the <code>spec.securityContext.sysctls</code> array (see Kubernetes <a href="https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#podsecuritycontext-v1-core">PodSecurityContext documentation</a>). I updated the pod definition to include the sysctl:</p>
<pre><code>$ cat userns-test.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: userns-test
  annotations:
    openshift.io/scc: restricted
    io.kubernetes.cri-o.userns-mode: &quot;auto:size=65536;map-to-root=true&quot;
spec:
  containers:
  - name: userns-test
    image: freeipa/freeipa-server:fedora-31
    command: [&quot;sleep&quot;, &quot;3601&quot;]
  securityContext:
    sysctls:
    - name: &quot;net.ipv4.ping_group_range&quot;
      value: &quot;0 65535&quot;</code></pre>
<p>As I write this, I don’t know yet how CRI-O behaves when both <code>default_sysctls</code> and the pod spec define the same sysctl. It might just set the value from the pod spec, which is the behaviour I need. Or it might first attempt to set the value from <code>default_sysctls</code>, and afterwards set it again to the value from the pod spec (this will fail as before).</p>
<p>Time to find out!</p>
<h2 id="creating-a-user-namespaced-pod---attempt-4">Creating a user namespaced pod - Attempt 4 <a href="#creating-a-user-namespaced-pod---attempt-4" class="section">§</a></h2>
<pre><code>$ oc --as test create -f userns-test.yaml
pod/userns-test created

# ... wait ...

$ oc get pod userns-test
NAME          READY   STATUS                 RESTARTS   AGE
userns-test   0/1     CreateContainerError   0          118s</code></pre>
<p>OK, progress was made! It did not get stuck at <code>ContainerCreating</code>; this time we got a <code>CreateContainerError</code>. This means that the CRI-O sysctl behaviour is what we were hoping for. As for the new error, <code>oc describe</code> gave the detail:</p>
<pre><code>Error: container create failed:
time=&quot;2020-12-01T12:38:45Z&quot;
level=error
msg=&quot;container_linux.go:366: starting container process caused:
  setup user: cannot set uid to unmapped user in user namespace&quot;</code></pre>
<p>My guess is that CRI-O is ignoring the fact that the pod is in a user namespace and is attempting to execute the process using the same uid as it would if the pod were not in a user namespace. The uid is outside the mapped range (<code>0</code>–<code>65535</code>). For my next attempt I will add <code>runAsUser</code> and <code>runAsGroup</code> to the <code>securityContext</code>.</p>
<p>But first some other quick notes and observations. First of all, a user namespace was indeed created for this pod!</p>
<pre><code>sh-4.4# lsns -t user
        NS TYPE  NPROCS    PID USER   COMMAND
4026531837 user     277      1 root   /usr/lib/systemd/systemd --switched-root --system --deserialize 16
4026532599 user       1 684279 200000 /usr/bin/pod</code></pre>
<p>We can examine the uid and gid maps for the namespace:</p>
<pre><code>sh-4.4# cat /proc/684279/uid_map
         0     200000      65536

sh-4.4# cat /proc/684279/gid_map
         1     200001      65535
         0 1000610000          1</code></pre>
<p>It surprised me that gid <code>0</code> is mapped to system user <code>1000610000</code>. I don’t know what consequences this might have; for now I am just noting it.</p>
<p>Because the pod sandbox does exist, I also decided to see if I could get a debug shell:</p>
<pre><code>$ oc debug pod/userns-test
Starting pod/userns-test-debug, command was: sleep 3601
Pod IP: 10.129.3.170
If you don't see a command prompt, try pressing enter.
sh-5.0$ id
uid=1000610000(1000610000) gid=0(root) groups=0(root),1000610000</code></pre>
<p>It worked! But the debug shell cannot be running in the user namespace; the uid (<code>1000610000</code>) is too high. Running <code>lsns</code> in my worker node debug shell confirms it; the namespace still has only one process running in it:</p>
<pre><code>sh-4.4# lsns -t user
        NS TYPE  NPROCS    PID USER   COMMAND
4026531837 user     282      1 root   /usr/lib/systemd/systemd --switched-root --system --deserialize 16
4026532599 user       1 684279 200000 /usr/bin/pod</code></pre>
<h2 id="creating-a-user-namespaced-pod---attempt-5">Creating a user namespaced pod - Attempt 5 <a href="#creating-a-user-namespaced-pod---attempt-5" class="section">§</a></h2>
<p>I once again deleted the <code>userns-test</code> pod. As proposed above, I modified the pod security context to specify that the entry point should be run as uid <code>0</code> and gid <code>0</code>:</p>
<pre><code>$ cat userns-test.yaml
apiVersion: v1
kind: Pod
metadata:
  name: userns-test
  annotations:
    openshift.io/scc: restricted
    io.kubernetes.cri-o.userns-mode: &quot;auto:size=65536;map-to-root=true&quot;
spec:
  containers:
  - name: userns-test
    image: freeipa/freeipa-server:fedora-31
    command: [&quot;sleep&quot;, &quot;3601&quot;]
  securityContext:
    runAsUser: 0
    runAsGroup: 0
    sysctls:
    - name: &quot;net.ipv4.ping_group_range&quot;
      value: &quot;0 65535&quot;</code></pre>
<p>Here we go:</p>
<pre><code>$ oc --as test create -f userns-test.yaml
Error from server (Forbidden): error when creating
&quot;userns-test.yaml&quot;: pods &quot;userns-test&quot; is forbidden: unable to
validate against any security context constraint:
[spec.containers[0].securityContext.runAsUser: Invalid value: 0:
must be in the ranges: [1000610000, 1000619999]]</code></pre>
<p><em>sad trombone</em></p>
<p>I don’t have a clear idea how I could proceed. The security context constraint (SCC) is prohibiting the use of uid <code>0</code> for the container process. Switching to a permissive SCC might allow me to proceed, but it would also mean using a more privileged OpenShift user account. Then that privileged account could then create containers running as <code>root</code> <em>in the system user namespace</em>. We want user namespaces in OpenShift so that we can <em>avoid</em> this exact scenario. So resorting to a permissive SCC (e.g. <code>anyuid</code>) feels like the wrong way to go.</p>
<p>It could be that it’s the only way to go for now, and that more nuanced security policy mechanisms must be implemented before user namespaces can be used in OpenShift to achieve the security objective. In any case, I’ll be reaching out to other engineers and OpenShift experts for their suggestions.</p>
<p>For now, I’m calling it a day! See you soon for the next episode.</p>
</div>
<div id="postFooter">
    <div id="recent">
Recent posts:
<ul>
    
        <li>
            <a href="../posts/2021-11-18-k8s-tcp-udp-ingress.html">Bare TCP and UDP ingress on Kubernetes</a>
        </li>
    
        <li>
            <a href="../posts/2021-10-15-openshift-userns-in-container.html">Creating user namespaces inside containers</a>
        </li>
    
        <li>
            <a href="../posts/2021-07-22-openshift-systemd-workload-demo.html">Demo: namespaced systemd workloads on OpenShift</a>
        </li>
    
        <li>
            <a href="../posts/2021-07-21-freeipa-on-openshift-update.html">FreeIPA on OpenShift: July 2021 update</a>
        </li>
    
        <li>
            <a href="../posts/2021-06-29-openshift-live-changes.html">Live-testing changes in OpenShift clusters</a>
        </li>
    
</ul>

<div id="recentLinks">
    <a type="application/atom+xml" href="../atom.xml">Atom feed</a>
     <!-- em space -->
    <a href="../archive.html">All posts…</a>
</div>

</div>

    <div id="license">
    <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">
        <img alt="Creative Commons License" style="border-width:0" src="https://licensebuttons.net/l/by/4.0/88x31.png">
    </a>
    <br />
    Except where otherwise noted, this work is licensed under a
    <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">
        Creative Commons Attribution 4.0 International License
    </a>.
</div>

</div>

        </div>
        <div class="clear"></div>
        <div id="footer">
            Generated by
            <a href="https://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
